{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dataloader import OCTDataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "root = os.getcwd()\n",
    "train = os.path.join(root, \"df_prime_train.csv\")\n",
    "test = os.path.join(root, \"df_prime_test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "LABELS_SEVERITY = {35: 0,\n",
    "                   43: 0,\n",
    "                   47: 1,\n",
    "                   53: 1,\n",
    "                   61: 2,\n",
    "                   65: 2,\n",
    "                   71: 2,\n",
    "                   85: 2}\n",
    "\n",
    "'''\n",
    "def normalize_array(arr):\n",
    "    \"\"\"\n",
    "    Normalize a 1D NumPy array to have values between 0 and 1.\n",
    "    \"\"\"\n",
    "    arr_min = np.min(arr)\n",
    "    arr_max = np.max(arr)\n",
    "    normalized_arr = (arr - arr_min) / (arr_max - arr_min)\n",
    "    return normalized_arr\n",
    "'''\n",
    "\n",
    "\n",
    "'''Obtain Dataset Images (image, label)'''\n",
    "\n",
    "\n",
    "mean = (.1706)\n",
    "std = (.2112)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "\n",
    "# horizontal flip transform\n",
    "transformHor = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# crop transform\n",
    "transformCrop = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.CenterCrop((224 - 25 - 25, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# blur transform\n",
    "transformBlur = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "args = 'C:/Users/jgril/Documents/GitHub/8803_Final_Project'\n",
    "\n",
    "image_trainset = OCTDataset(args, 'train', transform=transform2)\n",
    "image_testset = OCTDataset(args, 'test', transform=transform2)\n",
    "\n",
    "# create augment versions of the dataset\n",
    "flip_trainset = OCTDataset(args, 'train', transform=transformHor)\n",
    "blur_trainset = OCTDataset(args, 'train', transform=transformBlur)\n",
    "crop_trainset = OCTDataset(args, 'train', transform=transformCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jgril\\Documents\\GitHub\\8803_Final_Project\\Resnet.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W5sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39m# count pictures in each class for FLIP TRAIN DATA\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W5sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m class_counts \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W5sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, label \u001b[39min\u001b[39;00m concatenated_dataset:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W5sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     \u001b[39mif\u001b[39;00m label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m class_counts:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W5sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m         class_counts[label] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jgril\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:235\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m--> 235\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[1;32mc:\\Users\\jgril\\Documents\\GitHub\\8803_Final_Project\\dataloader.py:67\u001b[0m, in \u001b[0;36mOCTDataset.__getitem__\u001b[1;34m(self, index, meta)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     65\u001b[0m csv_file\u001b[39m.\u001b[39mclose()\n\u001b[1;32m---> 67\u001b[0m img, target \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_list[index])\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_labels[index]\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[1;32mc:\\Users\\jgril\\miniconda3\\lib\\site-packages\\PIL\\Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3089\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3092\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3093\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "# count pictures in each class for TRAINING DATA\n",
    "class_counts = {}\n",
    "for _, label in image_trainset:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Training Data Class counts:\", class_counts)\n",
    "print()\n",
    "\n",
    "# count pictures in each class for TESTING DATA\n",
    "class_counts = {}\n",
    "for _, label in image_testset:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Testing Data Class counts:\", class_counts)\n",
    "print()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\" For Class 0 Flip\"\"\"\n",
    "\n",
    "# get the labels\n",
    "labels = flip_trainset._labels\n",
    "# create a list of indices corresponding to data points with label value 0\n",
    "indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "# create a new dataset that only includes data points with label value 0\n",
    "flip_trainset_class_0 = Subset(flip_trainset, indices)\n",
    "\n",
    "'''\n",
    "# count pictures in each class for FLIP TRAIN DATA\n",
    "class_counts = {}\n",
    "for _, label in flip_trainset:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "print(\"Flip Data Class 0 counts:\", class_counts)\n",
    "\n",
    "# check to see if there is only one class\n",
    "unique_labels = set()\n",
    "for _, label in flip_trainset_class_0:\n",
    "    unique_labels.add(label)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of classes:\", num_classes, \"Total Length\", len(flip_trainset_class_0))\n",
    "print()\n",
    "'''\n",
    "\"\"\" For Class 2 Flip\"\"\"\n",
    "\n",
    "# get the labels\n",
    "labels = flip_trainset._labels\n",
    "# create a list of indices corresponding to data points with label value 0\n",
    "indices = [i for i, label in enumerate(labels) if label == 2]\n",
    "# create a new dataset that only includes data points with label value 0\n",
    "flip_trainset_class_2 = Subset(flip_trainset, indices)\n",
    "\n",
    "'''\n",
    "# count pictures in each class for FLIP TRAIN DATA\n",
    "class_counts = {}\n",
    "for _, label in flip_trainset_class_2:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Flip Data Class 2 counts:\", class_counts)\n",
    "\n",
    "# check to see if there is only one class\n",
    "unique_labels = set()\n",
    "for _, label in flip_trainset_class_2:\n",
    "    unique_labels.add(label)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of classes:\", num_classes, \"Total Length\", len(flip_trainset_class_2))\n",
    "print()\n",
    "'''\n",
    "\n",
    "\"\"\" For Class 0 Blur\"\"\"\n",
    "\n",
    "# get the labels\n",
    "labels = blur_trainset._labels\n",
    "# create a list of indices corresponding to data points with label value 0\n",
    "indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "# create a new dataset that only includes data points with label value 0\n",
    "blur_trainset_class_0 = Subset(blur_trainset, indices)\n",
    "\n",
    "'''\n",
    "# count pictures in each class for FLIP TRAIN DATA\n",
    "class_counts = {}\n",
    "for _, label in blur_trainset_class_0:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Blur Data Class 0 counts:\", class_counts)\n",
    "\n",
    "# check to see if there is only one class\n",
    "unique_labels = set()\n",
    "for _, label in blur_trainset_class_0:\n",
    "    unique_labels.add(label)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of classes:\", num_classes, \"Total Length\", len(blur_trainset_class_0))\n",
    "print()\n",
    "'''\n",
    "\n",
    "\"\"\" For Class 2 Blur\"\"\"\n",
    "\n",
    "labels = blur_trainset._labels\n",
    "# create a list of indices corresponding to data points with label value 0\n",
    "indices = [i for i, label in enumerate(labels) if label == 2]\n",
    "# create a new dataset that only includes data points with label value 0\n",
    "blur_trainset_class_2 = Subset(blur_trainset, indices)\n",
    "\n",
    "\n",
    "'''\n",
    "# count pictures in each class for FLIP TRAIN DATA\n",
    "class_counts = {}\n",
    "for _, label in blur_trainset_class_2:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Blur Data Class 2 counts:\", class_counts)\n",
    "\n",
    "# check to see if there is only one class\n",
    "unique_labels = set()\n",
    "for _, label in blur_trainset_class_2:\n",
    "    unique_labels.add(label)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of classes:\", num_classes, \"Total Length\", len(blur_trainset_class_2))\n",
    "print()\n",
    "'''\n",
    "\n",
    "# New Dataset with extra class 2 images\n",
    "concatenated_dataset = ConcatDataset([image_trainset, blur_trainset_class_2])\n",
    "# count pictures in each class for FLIP TRAIN DATA\n",
    "class_counts = {}\n",
    "for _, label in concatenated_dataset:\n",
    "    if label not in class_counts:\n",
    "        class_counts[label] = 0\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(\"Concated Data Class 2 counts:\", class_counts)\n",
    "\n",
    "# check to see if there is only one class\n",
    "unique_labels = set()\n",
    "for _, label in concatenated_dataset:\n",
    "    unique_labels.add(label)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of classes:\", num_classes, \"Total Length\", len(concatenated_dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Resnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgril\\miniconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgril\\miniconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jgril\\Documents\\GitHub\\8803_Final_Project\\Resnet.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/Resnet.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m total_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "raw_train_dataset = image_trainset\n",
    "test_dataset = image_testset\n",
    "\n",
    "train_labels = [label for _, label in raw_train_dataset]\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "train_dataset, val_dataset = train_test_split(raw_train_dataset, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "\n",
    "lr = 0.001\n",
    "gamma = 0.001\n",
    "epochs = 10\n",
    "\n",
    "num_classes = 3 \n",
    "\n",
    "# Load your training and validation dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Load the pre-trained ResNet-18 model and modify it for grayscale images\n",
    "model = resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "num_classes=3\n",
    "\n",
    "#freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=3, bias=False)\n",
    "model.fc = nn.Linear(num_features, 3)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=gamma)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Train the model\n",
    "print(\"Beginning Training\")\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            # calculates validation loss\n",
    "            val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "        val_loss /= len(val_dataset)\n",
    "\n",
    "    # save accuracies and print\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_acc.append(train_accuracy)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_acc.append(val_accuracy)\n",
    "    print('Train accuracy after epoch %d: %.2f %%' % (epoch + 1, train_accuracy))\n",
    "    print('Validation accuracy after epoch %d: %.2f %%' % (epoch + 1, val_accuracy))\n",
    "\n",
    "    # save losses and print\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('Train loss after epoch %d: %.2f' % (epoch + 1, train_loss))\n",
    "    print('Validation loss after epoch %d: %.2f' % (epoch + 1, val_loss))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from dataloader import OCTDataset, transform\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs + metadata_size, nb_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Define dataloaders\n",
    "trainset = OCTDataset(root, 'train', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)\n",
    "testset = OCTDataset(root, 'test', transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs\n",
    "        inputs, labels, metadata = data\n",
    "        inputs, labels, metadata = inputs.to(device), labels.to(device), metadata.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(torch.cat((inputs, metadata), dim=1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
