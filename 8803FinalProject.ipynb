{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECE 8803 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dataloader import OCTDataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "root = os.getcwd()\n",
    "train = os.path.join(root, \"df_prime_train.csv\")\n",
    "test = os.path.join(root, \"df_prime_test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "LABELS_SEVERITY = {35: 0,\n",
    "                   43: 0,\n",
    "                   47: 1,\n",
    "                   53: 1,\n",
    "                   61: 2,\n",
    "                   65: 2,\n",
    "                   71: 2,\n",
    "                   85: 2}\n",
    "\n",
    "\n",
    "def normalize_array(arr):\n",
    "    \"\"\"\n",
    "    Normalize a 1D NumPy array to have values between 0 and 1.\n",
    "    \"\"\"\n",
    "    arr_min = np.min(arr)\n",
    "    arr_max = np.max(arr)\n",
    "    normalized_arr = (arr - arr_min) / (arr_max - arr_min)\n",
    "    return normalized_arr\n",
    "\n",
    "\n",
    "\n",
    "'''Obtain Dataset Images (image, label)'''\n",
    "\n",
    "\n",
    "mean = (.1706)\n",
    "std = (.2112)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "args = 'C:/Users/jgril/Documents/GitHub/8803_Final_Project'\n",
    "image_trainset = OCTDataset(args, 'train', transform=transform)\n",
    "image_testset = OCTDataset(args, 'test', transform=transform)\n",
    "\n",
    "\n",
    "print(\"loading data\")\n",
    "\n",
    "images_train = []\n",
    "labels_train = []\n",
    "\n",
    "images_o_train = []\n",
    "labels_o_train = []\n",
    "\n",
    "for i in range(len(image_trainset)):\n",
    "    #flattened training dataset\n",
    "    image,label = image_trainset.__getitem__(i)\n",
    "    my_image = normalize_array(image.flatten().numpy())\n",
    "    images_train.append(my_image)\n",
    "    labels_train.append(label)\n",
    "\n",
    "    #non-flattened training dataset\n",
    "    image_o ,label_o = image_trainset.__getitem__(i)\n",
    "    my_image_o = torch.from_numpy(normalize_array(image_o.numpy()))\n",
    "    images_o_train.append(my_image_o)\n",
    "    labels_o_train.append(label_o)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "\n",
    "images_o_test = []\n",
    "labels_o_test = []\n",
    "\n",
    "for i in range(len(image_testset)):\n",
    "    #flattened training dataset\n",
    "    image,label = image_trainset.__getitem__(i)\n",
    "    my_image = normalize_array(image.flatten().numpy())\n",
    "    #print(my_image)\n",
    "    images_test.append(my_image)\n",
    "    labels_test.append(label)\n",
    "\n",
    "    #non-flattened training dataset\n",
    "    image_o ,label_o = image_trainset.__getitem__(i)\n",
    "    my_image_o = torch.from_numpy(normalize_array(image_o.numpy()))\n",
    "    images_o_test.append(my_image_o)\n",
    "    labels_o_test.append(label_o)\n",
    "\n",
    "print(\"data loaded\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Meta Train Data'''\n",
    "meta_trainset = pd.read_csv(train).dropna()\n",
    "meta_testset = pd.read_csv(test).dropna()\n",
    "\n",
    "train = os.path.join(root, \"df_prime_train.csv\")\n",
    "test = os.path.join(root, \"df_prime_test.csv\")\n",
    "\n",
    "meta_X_train = meta_trainset[['Age', 'Gender', 'Race', \"Diabetes_Type\", \"Diabetes_Years\", \"BMI\", \"BCVA\", \"CST\", \"Leakage_Index\"]].to_numpy()\n",
    "meta_Y_train = meta_trainset[[\"DRSS\"]].to_numpy()\n",
    "y = []\n",
    "for i in range(meta_Y_train.shape[0]):\n",
    "    y.append(LABELS_SEVERITY[int(meta_Y_train[i])])\n",
    "\n",
    "meta_Y_train = np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "'''Meta Test Data'''\n",
    "meta_X_test = meta_testset[['Age', 'Gender', 'Race', \"Diabetes_Type\", \"Diabetes_Years\", \"BMI\", \"BCVA\", \"CST\", \"Leakage_Index\"]].to_numpy()\n",
    "meta_Y_test = meta_testset[[\"DRSS\"]].to_numpy() \n",
    "y = []\n",
    "for i in range(meta_Y_test.shape[0]):\n",
    "    y.append(LABELS_SEVERITY[int(meta_Y_test[i])])\n",
    "\n",
    "Y_test = np.array(y)\n",
    "\n",
    "\n",
    "#print(meta_X_train)\n",
    "#print(meta_Y_train)\n",
    "#print(meta_X_test)\n",
    "#print(meta_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened trained dataset\n",
      "[0.032      0.01999999 0.028      ... 0.         0.         0.        ]\n",
      "(50176,)\n",
      "2\n",
      "\n",
      "Flattened test dataset\n",
      "[0.032      0.01999999 0.028      ... 0.         0.         0.        ]\n",
      "(50176,)\n",
      "2\n",
      "\n",
      "Original trained dataset\n",
      "tensor([[[0.0320, 0.0200, 0.0280,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1320, 0.0760, 0.0960,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0720, 0.0400, 0.0360,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0400, 0.0080, 0.0040,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0160, 0.0040, 0.0080,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0040, 0.0080,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "torch.Size([1, 224, 224])\n",
      "2\n",
      "\n",
      "Original test dataset\n",
      "tensor([[[0.0320, 0.0200, 0.0280,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1320, 0.0760, 0.0960,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0720, 0.0400, 0.0360,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0400, 0.0080, 0.0040,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0160, 0.0040, 0.0080,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0040, 0.0080,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "torch.Size([1, 224, 224])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# evaluate dataset\n",
    "print(\"Flattened trained dataset\")\n",
    "print(images_train[0])\n",
    "print(images_train[0].shape)\n",
    "print(labels_train[0])\n",
    "print()\n",
    "\n",
    "print(\"Flattened test dataset\")\n",
    "print(images_test[0])\n",
    "print(images_test[0].shape)\n",
    "print(labels_test[0])\n",
    "print()\n",
    "\n",
    "print(\"Original trained dataset\")\n",
    "print(images_o_train[0])\n",
    "print(images_o_train[0].shape)\n",
    "print(labels_o_train[0])\n",
    "print()\n",
    "\n",
    "print(\"Original test dataset\")\n",
    "print(images_o_test[0])\n",
    "print(images_o_test[0].shape)\n",
    "print(labels_o_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44347063978965817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#print(type(images_train[0]))\n",
    "#print(images_train[0])\n",
    "X_train = images_train\n",
    "y_train = labels_train\n",
    "\n",
    "X_test = images_test\n",
    "y_test = labels_test\n",
    "\n",
    "\n",
    "# train a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the classifier on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "\n",
    "X_train = np.array(images_train).reshape((len(images_train), -1))\n",
    "y_train = np.array(labels_train)\n",
    "\n",
    "X_test = np.array(images_test).reshape((len(images_test), -1))\n",
    "Y_test = np.array(labels_test)\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "\n",
    "# Create the SVM model\n",
    "model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the SVM model\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "train_batch_size = 100\n",
    "test_batch_size = 1\n",
    "image_trainset\n",
    "trainloader = DataLoader(image_trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "testloader = DataLoader(image_testset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "class MySimpleCNN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Inititalizes the various layers in the network\"\"\"\n",
    "    super(MySimpleCNN, self).__init__()\n",
    "    ##TODO\n",
    "    self.cnn1 = nn.Conv2d(1,16, kernel_size = 3, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.cnn2 = nn.Conv2d(16,32, kernel_size = 3, padding=1)\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size = 2)\n",
    "    self.linear = nn.Linear(401408,3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Processes the input from the dataloaders to return predicted output \n",
    "    probability vectors for each example in the batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.tensor, shape(batch_size, 1, 8, 8), dtype torch.float\n",
    "      output from dataloade containing batch_size number of 8 x 8 images as \n",
    "      torch tensors\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : torch.tensor, shape (batch_size,10), dtype= torch.float.\n",
    "      batch_size number of 10-element vectors for each image in the input batch.  \n",
    "    \"\"\"\n",
    "    ##TODO\n",
    "    x1 = self.relu(self.cnn1(x))\n",
    "    x2 = self.relu(self.cnn2(x1))\n",
    "    x3 = self.maxpool(x2)\n",
    "    out = self.linear(x3.reshape(x.shape[0],-1))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Training Loss : 1.0723\n",
      "Epoch : 0 | Training Loss : 41.4669\n",
      "Epoch : 0 | Training Loss : 22.8374\n",
      "Epoch : 0 | Training Loss : 8.9478\n",
      "Epoch : 0 | Training Loss : 7.8341\n",
      "Epoch : 0 | Training Loss : 4.3709\n",
      "Epoch : 0 | Training Loss : 2.2446\n",
      "Epoch : 0 | Training Loss : 3.2174\n",
      "Epoch : 0 | Training Loss : 3.2440\n",
      "Epoch : 0 | Training Loss : 1.6683\n",
      "Epoch : 0 | Training Loss : 1.6667\n",
      "Epoch : 0 | Training Loss : 1.5963\n",
      "Epoch : 0 | Training Loss : 1.9625\n",
      "Epoch : 0 | Training Loss : 1.6007\n",
      "Epoch : 0 | Training Loss : 1.2592\n",
      "Epoch : 0 | Training Loss : 0.9202\n",
      "Epoch : 0 | Training Loss : 0.9459\n",
      "Epoch : 0 | Training Loss : 1.2290\n",
      "Epoch : 0 | Training Loss : 1.2860\n",
      "Epoch : 0 | Training Loss : 1.1468\n",
      "Epoch : 0 | Training Loss : 1.0085\n",
      "Epoch : 0 | Training Loss : 0.9737\n",
      "Epoch : 0 | Training Loss : 0.9725\n",
      "Epoch : 0 | Training Loss : 1.1072\n",
      "Epoch : 0 | Training Loss : 0.9762\n",
      "Epoch : 0 | Training Loss : 0.9565\n",
      "Epoch : 0 | Training Loss : 0.9312\n",
      "Epoch : 0 | Training Loss : 0.9663\n",
      "Epoch : 0 | Training Loss : 0.9270\n",
      "Epoch : 0 | Training Loss : 0.9447\n",
      "Epoch : 0 | Training Loss : 0.9126\n",
      "Epoch : 0 | Training Loss : 0.8441\n",
      "Epoch : 0 | Training Loss : 0.8030\n",
      "Epoch : 0 | Training Loss : 0.8877\n",
      "Epoch : 0 | Training Loss : 0.6988\n",
      "Epoch : 0 | Training Loss : 0.9140\n",
      "Epoch : 0 | Training Loss : 0.9087\n",
      "Epoch : 0 | Training Loss : 0.7949\n",
      "Epoch : 0 | Training Loss : 0.8013\n",
      "Epoch : 0 | Training Loss : 0.8139\n",
      "Epoch : 0 | Training Loss : 0.8102\n",
      "Epoch : 0 | Training Loss : 0.8216\n",
      "Epoch : 0 | Training Loss : 0.8961\n",
      "Epoch : 0 | Training Loss : 0.7269\n",
      "Epoch : 0 | Training Loss : 0.7771\n",
      "Epoch : 0 | Training Loss : 0.7950\n",
      "Epoch : 0 | Training Loss : 0.8489\n",
      "Epoch : 0 | Training Loss : 0.7688\n",
      "Epoch : 0 | Training Loss : 0.7426\n",
      "Epoch : 0 | Training Loss : 0.7652\n",
      "Epoch : 0 | Training Loss : 0.6845\n",
      "Epoch : 0 | Training Loss : 0.7595\n",
      "Epoch : 0 | Training Loss : 0.7749\n",
      "Epoch : 0 | Training Loss : 0.8115\n",
      "Epoch : 0 | Training Loss : 0.8344\n",
      "Epoch : 0 | Training Loss : 0.7143\n",
      "Epoch : 0 | Training Loss : 0.6963\n",
      "Epoch : 0 | Training Loss : 0.7279\n",
      "Epoch : 0 | Training Loss : 0.7668\n",
      "Epoch : 0 | Training Loss : 0.7526\n",
      "Epoch : 0 | Training Loss : 0.7214\n",
      "Epoch : 0 | Training Loss : 0.7782\n",
      "Epoch : 0 | Training Loss : 0.7318\n",
      "Epoch : 0 | Training Loss : 0.7573\n",
      "Epoch : 0 | Training Loss : 0.7153\n",
      "Epoch : 0 | Training Loss : 0.7148\n",
      "Epoch : 0 | Training Loss : 0.7569\n",
      "Epoch : 0 | Training Loss : 0.8871\n",
      "Epoch : 0 | Training Loss : 0.6606\n",
      "Epoch : 0 | Training Loss : 0.7248\n",
      "Epoch : 0 | Training Loss : 0.7558\n",
      "Epoch : 0 | Training Loss : 0.6170\n",
      "Epoch : 0 | Training Loss : 0.7224\n",
      "Epoch : 0 | Training Loss : 0.8031\n",
      "Epoch : 0 | Training Loss : 0.7683\n",
      "Epoch : 0 | Training Loss : 0.7616\n",
      "Epoch : 0 | Training Loss : 0.7083\n",
      "Epoch : 0 | Training Loss : 0.6950\n",
      "Epoch : 0 | Training Loss : 0.6449\n",
      "Epoch : 0 | Training Loss : 0.6896\n",
      "Epoch : 0 | Training Loss : 0.6662\n",
      "Epoch : 0 | Training Loss : 0.7882\n",
      "Epoch : 0 | Training Loss : 0.6134\n",
      "Epoch : 0 | Training Loss : 0.5723\n",
      "Epoch : 0 | Training Loss : 0.7436\n",
      "Epoch : 0 | Training Loss : 0.6393\n",
      "Epoch : 0 | Training Loss : 0.7151\n",
      "Epoch : 0 | Training Loss : 0.5602\n",
      "Epoch : 0 | Training Loss : 0.7071\n",
      "Epoch : 0 | Training Loss : 0.7208\n",
      "Epoch : 0 | Training Loss : 0.5971\n",
      "Epoch : 0 | Training Loss : 0.6763\n",
      "Epoch : 0 | Training Loss : 0.7265\n",
      "Epoch : 0 | Training Loss : 0.5835\n",
      "Epoch : 0 | Training Loss : 0.6569\n",
      "Epoch : 0 | Training Loss : 0.6822\n",
      "Epoch : 0 | Training Loss : 0.6651\n",
      "Epoch : 0 | Training Loss : 0.7579\n",
      "Epoch : 0 | Training Loss : 0.6117\n",
      "Epoch : 0 | Training Loss : 0.6487\n",
      "Epoch : 0 | Training Loss : 0.7882\n",
      "Epoch : 0 | Training Loss : 0.6931\n",
      "Epoch : 0 | Training Loss : 0.5774\n",
      "Epoch : 0 | Training Loss : 0.6493\n",
      "Epoch : 0 | Training Loss : 0.6626\n",
      "Epoch : 0 | Training Loss : 0.7131\n",
      "Epoch : 0 | Training Loss : 0.7078\n",
      "Epoch : 0 | Training Loss : 0.6385\n",
      "Epoch : 0 | Training Loss : 0.6425\n",
      "Epoch : 0 | Training Loss : 0.7161\n",
      "Epoch : 0 | Training Loss : 0.6980\n",
      "Epoch : 0 | Training Loss : 0.6062\n",
      "Epoch : 0 | Training Loss : 0.6650\n",
      "Epoch : 0 | Training Loss : 0.6730\n",
      "Epoch : 0 | Training Loss : 0.5489\n",
      "Epoch : 0 | Training Loss : 0.6839\n",
      "Epoch : 0 | Training Loss : 0.5606\n",
      "Epoch : 0 | Training Loss : 0.6475\n",
      "Epoch : 0 | Training Loss : 0.5821\n",
      "Epoch : 0 | Training Loss : 0.6589\n",
      "Epoch : 0 | Training Loss : 0.6621\n",
      "Epoch : 0 | Training Loss : 0.6260\n",
      "Epoch : 0 | Training Loss : 0.6037\n",
      "Epoch : 0 | Training Loss : 0.5945\n",
      "Epoch : 0 | Training Loss : 0.8049\n",
      "Epoch : 0 | Training Loss : 0.5736\n",
      "Epoch : 0 | Training Loss : 0.6086\n",
      "Epoch : 0 | Training Loss : 0.7196\n",
      "Epoch : 0 | Training Loss : 0.6231\n",
      "Epoch : 0 | Training Loss : 0.5675\n",
      "Epoch : 0 | Training Loss : 0.6346\n",
      "Epoch : 0 | Training Loss : 0.5833\n",
      "Epoch : 0 | Training Loss : 0.5070\n",
      "Epoch : 0 | Training Loss : 0.5499\n",
      "Epoch : 0 | Training Loss : 0.6261\n",
      "Epoch : 0 | Training Loss : 0.6053\n",
      "Epoch : 0 | Training Loss : 0.5636\n",
      "Epoch : 0 | Training Loss : 0.6575\n",
      "Epoch : 0 | Training Loss : 0.5847\n",
      "Epoch : 0 | Training Loss : 0.5575\n",
      "Epoch : 0 | Training Loss : 0.5844\n",
      "Epoch : 0 | Training Loss : 0.6502\n",
      "Epoch : 0 | Training Loss : 0.5064\n",
      "Epoch : 0 | Training Loss : 0.5163\n",
      "Epoch : 0 | Training Loss : 0.6524\n",
      "Epoch : 0 | Training Loss : 0.6401\n",
      "Epoch : 0 | Training Loss : 0.4487\n",
      "Epoch : 0 | Training Loss : 0.6961\n",
      "Epoch : 0 | Training Loss : 0.6468\n",
      "Epoch : 0 | Training Loss : 0.6773\n",
      "Epoch : 0 | Training Loss : 0.6398\n",
      "Epoch : 0 | Training Loss : 0.6295\n",
      "Epoch : 0 | Training Loss : 0.6726\n",
      "Epoch : 0 | Training Loss : 0.5199\n",
      "Epoch : 0 | Training Loss : 0.5692\n",
      "Epoch : 0 | Training Loss : 0.6078\n",
      "Epoch : 0 | Training Loss : 0.5220\n",
      "Epoch : 0 | Training Loss : 0.6306\n",
      "Epoch : 0 | Training Loss : 0.6214\n",
      "Epoch : 0 | Training Loss : 0.6123\n",
      "Epoch : 0 | Training Loss : 0.6204\n",
      "Epoch : 0 | Training Loss : 0.5939\n",
      "Epoch : 0 | Training Loss : 0.4928\n",
      "Epoch : 0 | Training Loss : 0.5900\n",
      "Epoch : 0 | Training Loss : 0.7079\n",
      "Epoch : 0 | Training Loss : 0.6348\n",
      "Epoch : 0 | Training Loss : 0.6339\n",
      "Epoch : 0 | Training Loss : 0.5902\n",
      "Epoch : 0 | Training Loss : 0.6005\n",
      "Epoch : 0 | Training Loss : 0.5668\n",
      "Epoch : 0 | Training Loss : 0.6097\n",
      "Epoch : 0 | Training Loss : 0.6376\n",
      "Epoch : 0 | Training Loss : 0.6700\n",
      "Epoch : 0 | Training Loss : 0.5870\n",
      "Epoch : 0 | Training Loss : 0.5696\n",
      "Epoch : 0 | Training Loss : 0.6434\n",
      "Epoch : 0 | Training Loss : 0.5561\n",
      "Epoch : 0 | Training Loss : 0.7104\n",
      "Epoch : 0 | Training Loss : 0.7254\n",
      "Epoch : 0 | Training Loss : 0.6284\n",
      "Epoch : 0 | Training Loss : 0.5454\n",
      "Epoch : 0 | Training Loss : 0.5774\n",
      "Epoch : 0 | Training Loss : 0.7507\n",
      "Epoch : 0 | Training Loss : 0.5806\n",
      "Epoch : 0 | Training Loss : 0.5089\n",
      "Epoch : 0 | Training Loss : 0.5804\n",
      "Epoch : 0 | Training Loss : 0.5530\n",
      "Epoch : 0 | Training Loss : 0.4910\n",
      "Epoch : 0 | Training Loss : 0.5927\n",
      "Epoch : 0 | Training Loss : 0.6085\n",
      "Epoch : 0 | Training Loss : 0.6154\n",
      "Epoch : 0 | Training Loss : 0.6084\n",
      "Epoch : 0 | Training Loss : 0.6736\n",
      "Epoch : 0 | Training Loss : 0.5876\n",
      "Epoch : 0 | Training Loss : 0.5265\n",
      "Epoch : 0 | Training Loss : 0.6019\n",
      "Epoch : 0 | Training Loss : 0.6009\n",
      "Epoch : 0 | Training Loss : 0.7068\n",
      "Epoch : 0 | Training Loss : 0.6529\n",
      "Epoch : 0 | Training Loss : 0.6708\n",
      "Epoch : 0 | Training Loss : 0.6401\n",
      "Epoch : 0 | Training Loss : 0.6335\n",
      "Epoch : 0 | Training Loss : 0.6387\n",
      "Epoch : 0 | Training Loss : 0.6176\n",
      "Epoch : 0 | Training Loss : 0.6434\n",
      "Epoch : 0 | Training Loss : 0.5597\n",
      "Epoch : 0 | Training Loss : 0.6005\n",
      "Epoch : 0 | Training Loss : 0.5920\n",
      "Epoch : 0 | Training Loss : 0.4952\n",
      "Epoch : 0 | Training Loss : 0.5880\n",
      "Epoch : 0 | Training Loss : 0.6399\n",
      "Epoch : 0 | Training Loss : 0.6846\n",
      "Epoch : 0 | Training Loss : 0.5200\n",
      "Epoch : 0 | Training Loss : 0.6030\n",
      "Epoch : 0 | Training Loss : 0.6028\n",
      "Epoch : 0 | Training Loss : 0.4884\n",
      "Epoch : 0 | Training Loss : 0.5671\n",
      "Epoch : 0 | Training Loss : 0.5166\n",
      "Epoch : 0 | Training Loss : 0.5057\n",
      "Epoch : 0 | Training Loss : 0.5477\n",
      "Epoch : 0 | Training Loss : 0.4721\n",
      "Epoch : 0 | Training Loss : 0.5630\n",
      "Epoch : 0 | Training Loss : 0.5400\n",
      "Epoch : 0 | Training Loss : 0.5508\n",
      "Epoch : 0 | Training Loss : 0.5453\n",
      "Epoch : 0 | Training Loss : 0.6043\n",
      "Epoch : 0 | Training Loss : 0.5506\n",
      "Epoch : 0 | Training Loss : 0.5044\n",
      "Epoch : 0 | Training Loss : 0.7939\n",
      "Epoch : 0 | Training Loss : 0.5405\n",
      "Epoch : 0 | Training Loss : 0.5171\n",
      "Epoch : 0 | Training Loss : 0.6278\n",
      "Epoch : 0 | Training Loss : 0.4954\n",
      "Epoch : 0 | Training Loss : 0.5838\n",
      "Epoch : 0 | Training Loss : 0.6998\n",
      "Epoch : 0 | Training Loss : 0.6331\n",
      "Epoch : 0 | Training Loss : 0.6284\n",
      "Epoch : 0 | Training Loss : 0.6183\n",
      "Epoch : 0 | Training Loss : 0.5433\n",
      "Epoch : 0 | Training Loss : 0.6792\n",
      "Epoch : 0 | Training Loss : 0.5677\n",
      "Epoch : 0 | Training Loss : 0.6207\n",
      "Epoch : 0 | Training Loss : 0.7626\n",
      "Epoch : 1 | Training Loss : 0.5713\n",
      "Epoch : 1 | Training Loss : 0.5537\n",
      "Epoch : 1 | Training Loss : 0.5794\n",
      "Epoch : 1 | Training Loss : 0.5479\n",
      "Epoch : 1 | Training Loss : 0.4982\n",
      "Epoch : 1 | Training Loss : 0.6314\n",
      "Epoch : 1 | Training Loss : 0.6519\n",
      "Epoch : 1 | Training Loss : 0.4875\n",
      "Epoch : 1 | Training Loss : 0.4887\n",
      "Epoch : 1 | Training Loss : 0.5340\n",
      "Epoch : 1 | Training Loss : 0.4330\n",
      "Epoch : 1 | Training Loss : 0.4863\n",
      "Epoch : 1 | Training Loss : 0.5581\n",
      "Epoch : 1 | Training Loss : 0.4821\n",
      "Epoch : 1 | Training Loss : 0.4855\n",
      "Epoch : 1 | Training Loss : 0.4962\n",
      "Epoch : 1 | Training Loss : 0.6123\n",
      "Epoch : 1 | Training Loss : 0.5897\n",
      "Epoch : 1 | Training Loss : 0.5442\n",
      "Epoch : 1 | Training Loss : 0.4429\n",
      "Epoch : 1 | Training Loss : 0.4804\n",
      "Epoch : 1 | Training Loss : 0.4345\n",
      "Epoch : 1 | Training Loss : 0.4632\n",
      "Epoch : 1 | Training Loss : 0.5445\n",
      "Epoch : 1 | Training Loss : 0.5335\n",
      "Epoch : 1 | Training Loss : 0.5052\n",
      "Epoch : 1 | Training Loss : 0.5537\n",
      "Epoch : 1 | Training Loss : 0.6241\n",
      "Epoch : 1 | Training Loss : 0.5008\n",
      "Epoch : 1 | Training Loss : 0.4325\n",
      "Epoch : 1 | Training Loss : 0.5210\n",
      "Epoch : 1 | Training Loss : 0.4285\n",
      "Epoch : 1 | Training Loss : 0.5264\n",
      "Epoch : 1 | Training Loss : 0.4931\n",
      "Epoch : 1 | Training Loss : 0.4033\n",
      "Epoch : 1 | Training Loss : 0.4622\n",
      "Epoch : 1 | Training Loss : 0.4529\n",
      "Epoch : 1 | Training Loss : 0.4444\n",
      "Epoch : 1 | Training Loss : 0.5496\n",
      "Epoch : 1 | Training Loss : 0.5969\n",
      "Epoch : 1 | Training Loss : 0.5366\n",
      "Epoch : 1 | Training Loss : 0.4788\n",
      "Epoch : 1 | Training Loss : 0.4673\n",
      "Epoch : 1 | Training Loss : 0.5692\n",
      "Epoch : 1 | Training Loss : 0.4924\n",
      "Epoch : 1 | Training Loss : 0.5332\n",
      "Epoch : 1 | Training Loss : 0.4064\n",
      "Epoch : 1 | Training Loss : 0.4635\n",
      "Epoch : 1 | Training Loss : 0.4089\n",
      "Epoch : 1 | Training Loss : 0.5039\n",
      "Epoch : 1 | Training Loss : 0.6173\n",
      "Epoch : 1 | Training Loss : 0.5942\n",
      "Epoch : 1 | Training Loss : 0.4815\n",
      "Epoch : 1 | Training Loss : 0.5657\n",
      "Epoch : 1 | Training Loss : 0.4358\n",
      "Epoch : 1 | Training Loss : 0.5239\n",
      "Epoch : 1 | Training Loss : 0.5331\n",
      "Epoch : 1 | Training Loss : 0.5863\n",
      "Epoch : 1 | Training Loss : 0.4343\n",
      "Epoch : 1 | Training Loss : 0.4619\n",
      "Epoch : 1 | Training Loss : 0.5971\n",
      "Epoch : 1 | Training Loss : 0.5485\n",
      "Epoch : 1 | Training Loss : 0.4142\n",
      "Epoch : 1 | Training Loss : 0.4837\n",
      "Epoch : 1 | Training Loss : 0.3923\n",
      "Epoch : 1 | Training Loss : 0.4919\n",
      "Epoch : 1 | Training Loss : 0.4885\n",
      "Epoch : 1 | Training Loss : 0.6402\n",
      "Epoch : 1 | Training Loss : 0.5191\n",
      "Epoch : 1 | Training Loss : 0.4327\n",
      "Epoch : 1 | Training Loss : 0.4878\n",
      "Epoch : 1 | Training Loss : 0.5251\n",
      "Epoch : 1 | Training Loss : 0.5164\n",
      "Epoch : 1 | Training Loss : 0.4527\n",
      "Epoch : 1 | Training Loss : 0.5252\n",
      "Epoch : 1 | Training Loss : 0.4572\n",
      "Epoch : 1 | Training Loss : 0.5600\n",
      "Epoch : 1 | Training Loss : 0.4609\n",
      "Epoch : 1 | Training Loss : 0.4065\n",
      "Epoch : 1 | Training Loss : 0.5989\n",
      "Epoch : 1 | Training Loss : 0.4828\n",
      "Epoch : 1 | Training Loss : 0.5060\n",
      "Epoch : 1 | Training Loss : 0.4481\n",
      "Epoch : 1 | Training Loss : 0.5005\n",
      "Epoch : 1 | Training Loss : 0.4896\n",
      "Epoch : 1 | Training Loss : 0.4784\n",
      "Epoch : 1 | Training Loss : 0.4724\n",
      "Epoch : 1 | Training Loss : 0.4805\n",
      "Epoch : 1 | Training Loss : 0.4576\n",
      "Epoch : 1 | Training Loss : 0.5226\n",
      "Epoch : 1 | Training Loss : 0.5039\n",
      "Epoch : 1 | Training Loss : 0.4910\n",
      "Epoch : 1 | Training Loss : 0.5320\n",
      "Epoch : 1 | Training Loss : 0.4799\n",
      "Epoch : 1 | Training Loss : 0.4212\n",
      "Epoch : 1 | Training Loss : 0.5872\n",
      "Epoch : 1 | Training Loss : 0.4907\n",
      "Epoch : 1 | Training Loss : 0.4833\n",
      "Epoch : 1 | Training Loss : 0.4648\n",
      "Epoch : 1 | Training Loss : 0.4412\n",
      "Epoch : 1 | Training Loss : 0.4646\n",
      "Epoch : 1 | Training Loss : 0.4846\n",
      "Epoch : 1 | Training Loss : 0.4232\n",
      "Epoch : 1 | Training Loss : 0.3992\n",
      "Epoch : 1 | Training Loss : 0.5470\n",
      "Epoch : 1 | Training Loss : 0.5117\n",
      "Epoch : 1 | Training Loss : 0.5564\n",
      "Epoch : 1 | Training Loss : 0.4801\n",
      "Epoch : 1 | Training Loss : 0.5270\n",
      "Epoch : 1 | Training Loss : 0.5159\n",
      "Epoch : 1 | Training Loss : 0.4691\n",
      "Epoch : 1 | Training Loss : 0.3712\n",
      "Epoch : 1 | Training Loss : 0.5242\n",
      "Epoch : 1 | Training Loss : 0.5070\n",
      "Epoch : 1 | Training Loss : 0.4659\n",
      "Epoch : 1 | Training Loss : 0.4330\n",
      "Epoch : 1 | Training Loss : 0.4770\n",
      "Epoch : 1 | Training Loss : 0.6074\n",
      "Epoch : 1 | Training Loss : 0.5770\n",
      "Epoch : 1 | Training Loss : 0.5236\n",
      "Epoch : 1 | Training Loss : 0.4305\n",
      "Epoch : 1 | Training Loss : 0.4508\n",
      "Epoch : 1 | Training Loss : 0.3485\n",
      "Epoch : 1 | Training Loss : 0.4887\n",
      "Epoch : 1 | Training Loss : 0.5043\n",
      "Epoch : 1 | Training Loss : 0.4155\n",
      "Epoch : 1 | Training Loss : 0.5786\n",
      "Epoch : 1 | Training Loss : 0.4358\n",
      "Epoch : 1 | Training Loss : 0.4834\n",
      "Epoch : 1 | Training Loss : 0.5072\n",
      "Epoch : 1 | Training Loss : 0.4142\n",
      "Epoch : 1 | Training Loss : 0.6140\n",
      "Epoch : 1 | Training Loss : 0.3943\n",
      "Epoch : 1 | Training Loss : 0.5690\n",
      "Epoch : 1 | Training Loss : 0.4711\n",
      "Epoch : 1 | Training Loss : 0.4834\n",
      "Epoch : 1 | Training Loss : 0.4528\n",
      "Epoch : 1 | Training Loss : 0.5367\n",
      "Epoch : 1 | Training Loss : 0.3990\n",
      "Epoch : 1 | Training Loss : 0.5194\n",
      "Epoch : 1 | Training Loss : 0.4006\n",
      "Epoch : 1 | Training Loss : 0.5276\n",
      "Epoch : 1 | Training Loss : 0.4510\n",
      "Epoch : 1 | Training Loss : 0.5846\n",
      "Epoch : 1 | Training Loss : 0.5017\n",
      "Epoch : 1 | Training Loss : 0.4741\n",
      "Epoch : 1 | Training Loss : 0.5032\n",
      "Epoch : 1 | Training Loss : 0.5149\n",
      "Epoch : 1 | Training Loss : 0.4418\n",
      "Epoch : 1 | Training Loss : 0.5192\n",
      "Epoch : 1 | Training Loss : 0.3288\n",
      "Epoch : 1 | Training Loss : 0.4744\n",
      "Epoch : 1 | Training Loss : 0.5202\n",
      "Epoch : 1 | Training Loss : 0.4477\n",
      "Epoch : 1 | Training Loss : 0.4753\n",
      "Epoch : 1 | Training Loss : 0.6406\n",
      "Epoch : 1 | Training Loss : 0.4431\n",
      "Epoch : 1 | Training Loss : 0.4673\n",
      "Epoch : 1 | Training Loss : 0.4404\n",
      "Epoch : 1 | Training Loss : 0.4850\n",
      "Epoch : 1 | Training Loss : 0.4642\n",
      "Epoch : 1 | Training Loss : 0.4323\n",
      "Epoch : 1 | Training Loss : 0.4705\n",
      "Epoch : 1 | Training Loss : 0.4740\n",
      "Epoch : 1 | Training Loss : 0.4469\n",
      "Epoch : 1 | Training Loss : 0.4752\n",
      "Epoch : 1 | Training Loss : 0.4742\n",
      "Epoch : 1 | Training Loss : 0.5685\n",
      "Epoch : 1 | Training Loss : 0.5312\n",
      "Epoch : 1 | Training Loss : 0.4546\n",
      "Epoch : 1 | Training Loss : 0.4200\n",
      "Epoch : 1 | Training Loss : 0.5004\n",
      "Epoch : 1 | Training Loss : 0.4972\n",
      "Epoch : 1 | Training Loss : 0.5398\n",
      "Epoch : 1 | Training Loss : 0.4968\n",
      "Epoch : 1 | Training Loss : 0.5736\n",
      "Epoch : 1 | Training Loss : 0.4635\n",
      "Epoch : 1 | Training Loss : 0.4426\n",
      "Epoch : 1 | Training Loss : 0.5131\n",
      "Epoch : 1 | Training Loss : 0.6107\n",
      "Epoch : 1 | Training Loss : 0.5118\n",
      "Epoch : 1 | Training Loss : 0.3879\n",
      "Epoch : 1 | Training Loss : 0.4887\n",
      "Epoch : 1 | Training Loss : 0.6120\n",
      "Epoch : 1 | Training Loss : 0.4309\n",
      "Epoch : 1 | Training Loss : 0.4279\n",
      "Epoch : 1 | Training Loss : 0.6289\n",
      "Epoch : 1 | Training Loss : 0.4762\n",
      "Epoch : 1 | Training Loss : 0.4280\n",
      "Epoch : 1 | Training Loss : 0.4192\n",
      "Epoch : 1 | Training Loss : 0.4711\n",
      "Epoch : 1 | Training Loss : 0.5080\n",
      "Epoch : 1 | Training Loss : 0.5525\n",
      "Epoch : 1 | Training Loss : 0.5486\n",
      "Epoch : 1 | Training Loss : 0.5369\n",
      "Epoch : 1 | Training Loss : 0.4258\n",
      "Epoch : 1 | Training Loss : 0.4927\n",
      "Epoch : 1 | Training Loss : 0.5243\n",
      "Epoch : 1 | Training Loss : 0.5171\n",
      "Epoch : 1 | Training Loss : 0.5576\n",
      "Epoch : 1 | Training Loss : 0.4568\n",
      "Epoch : 1 | Training Loss : 0.5571\n",
      "Epoch : 1 | Training Loss : 0.4835\n",
      "Epoch : 1 | Training Loss : 0.4358\n",
      "Epoch : 1 | Training Loss : 0.4731\n",
      "Epoch : 1 | Training Loss : 0.5370\n",
      "Epoch : 1 | Training Loss : 0.4302\n",
      "Epoch : 1 | Training Loss : 0.4068\n",
      "Epoch : 1 | Training Loss : 0.5195\n",
      "Epoch : 1 | Training Loss : 0.3982\n",
      "Epoch : 1 | Training Loss : 0.4110\n",
      "Epoch : 1 | Training Loss : 0.4671\n",
      "Epoch : 1 | Training Loss : 0.5267\n",
      "Epoch : 1 | Training Loss : 0.5306\n",
      "Epoch : 1 | Training Loss : 0.4710\n",
      "Epoch : 1 | Training Loss : 0.5763\n",
      "Epoch : 1 | Training Loss : 0.4498\n",
      "Epoch : 1 | Training Loss : 0.3572\n",
      "Epoch : 1 | Training Loss : 0.4466\n",
      "Epoch : 1 | Training Loss : 0.3740\n",
      "Epoch : 1 | Training Loss : 0.5008\n",
      "Epoch : 1 | Training Loss : 0.5560\n",
      "Epoch : 1 | Training Loss : 0.5556\n",
      "Epoch : 1 | Training Loss : 0.5066\n",
      "Epoch : 1 | Training Loss : 0.4887\n",
      "Epoch : 1 | Training Loss : 0.5021\n",
      "Epoch : 1 | Training Loss : 0.4203\n",
      "Epoch : 1 | Training Loss : 0.4799\n",
      "Epoch : 1 | Training Loss : 0.4824\n",
      "Epoch : 1 | Training Loss : 0.5448\n",
      "Epoch : 1 | Training Loss : 0.4822\n",
      "Epoch : 1 | Training Loss : 0.5326\n",
      "Epoch : 1 | Training Loss : 0.4412\n",
      "Epoch : 1 | Training Loss : 0.4604\n",
      "Epoch : 1 | Training Loss : 0.4464\n",
      "Epoch : 1 | Training Loss : 0.4498\n",
      "Epoch : 1 | Training Loss : 0.4327\n",
      "Epoch : 1 | Training Loss : 0.4795\n",
      "Epoch : 1 | Training Loss : 0.5319\n",
      "Epoch : 1 | Training Loss : 0.4293\n",
      "Epoch : 1 | Training Loss : 0.3651\n",
      "Epoch : 1 | Training Loss : 0.6067\n",
      "Epoch : 1 | Training Loss : 0.6264\n",
      "Epoch : 2 | Training Loss : 0.4160\n",
      "Epoch : 2 | Training Loss : 0.3809\n",
      "Epoch : 2 | Training Loss : 0.4194\n",
      "Epoch : 2 | Training Loss : 0.3522\n",
      "Epoch : 2 | Training Loss : 0.3683\n",
      "Epoch : 2 | Training Loss : 0.3957\n",
      "Epoch : 2 | Training Loss : 0.5007\n",
      "Epoch : 2 | Training Loss : 0.3659\n",
      "Epoch : 2 | Training Loss : 0.3868\n",
      "Epoch : 2 | Training Loss : 0.3031\n",
      "Epoch : 2 | Training Loss : 0.4591\n",
      "Epoch : 2 | Training Loss : 0.3696\n",
      "Epoch : 2 | Training Loss : 0.3926\n",
      "Epoch : 2 | Training Loss : 0.3110\n",
      "Epoch : 2 | Training Loss : 0.3298\n",
      "Epoch : 2 | Training Loss : 0.3213\n",
      "Epoch : 2 | Training Loss : 0.3988\n",
      "Epoch : 2 | Training Loss : 0.3751\n",
      "Epoch : 2 | Training Loss : 0.2979\n",
      "Epoch : 2 | Training Loss : 0.2989\n",
      "Epoch : 2 | Training Loss : 0.5194\n",
      "Epoch : 2 | Training Loss : 0.3184\n",
      "Epoch : 2 | Training Loss : 0.3620\n",
      "Epoch : 2 | Training Loss : 0.3540\n",
      "Epoch : 2 | Training Loss : 0.3340\n",
      "Epoch : 2 | Training Loss : 0.3467\n",
      "Epoch : 2 | Training Loss : 0.4730\n",
      "Epoch : 2 | Training Loss : 0.3434\n",
      "Epoch : 2 | Training Loss : 0.3748\n",
      "Epoch : 2 | Training Loss : 0.3160\n",
      "Epoch : 2 | Training Loss : 0.4009\n",
      "Epoch : 2 | Training Loss : 0.3877\n",
      "Epoch : 2 | Training Loss : 0.4891\n",
      "Epoch : 2 | Training Loss : 0.4229\n",
      "Epoch : 2 | Training Loss : 0.3363\n",
      "Epoch : 2 | Training Loss : 0.3867\n",
      "Epoch : 2 | Training Loss : 0.4192\n",
      "Epoch : 2 | Training Loss : 0.3600\n",
      "Epoch : 2 | Training Loss : 0.4577\n",
      "Epoch : 2 | Training Loss : 0.3233\n",
      "Epoch : 2 | Training Loss : 0.3451\n",
      "Epoch : 2 | Training Loss : 0.4208\n",
      "Epoch : 2 | Training Loss : 0.4418\n",
      "Epoch : 2 | Training Loss : 0.2957\n",
      "Epoch : 2 | Training Loss : 0.4074\n",
      "Epoch : 2 | Training Loss : 0.5531\n",
      "Epoch : 2 | Training Loss : 0.4496\n",
      "Epoch : 2 | Training Loss : 0.4115\n",
      "Epoch : 2 | Training Loss : 0.2993\n",
      "Epoch : 2 | Training Loss : 0.4816\n",
      "Epoch : 2 | Training Loss : 0.3313\n",
      "Epoch : 2 | Training Loss : 0.5307\n",
      "Epoch : 2 | Training Loss : 0.3517\n",
      "Epoch : 2 | Training Loss : 0.4069\n",
      "Epoch : 2 | Training Loss : 0.4326\n",
      "Epoch : 2 | Training Loss : 0.4145\n",
      "Epoch : 2 | Training Loss : 0.4923\n",
      "Epoch : 2 | Training Loss : 0.4051\n",
      "Epoch : 2 | Training Loss : 0.3955\n",
      "Epoch : 2 | Training Loss : 0.3869\n",
      "Epoch : 2 | Training Loss : 0.4019\n",
      "Epoch : 2 | Training Loss : 0.5239\n",
      "Epoch : 2 | Training Loss : 0.4478\n",
      "Epoch : 2 | Training Loss : 0.4163\n",
      "Epoch : 2 | Training Loss : 0.4785\n",
      "Epoch : 2 | Training Loss : 0.2986\n",
      "Epoch : 2 | Training Loss : 0.4924\n",
      "Epoch : 2 | Training Loss : 0.4903\n",
      "Epoch : 2 | Training Loss : 0.4283\n",
      "Epoch : 2 | Training Loss : 0.3828\n",
      "Epoch : 2 | Training Loss : 0.3056\n",
      "Epoch : 2 | Training Loss : 0.3885\n",
      "Epoch : 2 | Training Loss : 0.4187\n",
      "Epoch : 2 | Training Loss : 0.3864\n",
      "Epoch : 2 | Training Loss : 0.4160\n",
      "Epoch : 2 | Training Loss : 0.3319\n",
      "Epoch : 2 | Training Loss : 0.4500\n",
      "Epoch : 2 | Training Loss : 0.3829\n",
      "Epoch : 2 | Training Loss : 0.3122\n",
      "Epoch : 2 | Training Loss : 0.3482\n",
      "Epoch : 2 | Training Loss : 0.3470\n",
      "Epoch : 2 | Training Loss : 0.3483\n",
      "Epoch : 2 | Training Loss : 0.3150\n",
      "Epoch : 2 | Training Loss : 0.3704\n",
      "Epoch : 2 | Training Loss : 0.3497\n",
      "Epoch : 2 | Training Loss : 0.4249\n",
      "Epoch : 2 | Training Loss : 0.3692\n",
      "Epoch : 2 | Training Loss : 0.3686\n",
      "Epoch : 2 | Training Loss : 0.3094\n",
      "Epoch : 2 | Training Loss : 0.4209\n",
      "Epoch : 2 | Training Loss : 0.3489\n",
      "Epoch : 2 | Training Loss : 0.4067\n",
      "Epoch : 2 | Training Loss : 0.3524\n",
      "Epoch : 2 | Training Loss : 0.4224\n",
      "Epoch : 2 | Training Loss : 0.3894\n",
      "Epoch : 2 | Training Loss : 0.3549\n",
      "Epoch : 2 | Training Loss : 0.3298\n",
      "Epoch : 2 | Training Loss : 0.3889\n",
      "Epoch : 2 | Training Loss : 0.3608\n",
      "Epoch : 2 | Training Loss : 0.3767\n",
      "Epoch : 2 | Training Loss : 0.3878\n",
      "Epoch : 2 | Training Loss : 0.3704\n",
      "Epoch : 2 | Training Loss : 0.3071\n",
      "Epoch : 2 | Training Loss : 0.3296\n",
      "Epoch : 2 | Training Loss : 0.3491\n",
      "Epoch : 2 | Training Loss : 0.3577\n",
      "Epoch : 2 | Training Loss : 0.3464\n",
      "Epoch : 2 | Training Loss : 0.3188\n",
      "Epoch : 2 | Training Loss : 0.3950\n",
      "Epoch : 2 | Training Loss : 0.3674\n",
      "Epoch : 2 | Training Loss : 0.3888\n",
      "Epoch : 2 | Training Loss : 0.3257\n",
      "Epoch : 2 | Training Loss : 0.3154\n",
      "Epoch : 2 | Training Loss : 0.3696\n",
      "Epoch : 2 | Training Loss : 0.3570\n",
      "Epoch : 2 | Training Loss : 0.4904\n",
      "Epoch : 2 | Training Loss : 0.3944\n",
      "Epoch : 2 | Training Loss : 0.3348\n",
      "Epoch : 2 | Training Loss : 0.3265\n",
      "Epoch : 2 | Training Loss : 0.3284\n",
      "Epoch : 2 | Training Loss : 0.3605\n",
      "Epoch : 2 | Training Loss : 0.2358\n",
      "Epoch : 2 | Training Loss : 0.3917\n",
      "Epoch : 2 | Training Loss : 0.4389\n",
      "Epoch : 2 | Training Loss : 0.3649\n",
      "Epoch : 2 | Training Loss : 0.4104\n",
      "Epoch : 2 | Training Loss : 0.4510\n",
      "Epoch : 2 | Training Loss : 0.3658\n",
      "Epoch : 2 | Training Loss : 0.4233\n",
      "Epoch : 2 | Training Loss : 0.2901\n",
      "Epoch : 2 | Training Loss : 0.3468\n",
      "Epoch : 2 | Training Loss : 0.3908\n",
      "Epoch : 2 | Training Loss : 0.3708\n",
      "Epoch : 2 | Training Loss : 0.4052\n",
      "Epoch : 2 | Training Loss : 0.3277\n",
      "Epoch : 2 | Training Loss : 0.3866\n",
      "Epoch : 2 | Training Loss : 0.3839\n",
      "Epoch : 2 | Training Loss : 0.5231\n",
      "Epoch : 2 | Training Loss : 0.4957\n",
      "Epoch : 2 | Training Loss : 0.3578\n",
      "Epoch : 2 | Training Loss : 0.4475\n",
      "Epoch : 2 | Training Loss : 0.3659\n",
      "Epoch : 2 | Training Loss : 0.3363\n",
      "Epoch : 2 | Training Loss : 0.3402\n",
      "Epoch : 2 | Training Loss : 0.3245\n",
      "Epoch : 2 | Training Loss : 0.3692\n",
      "Epoch : 2 | Training Loss : 0.3712\n",
      "Epoch : 2 | Training Loss : 0.3129\n",
      "Epoch : 2 | Training Loss : 0.3846\n",
      "Epoch : 2 | Training Loss : 0.3357\n",
      "Epoch : 2 | Training Loss : 0.2778\n",
      "Epoch : 2 | Training Loss : 0.2787\n",
      "Epoch : 2 | Training Loss : 0.4613\n",
      "Epoch : 2 | Training Loss : 0.3560\n",
      "Epoch : 2 | Training Loss : 0.3082\n",
      "Epoch : 2 | Training Loss : 0.3667\n",
      "Epoch : 2 | Training Loss : 0.5011\n",
      "Epoch : 2 | Training Loss : 0.3296\n",
      "Epoch : 2 | Training Loss : 0.3950\n",
      "Epoch : 2 | Training Loss : 0.4172\n",
      "Epoch : 2 | Training Loss : 0.3612\n",
      "Epoch : 2 | Training Loss : 0.3561\n",
      "Epoch : 2 | Training Loss : 0.3018\n",
      "Epoch : 2 | Training Loss : 0.3578\n",
      "Epoch : 2 | Training Loss : 0.3776\n",
      "Epoch : 2 | Training Loss : 0.3257\n",
      "Epoch : 2 | Training Loss : 0.3703\n",
      "Epoch : 2 | Training Loss : 0.3658\n",
      "Epoch : 2 | Training Loss : 0.3825\n",
      "Epoch : 2 | Training Loss : 0.3903\n",
      "Epoch : 2 | Training Loss : 0.3889\n",
      "Epoch : 2 | Training Loss : 0.3686\n",
      "Epoch : 2 | Training Loss : 0.3917\n",
      "Epoch : 2 | Training Loss : 0.3374\n",
      "Epoch : 2 | Training Loss : 0.2612\n",
      "Epoch : 2 | Training Loss : 0.2991\n",
      "Epoch : 2 | Training Loss : 0.3527\n",
      "Epoch : 2 | Training Loss : 0.4356\n",
      "Epoch : 2 | Training Loss : 0.3606\n",
      "Epoch : 2 | Training Loss : 0.3936\n",
      "Epoch : 2 | Training Loss : 0.3285\n",
      "Epoch : 2 | Training Loss : 0.2808\n",
      "Epoch : 2 | Training Loss : 0.3515\n",
      "Epoch : 2 | Training Loss : 0.4439\n",
      "Epoch : 2 | Training Loss : 0.3781\n",
      "Epoch : 2 | Training Loss : 0.4792\n",
      "Epoch : 2 | Training Loss : 0.3214\n",
      "Epoch : 2 | Training Loss : 0.3434\n",
      "Epoch : 2 | Training Loss : 0.2728\n",
      "Epoch : 2 | Training Loss : 0.3459\n",
      "Epoch : 2 | Training Loss : 0.4770\n",
      "Epoch : 2 | Training Loss : 0.3202\n",
      "Epoch : 2 | Training Loss : 0.4114\n",
      "Epoch : 2 | Training Loss : 0.3554\n",
      "Epoch : 2 | Training Loss : 0.3620\n",
      "Epoch : 2 | Training Loss : 0.5826\n",
      "Epoch : 2 | Training Loss : 0.5276\n",
      "Epoch : 2 | Training Loss : 0.2797\n",
      "Epoch : 2 | Training Loss : 0.2386\n",
      "Epoch : 2 | Training Loss : 0.3324\n",
      "Epoch : 2 | Training Loss : 0.4432\n",
      "Epoch : 2 | Training Loss : 0.3465\n",
      "Epoch : 2 | Training Loss : 0.2918\n",
      "Epoch : 2 | Training Loss : 0.4329\n",
      "Epoch : 2 | Training Loss : 0.3008\n",
      "Epoch : 2 | Training Loss : 0.2342\n",
      "Epoch : 2 | Training Loss : 0.3501\n",
      "Epoch : 2 | Training Loss : 0.3850\n",
      "Epoch : 2 | Training Loss : 0.3355\n",
      "Epoch : 2 | Training Loss : 0.3494\n",
      "Epoch : 2 | Training Loss : 0.3872\n",
      "Epoch : 2 | Training Loss : 0.3521\n",
      "Epoch : 2 | Training Loss : 0.3570\n",
      "Epoch : 2 | Training Loss : 0.3552\n",
      "Epoch : 2 | Training Loss : 0.3542\n",
      "Epoch : 2 | Training Loss : 0.4548\n",
      "Epoch : 2 | Training Loss : 0.3593\n",
      "Epoch : 2 | Training Loss : 0.3935\n",
      "Epoch : 2 | Training Loss : 0.2763\n",
      "Epoch : 2 | Training Loss : 0.3198\n",
      "Epoch : 2 | Training Loss : 0.2968\n",
      "Epoch : 2 | Training Loss : 0.3703\n",
      "Epoch : 2 | Training Loss : 0.3115\n",
      "Epoch : 2 | Training Loss : 0.3351\n",
      "Epoch : 2 | Training Loss : 0.2310\n",
      "Epoch : 2 | Training Loss : 0.4072\n",
      "Epoch : 2 | Training Loss : 0.3962\n",
      "Epoch : 2 | Training Loss : 0.3248\n",
      "Epoch : 2 | Training Loss : 0.3458\n",
      "Epoch : 2 | Training Loss : 0.3695\n",
      "Epoch : 2 | Training Loss : 0.3067\n",
      "Epoch : 2 | Training Loss : 0.3893\n",
      "Epoch : 2 | Training Loss : 0.3859\n",
      "Epoch : 2 | Training Loss : 0.2853\n",
      "Epoch : 2 | Training Loss : 0.3482\n",
      "Epoch : 2 | Training Loss : 0.3720\n",
      "Epoch : 2 | Training Loss : 0.4696\n",
      "Epoch : 2 | Training Loss : 0.4215\n",
      "Epoch : 2 | Training Loss : 0.3489\n",
      "Epoch : 2 | Training Loss : 0.2445\n",
      "Epoch : 2 | Training Loss : 0.2580\n",
      "Epoch : 2 | Training Loss : 0.3308\n",
      "Epoch : 2 | Training Loss : 0.2858\n",
      "Epoch : 3 | Training Loss : 0.2771\n",
      "Epoch : 3 | Training Loss : 0.2466\n",
      "Epoch : 3 | Training Loss : 0.2047\n",
      "Epoch : 3 | Training Loss : 0.2325\n",
      "Epoch : 3 | Training Loss : 0.2498\n",
      "Epoch : 3 | Training Loss : 0.2388\n",
      "Epoch : 3 | Training Loss : 0.2891\n",
      "Epoch : 3 | Training Loss : 0.2851\n",
      "Epoch : 3 | Training Loss : 0.2972\n",
      "Epoch : 3 | Training Loss : 0.1763\n",
      "Epoch : 3 | Training Loss : 0.2336\n",
      "Epoch : 3 | Training Loss : 0.2745\n",
      "Epoch : 3 | Training Loss : 0.2461\n",
      "Epoch : 3 | Training Loss : 0.2469\n",
      "Epoch : 3 | Training Loss : 0.3295\n",
      "Epoch : 3 | Training Loss : 0.2127\n",
      "Epoch : 3 | Training Loss : 0.2834\n",
      "Epoch : 3 | Training Loss : 0.2975\n",
      "Epoch : 3 | Training Loss : 0.2748\n",
      "Epoch : 3 | Training Loss : 0.3055\n",
      "Epoch : 3 | Training Loss : 0.2756\n",
      "Epoch : 3 | Training Loss : 0.2111\n",
      "Epoch : 3 | Training Loss : 0.2745\n",
      "Epoch : 3 | Training Loss : 0.2523\n",
      "Epoch : 3 | Training Loss : 0.2336\n",
      "Epoch : 3 | Training Loss : 0.2880\n",
      "Epoch : 3 | Training Loss : 0.2252\n",
      "Epoch : 3 | Training Loss : 0.2518\n",
      "Epoch : 3 | Training Loss : 0.2118\n",
      "Epoch : 3 | Training Loss : 0.2049\n",
      "Epoch : 3 | Training Loss : 0.2493\n",
      "Epoch : 3 | Training Loss : 0.3155\n",
      "Epoch : 3 | Training Loss : 0.3239\n",
      "Epoch : 3 | Training Loss : 0.2021\n",
      "Epoch : 3 | Training Loss : 0.2142\n",
      "Epoch : 3 | Training Loss : 0.3336\n",
      "Epoch : 3 | Training Loss : 0.2538\n",
      "Epoch : 3 | Training Loss : 0.2722\n",
      "Epoch : 3 | Training Loss : 0.1494\n",
      "Epoch : 3 | Training Loss : 0.2194\n",
      "Epoch : 3 | Training Loss : 0.1863\n",
      "Epoch : 3 | Training Loss : 0.2145\n",
      "Epoch : 3 | Training Loss : 0.2643\n",
      "Epoch : 3 | Training Loss : 0.3966\n",
      "Epoch : 3 | Training Loss : 0.2120\n",
      "Epoch : 3 | Training Loss : 0.2756\n",
      "Epoch : 3 | Training Loss : 0.2242\n",
      "Epoch : 3 | Training Loss : 0.1970\n",
      "Epoch : 3 | Training Loss : 0.2994\n",
      "Epoch : 3 | Training Loss : 0.2510\n",
      "Epoch : 3 | Training Loss : 0.2938\n",
      "Epoch : 3 | Training Loss : 0.1892\n",
      "Epoch : 3 | Training Loss : 0.2585\n",
      "Epoch : 3 | Training Loss : 0.2492\n",
      "Epoch : 3 | Training Loss : 0.2267\n",
      "Epoch : 3 | Training Loss : 0.2270\n",
      "Epoch : 3 | Training Loss : 0.1874\n",
      "Epoch : 3 | Training Loss : 0.3440\n",
      "Epoch : 3 | Training Loss : 0.3200\n",
      "Epoch : 3 | Training Loss : 0.2580\n",
      "Epoch : 3 | Training Loss : 0.2390\n",
      "Epoch : 3 | Training Loss : 0.2597\n",
      "Epoch : 3 | Training Loss : 0.2757\n",
      "Epoch : 3 | Training Loss : 0.2447\n",
      "Epoch : 3 | Training Loss : 0.3467\n",
      "Epoch : 3 | Training Loss : 0.1907\n",
      "Epoch : 3 | Training Loss : 0.2940\n",
      "Epoch : 3 | Training Loss : 0.3083\n",
      "Epoch : 3 | Training Loss : 0.1790\n",
      "Epoch : 3 | Training Loss : 0.2399\n",
      "Epoch : 3 | Training Loss : 0.2969\n",
      "Epoch : 3 | Training Loss : 0.2296\n",
      "Epoch : 3 | Training Loss : 0.3406\n",
      "Epoch : 3 | Training Loss : 0.2732\n",
      "Epoch : 3 | Training Loss : 0.3336\n",
      "Epoch : 3 | Training Loss : 0.2941\n",
      "Epoch : 3 | Training Loss : 0.2430\n",
      "Epoch : 3 | Training Loss : 0.2585\n",
      "Epoch : 3 | Training Loss : 0.2593\n",
      "Epoch : 3 | Training Loss : 0.2876\n",
      "Epoch : 3 | Training Loss : 0.3259\n",
      "Epoch : 3 | Training Loss : 0.2408\n",
      "Epoch : 3 | Training Loss : 0.2391\n",
      "Epoch : 3 | Training Loss : 0.2912\n",
      "Epoch : 3 | Training Loss : 0.2501\n",
      "Epoch : 3 | Training Loss : 0.3305\n",
      "Epoch : 3 | Training Loss : 0.2908\n",
      "Epoch : 3 | Training Loss : 0.2630\n",
      "Epoch : 3 | Training Loss : 0.2357\n",
      "Epoch : 3 | Training Loss : 0.1896\n",
      "Epoch : 3 | Training Loss : 0.2199\n",
      "Epoch : 3 | Training Loss : 0.2720\n",
      "Epoch : 3 | Training Loss : 0.3403\n",
      "Epoch : 3 | Training Loss : 0.2535\n",
      "Epoch : 3 | Training Loss : 0.3425\n",
      "Epoch : 3 | Training Loss : 0.2885\n",
      "Epoch : 3 | Training Loss : 0.2323\n",
      "Epoch : 3 | Training Loss : 0.2602\n",
      "Epoch : 3 | Training Loss : 0.2117\n",
      "Epoch : 3 | Training Loss : 0.2747\n",
      "Epoch : 3 | Training Loss : 0.3385\n",
      "Epoch : 3 | Training Loss : 0.2585\n",
      "Epoch : 3 | Training Loss : 0.2478\n",
      "Epoch : 3 | Training Loss : 0.2653\n",
      "Epoch : 3 | Training Loss : 0.2881\n",
      "Epoch : 3 | Training Loss : 0.2857\n",
      "Epoch : 3 | Training Loss : 0.3153\n",
      "Epoch : 3 | Training Loss : 0.1921\n",
      "Epoch : 3 | Training Loss : 0.2895\n",
      "Epoch : 3 | Training Loss : 0.1864\n",
      "Epoch : 3 | Training Loss : 0.2400\n",
      "Epoch : 3 | Training Loss : 0.2620\n",
      "Epoch : 3 | Training Loss : 0.2865\n",
      "Epoch : 3 | Training Loss : 0.2241\n",
      "Epoch : 3 | Training Loss : 0.1848\n",
      "Epoch : 3 | Training Loss : 0.1910\n",
      "Epoch : 3 | Training Loss : 0.2235\n",
      "Epoch : 3 | Training Loss : 0.2706\n",
      "Epoch : 3 | Training Loss : 0.2669\n",
      "Epoch : 3 | Training Loss : 0.2300\n",
      "Epoch : 3 | Training Loss : 0.2096\n",
      "Epoch : 3 | Training Loss : 0.2822\n",
      "Epoch : 3 | Training Loss : 0.2862\n",
      "Epoch : 3 | Training Loss : 0.2694\n",
      "Epoch : 3 | Training Loss : 0.1950\n",
      "Epoch : 3 | Training Loss : 0.3461\n",
      "Epoch : 3 | Training Loss : 0.2677\n",
      "Epoch : 3 | Training Loss : 0.2963\n",
      "Epoch : 3 | Training Loss : 0.2756\n",
      "Epoch : 3 | Training Loss : 0.2448\n",
      "Epoch : 3 | Training Loss : 0.2695\n",
      "Epoch : 3 | Training Loss : 0.2572\n",
      "Epoch : 3 | Training Loss : 0.2357\n",
      "Epoch : 3 | Training Loss : 0.3135\n",
      "Epoch : 3 | Training Loss : 0.3450\n",
      "Epoch : 3 | Training Loss : 0.2064\n",
      "Epoch : 3 | Training Loss : 0.1615\n",
      "Epoch : 3 | Training Loss : 0.2468\n",
      "Epoch : 3 | Training Loss : 0.3410\n",
      "Epoch : 3 | Training Loss : 0.2475\n",
      "Epoch : 3 | Training Loss : 0.1719\n",
      "Epoch : 3 | Training Loss : 0.2210\n",
      "Epoch : 3 | Training Loss : 0.2131\n",
      "Epoch : 3 | Training Loss : 0.2162\n",
      "Epoch : 3 | Training Loss : 0.1844\n",
      "Epoch : 3 | Training Loss : 0.2359\n",
      "Epoch : 3 | Training Loss : 0.2456\n",
      "Epoch : 3 | Training Loss : 0.3077\n",
      "Epoch : 3 | Training Loss : 0.1956\n",
      "Epoch : 3 | Training Loss : 0.2400\n",
      "Epoch : 3 | Training Loss : 0.3361\n",
      "Epoch : 3 | Training Loss : 0.2398\n",
      "Epoch : 3 | Training Loss : 0.2769\n",
      "Epoch : 3 | Training Loss : 0.2768\n",
      "Epoch : 3 | Training Loss : 0.2793\n",
      "Epoch : 3 | Training Loss : 0.2107\n",
      "Epoch : 3 | Training Loss : 0.2491\n",
      "Epoch : 3 | Training Loss : 0.2580\n",
      "Epoch : 3 | Training Loss : 0.3771\n",
      "Epoch : 3 | Training Loss : 0.2915\n",
      "Epoch : 3 | Training Loss : 0.2692\n",
      "Epoch : 3 | Training Loss : 0.2927\n",
      "Epoch : 3 | Training Loss : 0.1940\n",
      "Epoch : 3 | Training Loss : 0.3237\n",
      "Epoch : 3 | Training Loss : 0.2108\n",
      "Epoch : 3 | Training Loss : 0.2499\n",
      "Epoch : 3 | Training Loss : 0.1968\n",
      "Epoch : 3 | Training Loss : 0.2972\n",
      "Epoch : 3 | Training Loss : 0.2542\n",
      "Epoch : 3 | Training Loss : 0.2713\n",
      "Epoch : 3 | Training Loss : 0.2916\n",
      "Epoch : 3 | Training Loss : 0.2424\n",
      "Epoch : 3 | Training Loss : 0.3649\n",
      "Epoch : 3 | Training Loss : 0.2519\n",
      "Epoch : 3 | Training Loss : 0.2934\n",
      "Epoch : 3 | Training Loss : 0.3190\n",
      "Epoch : 3 | Training Loss : 0.2514\n",
      "Epoch : 3 | Training Loss : 0.2122\n",
      "Epoch : 3 | Training Loss : 0.3616\n",
      "Epoch : 3 | Training Loss : 0.3051\n",
      "Epoch : 3 | Training Loss : 0.2419\n",
      "Epoch : 3 | Training Loss : 0.2885\n",
      "Epoch : 3 | Training Loss : 0.2014\n",
      "Epoch : 3 | Training Loss : 0.3537\n",
      "Epoch : 3 | Training Loss : 0.3359\n",
      "Epoch : 3 | Training Loss : 0.3404\n",
      "Epoch : 3 | Training Loss : 0.2446\n",
      "Epoch : 3 | Training Loss : 0.2879\n",
      "Epoch : 3 | Training Loss : 0.2518\n",
      "Epoch : 3 | Training Loss : 0.2897\n",
      "Epoch : 3 | Training Loss : 0.2213\n",
      "Epoch : 3 | Training Loss : 0.3059\n",
      "Epoch : 3 | Training Loss : 0.2151\n",
      "Epoch : 3 | Training Loss : 0.3499\n",
      "Epoch : 3 | Training Loss : 0.2998\n",
      "Epoch : 3 | Training Loss : 0.2347\n",
      "Epoch : 3 | Training Loss : 0.2463\n",
      "Epoch : 3 | Training Loss : 0.2828\n",
      "Epoch : 3 | Training Loss : 0.2668\n",
      "Epoch : 3 | Training Loss : 0.1719\n",
      "Epoch : 3 | Training Loss : 0.3062\n",
      "Epoch : 3 | Training Loss : 0.2742\n",
      "Epoch : 3 | Training Loss : 0.2718\n",
      "Epoch : 3 | Training Loss : 0.1763\n",
      "Epoch : 3 | Training Loss : 0.2339\n",
      "Epoch : 3 | Training Loss : 0.2127\n",
      "Epoch : 3 | Training Loss : 0.2165\n",
      "Epoch : 3 | Training Loss : 0.2164\n",
      "Epoch : 3 | Training Loss : 0.3258\n",
      "Epoch : 3 | Training Loss : 0.2321\n",
      "Epoch : 3 | Training Loss : 0.3873\n",
      "Epoch : 3 | Training Loss : 0.2551\n",
      "Epoch : 3 | Training Loss : 0.2167\n",
      "Epoch : 3 | Training Loss : 0.3066\n",
      "Epoch : 3 | Training Loss : 0.2893\n",
      "Epoch : 3 | Training Loss : 0.2678\n",
      "Epoch : 3 | Training Loss : 0.2734\n",
      "Epoch : 3 | Training Loss : 0.2207\n",
      "Epoch : 3 | Training Loss : 0.2257\n",
      "Epoch : 3 | Training Loss : 0.2080\n",
      "Epoch : 3 | Training Loss : 0.1845\n",
      "Epoch : 3 | Training Loss : 0.3447\n",
      "Epoch : 3 | Training Loss : 0.3343\n",
      "Epoch : 3 | Training Loss : 0.2902\n",
      "Epoch : 3 | Training Loss : 0.2508\n",
      "Epoch : 3 | Training Loss : 0.2724\n",
      "Epoch : 3 | Training Loss : 0.2151\n",
      "Epoch : 3 | Training Loss : 0.2760\n",
      "Epoch : 3 | Training Loss : 0.2003\n",
      "Epoch : 3 | Training Loss : 0.2545\n",
      "Epoch : 3 | Training Loss : 0.2525\n",
      "Epoch : 3 | Training Loss : 0.2551\n",
      "Epoch : 3 | Training Loss : 0.2476\n",
      "Epoch : 3 | Training Loss : 0.1753\n",
      "Epoch : 3 | Training Loss : 0.2369\n",
      "Epoch : 3 | Training Loss : 0.2935\n",
      "Epoch : 3 | Training Loss : 0.2239\n",
      "Epoch : 3 | Training Loss : 0.2699\n",
      "Epoch : 3 | Training Loss : 0.2267\n",
      "Epoch : 3 | Training Loss : 0.2332\n",
      "Epoch : 3 | Training Loss : 0.1541\n",
      "Epoch : 3 | Training Loss : 0.3113\n",
      "Epoch : 3 | Training Loss : 0.2177\n",
      "Epoch : 4 | Training Loss : 0.1796\n",
      "Epoch : 4 | Training Loss : 0.1884\n",
      "Epoch : 4 | Training Loss : 0.1740\n",
      "Epoch : 4 | Training Loss : 0.1648\n",
      "Epoch : 4 | Training Loss : 0.2082\n",
      "Epoch : 4 | Training Loss : 0.1861\n",
      "Epoch : 4 | Training Loss : 0.2188\n",
      "Epoch : 4 | Training Loss : 0.1758\n",
      "Epoch : 4 | Training Loss : 0.2426\n",
      "Epoch : 4 | Training Loss : 0.2434\n",
      "Epoch : 4 | Training Loss : 0.2032\n",
      "Epoch : 4 | Training Loss : 0.1931\n",
      "Epoch : 4 | Training Loss : 0.1475\n",
      "Epoch : 4 | Training Loss : 0.1562\n",
      "Epoch : 4 | Training Loss : 0.1746\n",
      "Epoch : 4 | Training Loss : 0.1850\n",
      "Epoch : 4 | Training Loss : 0.1577\n",
      "Epoch : 4 | Training Loss : 0.1963\n",
      "Epoch : 4 | Training Loss : 0.2059\n",
      "Epoch : 4 | Training Loss : 0.2183\n",
      "Epoch : 4 | Training Loss : 0.1612\n",
      "Epoch : 4 | Training Loss : 0.1443\n",
      "Epoch : 4 | Training Loss : 0.1253\n",
      "Epoch : 4 | Training Loss : 0.1328\n",
      "Epoch : 4 | Training Loss : 0.1978\n",
      "Epoch : 4 | Training Loss : 0.2034\n",
      "Epoch : 4 | Training Loss : 0.1758\n",
      "Epoch : 4 | Training Loss : 0.1878\n",
      "Epoch : 4 | Training Loss : 0.1881\n",
      "Epoch : 4 | Training Loss : 0.1921\n",
      "Epoch : 4 | Training Loss : 0.1533\n",
      "Epoch : 4 | Training Loss : 0.2291\n",
      "Epoch : 4 | Training Loss : 0.2048\n",
      "Epoch : 4 | Training Loss : 0.1502\n",
      "Epoch : 4 | Training Loss : 0.1934\n",
      "Epoch : 4 | Training Loss : 0.2070\n",
      "Epoch : 4 | Training Loss : 0.1690\n",
      "Epoch : 4 | Training Loss : 0.1803\n",
      "Epoch : 4 | Training Loss : 0.1600\n",
      "Epoch : 4 | Training Loss : 0.1376\n",
      "Epoch : 4 | Training Loss : 0.2252\n",
      "Epoch : 4 | Training Loss : 0.2155\n",
      "Epoch : 4 | Training Loss : 0.1695\n",
      "Epoch : 4 | Training Loss : 0.1633\n",
      "Epoch : 4 | Training Loss : 0.1963\n",
      "Epoch : 4 | Training Loss : 0.2082\n",
      "Epoch : 4 | Training Loss : 0.1890\n",
      "Epoch : 4 | Training Loss : 0.1987\n",
      "Epoch : 4 | Training Loss : 0.1221\n",
      "Epoch : 4 | Training Loss : 0.1282\n",
      "Epoch : 4 | Training Loss : 0.2119\n",
      "Epoch : 4 | Training Loss : 0.0981\n",
      "Epoch : 4 | Training Loss : 0.1376\n",
      "Epoch : 4 | Training Loss : 0.1879\n",
      "Epoch : 4 | Training Loss : 0.1305\n",
      "Epoch : 4 | Training Loss : 0.2268\n",
      "Epoch : 4 | Training Loss : 0.2306\n",
      "Epoch : 4 | Training Loss : 0.1529\n",
      "Epoch : 4 | Training Loss : 0.1936\n",
      "Epoch : 4 | Training Loss : 0.1892\n",
      "Epoch : 4 | Training Loss : 0.1600\n",
      "Epoch : 4 | Training Loss : 0.1959\n",
      "Epoch : 4 | Training Loss : 0.2083\n",
      "Epoch : 4 | Training Loss : 0.1822\n",
      "Epoch : 4 | Training Loss : 0.1162\n",
      "Epoch : 4 | Training Loss : 0.2766\n",
      "Epoch : 4 | Training Loss : 0.1517\n",
      "Epoch : 4 | Training Loss : 0.1199\n",
      "Epoch : 4 | Training Loss : 0.1775\n",
      "Epoch : 4 | Training Loss : 0.1188\n",
      "Epoch : 4 | Training Loss : 0.1877\n",
      "Epoch : 4 | Training Loss : 0.1599\n",
      "Epoch : 4 | Training Loss : 0.1657\n",
      "Epoch : 4 | Training Loss : 0.2073\n",
      "Epoch : 4 | Training Loss : 0.2052\n",
      "Epoch : 4 | Training Loss : 0.2375\n",
      "Epoch : 4 | Training Loss : 0.1427\n",
      "Epoch : 4 | Training Loss : 0.1555\n",
      "Epoch : 4 | Training Loss : 0.1658\n",
      "Epoch : 4 | Training Loss : 0.1850\n",
      "Epoch : 4 | Training Loss : 0.1199\n",
      "Epoch : 4 | Training Loss : 0.1656\n",
      "Epoch : 4 | Training Loss : 0.1913\n",
      "Epoch : 4 | Training Loss : 0.1581\n",
      "Epoch : 4 | Training Loss : 0.1995\n",
      "Epoch : 4 | Training Loss : 0.1733\n",
      "Epoch : 4 | Training Loss : 0.1585\n",
      "Epoch : 4 | Training Loss : 0.2029\n",
      "Epoch : 4 | Training Loss : 0.1596\n",
      "Epoch : 4 | Training Loss : 0.1716\n",
      "Epoch : 4 | Training Loss : 0.1570\n",
      "Epoch : 4 | Training Loss : 0.1098\n",
      "Epoch : 4 | Training Loss : 0.1475\n",
      "Epoch : 4 | Training Loss : 0.1700\n",
      "Epoch : 4 | Training Loss : 0.0959\n",
      "Epoch : 4 | Training Loss : 0.2083\n",
      "Epoch : 4 | Training Loss : 0.1647\n",
      "Epoch : 4 | Training Loss : 0.2429\n",
      "Epoch : 4 | Training Loss : 0.2071\n",
      "Epoch : 4 | Training Loss : 0.1514\n",
      "Epoch : 4 | Training Loss : 0.1543\n",
      "Epoch : 4 | Training Loss : 0.1883\n",
      "Epoch : 4 | Training Loss : 0.1781\n",
      "Epoch : 4 | Training Loss : 0.2490\n",
      "Epoch : 4 | Training Loss : 0.2385\n",
      "Epoch : 4 | Training Loss : 0.2126\n",
      "Epoch : 4 | Training Loss : 0.1882\n",
      "Epoch : 4 | Training Loss : 0.1076\n",
      "Epoch : 4 | Training Loss : 0.1583\n",
      "Epoch : 4 | Training Loss : 0.2494\n",
      "Epoch : 4 | Training Loss : 0.1160\n",
      "Epoch : 4 | Training Loss : 0.1349\n",
      "Epoch : 4 | Training Loss : 0.2031\n",
      "Epoch : 4 | Training Loss : 0.1669\n",
      "Epoch : 4 | Training Loss : 0.2045\n",
      "Epoch : 4 | Training Loss : 0.1106\n",
      "Epoch : 4 | Training Loss : 0.2025\n",
      "Epoch : 4 | Training Loss : 0.1521\n",
      "Epoch : 4 | Training Loss : 0.1325\n",
      "Epoch : 4 | Training Loss : 0.2268\n",
      "Epoch : 4 | Training Loss : 0.1239\n",
      "Epoch : 4 | Training Loss : 0.0952\n",
      "Epoch : 4 | Training Loss : 0.1135\n",
      "Epoch : 4 | Training Loss : 0.1540\n",
      "Epoch : 4 | Training Loss : 0.1708\n",
      "Epoch : 4 | Training Loss : 0.1837\n",
      "Epoch : 4 | Training Loss : 0.1597\n",
      "Epoch : 4 | Training Loss : 0.1128\n",
      "Epoch : 4 | Training Loss : 0.1266\n",
      "Epoch : 4 | Training Loss : 0.1957\n",
      "Epoch : 4 | Training Loss : 0.1589\n",
      "Epoch : 4 | Training Loss : 0.0759\n",
      "Epoch : 4 | Training Loss : 0.1840\n",
      "Epoch : 4 | Training Loss : 0.2392\n",
      "Epoch : 4 | Training Loss : 0.2544\n",
      "Epoch : 4 | Training Loss : 0.1716\n",
      "Epoch : 4 | Training Loss : 0.1449\n",
      "Epoch : 4 | Training Loss : 0.1562\n",
      "Epoch : 4 | Training Loss : 0.2085\n",
      "Epoch : 4 | Training Loss : 0.2735\n",
      "Epoch : 4 | Training Loss : 0.1702\n",
      "Epoch : 4 | Training Loss : 0.1990\n",
      "Epoch : 4 | Training Loss : 0.2297\n",
      "Epoch : 4 | Training Loss : 0.2058\n",
      "Epoch : 4 | Training Loss : 0.1590\n",
      "Epoch : 4 | Training Loss : 0.1458\n",
      "Epoch : 4 | Training Loss : 0.2012\n",
      "Epoch : 4 | Training Loss : 0.1835\n",
      "Epoch : 4 | Training Loss : 0.1573\n",
      "Epoch : 4 | Training Loss : 0.1371\n",
      "Epoch : 4 | Training Loss : 0.1529\n",
      "Epoch : 4 | Training Loss : 0.1972\n",
      "Epoch : 4 | Training Loss : 0.1298\n",
      "Epoch : 4 | Training Loss : 0.1391\n",
      "Epoch : 4 | Training Loss : 0.2152\n",
      "Epoch : 4 | Training Loss : 0.1618\n",
      "Epoch : 4 | Training Loss : 0.1767\n",
      "Epoch : 4 | Training Loss : 0.1451\n",
      "Epoch : 4 | Training Loss : 0.2219\n",
      "Epoch : 4 | Training Loss : 0.1603\n",
      "Epoch : 4 | Training Loss : 0.1542\n",
      "Epoch : 4 | Training Loss : 0.1594\n",
      "Epoch : 4 | Training Loss : 0.2744\n",
      "Epoch : 4 | Training Loss : 0.1917\n",
      "Epoch : 4 | Training Loss : 0.2019\n",
      "Epoch : 4 | Training Loss : 0.1893\n",
      "Epoch : 4 | Training Loss : 0.2306\n",
      "Epoch : 4 | Training Loss : 0.2055\n",
      "Epoch : 4 | Training Loss : 0.1539\n",
      "Epoch : 4 | Training Loss : 0.1571\n",
      "Epoch : 4 | Training Loss : 0.1953\n",
      "Epoch : 4 | Training Loss : 0.2219\n",
      "Epoch : 4 | Training Loss : 0.2091\n",
      "Epoch : 4 | Training Loss : 0.2696\n",
      "Epoch : 4 | Training Loss : 0.1691\n",
      "Epoch : 4 | Training Loss : 0.1409\n",
      "Epoch : 4 | Training Loss : 0.2048\n",
      "Epoch : 4 | Training Loss : 0.1034\n",
      "Epoch : 4 | Training Loss : 0.1499\n",
      "Epoch : 4 | Training Loss : 0.1430\n",
      "Epoch : 4 | Training Loss : 0.1822\n",
      "Epoch : 4 | Training Loss : 0.2023\n",
      "Epoch : 4 | Training Loss : 0.3208\n",
      "Epoch : 4 | Training Loss : 0.1676\n",
      "Epoch : 4 | Training Loss : 0.2111\n",
      "Epoch : 4 | Training Loss : 0.2512\n",
      "Epoch : 4 | Training Loss : 0.2200\n",
      "Epoch : 4 | Training Loss : 0.1225\n",
      "Epoch : 4 | Training Loss : 0.1802\n",
      "Epoch : 4 | Training Loss : 0.1961\n",
      "Epoch : 4 | Training Loss : 0.1698\n",
      "Epoch : 4 | Training Loss : 0.2041\n",
      "Epoch : 4 | Training Loss : 0.2023\n",
      "Epoch : 4 | Training Loss : 0.1614\n",
      "Epoch : 4 | Training Loss : 0.1208\n",
      "Epoch : 4 | Training Loss : 0.1193\n",
      "Epoch : 4 | Training Loss : 0.1638\n",
      "Epoch : 4 | Training Loss : 0.1900\n",
      "Epoch : 4 | Training Loss : 0.1907\n",
      "Epoch : 4 | Training Loss : 0.1771\n",
      "Epoch : 4 | Training Loss : 0.1742\n",
      "Epoch : 4 | Training Loss : 0.2045\n",
      "Epoch : 4 | Training Loss : 0.2167\n",
      "Epoch : 4 | Training Loss : 0.2986\n",
      "Epoch : 4 | Training Loss : 0.1598\n",
      "Epoch : 4 | Training Loss : 0.1473\n",
      "Epoch : 4 | Training Loss : 0.1010\n",
      "Epoch : 4 | Training Loss : 0.1215\n",
      "Epoch : 4 | Training Loss : 0.1512\n",
      "Epoch : 4 | Training Loss : 0.1402\n",
      "Epoch : 4 | Training Loss : 0.1652\n",
      "Epoch : 4 | Training Loss : 0.2368\n",
      "Epoch : 4 | Training Loss : 0.1667\n",
      "Epoch : 4 | Training Loss : 0.1894\n",
      "Epoch : 4 | Training Loss : 0.1805\n",
      "Epoch : 4 | Training Loss : 0.1561\n",
      "Epoch : 4 | Training Loss : 0.2150\n",
      "Epoch : 4 | Training Loss : 0.1526\n",
      "Epoch : 4 | Training Loss : 0.1905\n",
      "Epoch : 4 | Training Loss : 0.2567\n",
      "Epoch : 4 | Training Loss : 0.1165\n",
      "Epoch : 4 | Training Loss : 0.1571\n",
      "Epoch : 4 | Training Loss : 0.2928\n",
      "Epoch : 4 | Training Loss : 0.2044\n",
      "Epoch : 4 | Training Loss : 0.1597\n",
      "Epoch : 4 | Training Loss : 0.1283\n",
      "Epoch : 4 | Training Loss : 0.1805\n",
      "Epoch : 4 | Training Loss : 0.2671\n",
      "Epoch : 4 | Training Loss : 0.2021\n",
      "Epoch : 4 | Training Loss : 0.2828\n",
      "Epoch : 4 | Training Loss : 0.1835\n",
      "Epoch : 4 | Training Loss : 0.2171\n",
      "Epoch : 4 | Training Loss : 0.2379\n",
      "Epoch : 4 | Training Loss : 0.1876\n",
      "Epoch : 4 | Training Loss : 0.0862\n",
      "Epoch : 4 | Training Loss : 0.1367\n",
      "Epoch : 4 | Training Loss : 0.2154\n",
      "Epoch : 4 | Training Loss : 0.2089\n",
      "Epoch : 4 | Training Loss : 0.1724\n",
      "Epoch : 4 | Training Loss : 0.2560\n",
      "Epoch : 4 | Training Loss : 0.1569\n",
      "Epoch : 4 | Training Loss : 0.1916\n",
      "Epoch : 4 | Training Loss : 0.2172\n",
      "Epoch : 5 | Training Loss : 0.1610\n",
      "Epoch : 5 | Training Loss : 0.1915\n",
      "Epoch : 5 | Training Loss : 0.1373\n",
      "Epoch : 5 | Training Loss : 0.1316\n",
      "Epoch : 5 | Training Loss : 0.1080\n",
      "Epoch : 5 | Training Loss : 0.1158\n",
      "Epoch : 5 | Training Loss : 0.1160\n",
      "Epoch : 5 | Training Loss : 0.1025\n",
      "Epoch : 5 | Training Loss : 0.1365\n",
      "Epoch : 5 | Training Loss : 0.1696\n",
      "Epoch : 5 | Training Loss : 0.1166\n",
      "Epoch : 5 | Training Loss : 0.0966\n",
      "Epoch : 5 | Training Loss : 0.0994\n",
      "Epoch : 5 | Training Loss : 0.1303\n",
      "Epoch : 5 | Training Loss : 0.1209\n",
      "Epoch : 5 | Training Loss : 0.1016\n",
      "Epoch : 5 | Training Loss : 0.0847\n",
      "Epoch : 5 | Training Loss : 0.1011\n",
      "Epoch : 5 | Training Loss : 0.1426\n",
      "Epoch : 5 | Training Loss : 0.1422\n",
      "Epoch : 5 | Training Loss : 0.0552\n",
      "Epoch : 5 | Training Loss : 0.1406\n",
      "Epoch : 5 | Training Loss : 0.1112\n",
      "Epoch : 5 | Training Loss : 0.1239\n",
      "Epoch : 5 | Training Loss : 0.1473\n",
      "Epoch : 5 | Training Loss : 0.0621\n",
      "Epoch : 5 | Training Loss : 0.1369\n",
      "Epoch : 5 | Training Loss : 0.1227\n",
      "Epoch : 5 | Training Loss : 0.1148\n",
      "Epoch : 5 | Training Loss : 0.0886\n",
      "Epoch : 5 | Training Loss : 0.1236\n",
      "Epoch : 5 | Training Loss : 0.1934\n",
      "Epoch : 5 | Training Loss : 0.1137\n",
      "Epoch : 5 | Training Loss : 0.0928\n",
      "Epoch : 5 | Training Loss : 0.1138\n",
      "Epoch : 5 | Training Loss : 0.1015\n",
      "Epoch : 5 | Training Loss : 0.1234\n",
      "Epoch : 5 | Training Loss : 0.0958\n",
      "Epoch : 5 | Training Loss : 0.0580\n",
      "Epoch : 5 | Training Loss : 0.1234\n",
      "Epoch : 5 | Training Loss : 0.1014\n",
      "Epoch : 5 | Training Loss : 0.1242\n",
      "Epoch : 5 | Training Loss : 0.1497\n",
      "Epoch : 5 | Training Loss : 0.1034\n",
      "Epoch : 5 | Training Loss : 0.1124\n",
      "Epoch : 5 | Training Loss : 0.1254\n",
      "Epoch : 5 | Training Loss : 0.1384\n",
      "Epoch : 5 | Training Loss : 0.1142\n",
      "Epoch : 5 | Training Loss : 0.1404\n",
      "Epoch : 5 | Training Loss : 0.1715\n",
      "Epoch : 5 | Training Loss : 0.1209\n",
      "Epoch : 5 | Training Loss : 0.1425\n",
      "Epoch : 5 | Training Loss : 0.0789\n",
      "Epoch : 5 | Training Loss : 0.1493\n",
      "Epoch : 5 | Training Loss : 0.1619\n",
      "Epoch : 5 | Training Loss : 0.1648\n",
      "Epoch : 5 | Training Loss : 0.1418\n",
      "Epoch : 5 | Training Loss : 0.0907\n",
      "Epoch : 5 | Training Loss : 0.1250\n",
      "Epoch : 5 | Training Loss : 0.0899\n",
      "Epoch : 5 | Training Loss : 0.1564\n",
      "Epoch : 5 | Training Loss : 0.1104\n",
      "Epoch : 5 | Training Loss : 0.0889\n",
      "Epoch : 5 | Training Loss : 0.1131\n",
      "Epoch : 5 | Training Loss : 0.1850\n",
      "Epoch : 5 | Training Loss : 0.0799\n",
      "Epoch : 5 | Training Loss : 0.0675\n",
      "Epoch : 5 | Training Loss : 0.0969\n",
      "Epoch : 5 | Training Loss : 0.2042\n",
      "Epoch : 5 | Training Loss : 0.1194\n",
      "Epoch : 5 | Training Loss : 0.1237\n",
      "Epoch : 5 | Training Loss : 0.0891\n",
      "Epoch : 5 | Training Loss : 0.0982\n",
      "Epoch : 5 | Training Loss : 0.1891\n",
      "Epoch : 5 | Training Loss : 0.1432\n",
      "Epoch : 5 | Training Loss : 0.1807\n",
      "Epoch : 5 | Training Loss : 0.0632\n",
      "Epoch : 5 | Training Loss : 0.1202\n",
      "Epoch : 5 | Training Loss : 0.0832\n",
      "Epoch : 5 | Training Loss : 0.0730\n",
      "Epoch : 5 | Training Loss : 0.1174\n",
      "Epoch : 5 | Training Loss : 0.1445\n",
      "Epoch : 5 | Training Loss : 0.1334\n",
      "Epoch : 5 | Training Loss : 0.0986\n",
      "Epoch : 5 | Training Loss : 0.0649\n",
      "Epoch : 5 | Training Loss : 0.1129\n",
      "Epoch : 5 | Training Loss : 0.1014\n",
      "Epoch : 5 | Training Loss : 0.0829\n",
      "Epoch : 5 | Training Loss : 0.0764\n",
      "Epoch : 5 | Training Loss : 0.0949\n",
      "Epoch : 5 | Training Loss : 0.1139\n",
      "Epoch : 5 | Training Loss : 0.1480\n",
      "Epoch : 5 | Training Loss : 0.0670\n",
      "Epoch : 5 | Training Loss : 0.1293\n",
      "Epoch : 5 | Training Loss : 0.1098\n",
      "Epoch : 5 | Training Loss : 0.1431\n",
      "Epoch : 5 | Training Loss : 0.0789\n",
      "Epoch : 5 | Training Loss : 0.1654\n",
      "Epoch : 5 | Training Loss : 0.1388\n",
      "Epoch : 5 | Training Loss : 0.0895\n",
      "Epoch : 5 | Training Loss : 0.1347\n",
      "Epoch : 5 | Training Loss : 0.1280\n",
      "Epoch : 5 | Training Loss : 0.0916\n",
      "Epoch : 5 | Training Loss : 0.1245\n",
      "Epoch : 5 | Training Loss : 0.1161\n",
      "Epoch : 5 | Training Loss : 0.0722\n",
      "Epoch : 5 | Training Loss : 0.1175\n",
      "Epoch : 5 | Training Loss : 0.1714\n",
      "Epoch : 5 | Training Loss : 0.0908\n",
      "Epoch : 5 | Training Loss : 0.0948\n",
      "Epoch : 5 | Training Loss : 0.0603\n",
      "Epoch : 5 | Training Loss : 0.0904\n",
      "Epoch : 5 | Training Loss : 0.1643\n",
      "Epoch : 5 | Training Loss : 0.1481\n",
      "Epoch : 5 | Training Loss : 0.0973\n",
      "Epoch : 5 | Training Loss : 0.1420\n",
      "Epoch : 5 | Training Loss : 0.1363\n",
      "Epoch : 5 | Training Loss : 0.1944\n",
      "Epoch : 5 | Training Loss : 0.0932\n",
      "Epoch : 5 | Training Loss : 0.1081\n",
      "Epoch : 5 | Training Loss : 0.0889\n",
      "Epoch : 5 | Training Loss : 0.1228\n",
      "Epoch : 5 | Training Loss : 0.1357\n",
      "Epoch : 5 | Training Loss : 0.1047\n",
      "Epoch : 5 | Training Loss : 0.1117\n",
      "Epoch : 5 | Training Loss : 0.1198\n",
      "Epoch : 5 | Training Loss : 0.1592\n",
      "Epoch : 5 | Training Loss : 0.1064\n",
      "Epoch : 5 | Training Loss : 0.0938\n",
      "Epoch : 5 | Training Loss : 0.1433\n",
      "Epoch : 5 | Training Loss : 0.1369\n",
      "Epoch : 5 | Training Loss : 0.0669\n",
      "Epoch : 5 | Training Loss : 0.1456\n",
      "Epoch : 5 | Training Loss : 0.1227\n",
      "Epoch : 5 | Training Loss : 0.0657\n",
      "Epoch : 5 | Training Loss : 0.0861\n",
      "Epoch : 5 | Training Loss : 0.1461\n",
      "Epoch : 5 | Training Loss : 0.0840\n",
      "Epoch : 5 | Training Loss : 0.0865\n",
      "Epoch : 5 | Training Loss : 0.0845\n",
      "Epoch : 5 | Training Loss : 0.1166\n",
      "Epoch : 5 | Training Loss : 0.0925\n",
      "Epoch : 5 | Training Loss : 0.0919\n",
      "Epoch : 5 | Training Loss : 0.0882\n",
      "Epoch : 5 | Training Loss : 0.0495\n",
      "Epoch : 5 | Training Loss : 0.0999\n",
      "Epoch : 5 | Training Loss : 0.0943\n",
      "Epoch : 5 | Training Loss : 0.0890\n",
      "Epoch : 5 | Training Loss : 0.1882\n",
      "Epoch : 5 | Training Loss : 0.1640\n",
      "Epoch : 5 | Training Loss : 0.1321\n",
      "Epoch : 5 | Training Loss : 0.0924\n",
      "Epoch : 5 | Training Loss : 0.0779\n",
      "Epoch : 5 | Training Loss : 0.0877\n",
      "Epoch : 5 | Training Loss : 0.0640\n",
      "Epoch : 5 | Training Loss : 0.0867\n",
      "Epoch : 5 | Training Loss : 0.0543\n",
      "Epoch : 5 | Training Loss : 0.1049\n",
      "Epoch : 5 | Training Loss : 0.0957\n",
      "Epoch : 5 | Training Loss : 0.0955\n",
      "Epoch : 5 | Training Loss : 0.0580\n",
      "Epoch : 5 | Training Loss : 0.0860\n",
      "Epoch : 5 | Training Loss : 0.0730\n",
      "Epoch : 5 | Training Loss : 0.0680\n",
      "Epoch : 5 | Training Loss : 0.1711\n",
      "Epoch : 5 | Training Loss : 0.0700\n",
      "Epoch : 5 | Training Loss : 0.1546\n",
      "Epoch : 5 | Training Loss : 0.1677\n",
      "Epoch : 5 | Training Loss : 0.0816\n",
      "Epoch : 5 | Training Loss : 0.1012\n",
      "Epoch : 5 | Training Loss : 0.1282\n",
      "Epoch : 5 | Training Loss : 0.0917\n",
      "Epoch : 5 | Training Loss : 0.0960\n",
      "Epoch : 5 | Training Loss : 0.0774\n",
      "Epoch : 5 | Training Loss : 0.0899\n",
      "Epoch : 5 | Training Loss : 0.0920\n",
      "Epoch : 5 | Training Loss : 0.1276\n",
      "Epoch : 5 | Training Loss : 0.1477\n",
      "Epoch : 5 | Training Loss : 0.1261\n",
      "Epoch : 5 | Training Loss : 0.1527\n",
      "Epoch : 5 | Training Loss : 0.0873\n",
      "Epoch : 5 | Training Loss : 0.0546\n",
      "Epoch : 5 | Training Loss : 0.0605\n",
      "Epoch : 5 | Training Loss : 0.1067\n",
      "Epoch : 5 | Training Loss : 0.1059\n",
      "Epoch : 5 | Training Loss : 0.1165\n",
      "Epoch : 5 | Training Loss : 0.1074\n",
      "Epoch : 5 | Training Loss : 0.0710\n",
      "Epoch : 5 | Training Loss : 0.0687\n",
      "Epoch : 5 | Training Loss : 0.0505\n",
      "Epoch : 5 | Training Loss : 0.1501\n",
      "Epoch : 5 | Training Loss : 0.0955\n",
      "Epoch : 5 | Training Loss : 0.0967\n",
      "Epoch : 5 | Training Loss : 0.0673\n",
      "Epoch : 5 | Training Loss : 0.0980\n",
      "Epoch : 5 | Training Loss : 0.1485\n",
      "Epoch : 5 | Training Loss : 0.1103\n",
      "Epoch : 5 | Training Loss : 0.2141\n",
      "Epoch : 5 | Training Loss : 0.0649\n",
      "Epoch : 5 | Training Loss : 0.1162\n",
      "Epoch : 5 | Training Loss : 0.1450\n",
      "Epoch : 5 | Training Loss : 0.1254\n",
      "Epoch : 5 | Training Loss : 0.1009\n",
      "Epoch : 5 | Training Loss : 0.0698\n",
      "Epoch : 5 | Training Loss : 0.1784\n",
      "Epoch : 5 | Training Loss : 0.0746\n",
      "Epoch : 5 | Training Loss : 0.2134\n",
      "Epoch : 5 | Training Loss : 0.1945\n",
      "Epoch : 5 | Training Loss : 0.1023\n",
      "Epoch : 5 | Training Loss : 0.1049\n",
      "Epoch : 5 | Training Loss : 0.1143\n",
      "Epoch : 5 | Training Loss : 0.1112\n",
      "Epoch : 5 | Training Loss : 0.0707\n",
      "Epoch : 5 | Training Loss : 0.0572\n",
      "Epoch : 5 | Training Loss : 0.1022\n",
      "Epoch : 5 | Training Loss : 0.1556\n",
      "Epoch : 5 | Training Loss : 0.1227\n",
      "Epoch : 5 | Training Loss : 0.0815\n",
      "Epoch : 5 | Training Loss : 0.0724\n",
      "Epoch : 5 | Training Loss : 0.0902\n",
      "Epoch : 5 | Training Loss : 0.0874\n",
      "Epoch : 5 | Training Loss : 0.1117\n",
      "Epoch : 5 | Training Loss : 0.0782\n",
      "Epoch : 5 | Training Loss : 0.0816\n",
      "Epoch : 5 | Training Loss : 0.1050\n",
      "Epoch : 5 | Training Loss : 0.1037\n",
      "Epoch : 5 | Training Loss : 0.1268\n",
      "Epoch : 5 | Training Loss : 0.2037\n",
      "Epoch : 5 | Training Loss : 0.0588\n",
      "Epoch : 5 | Training Loss : 0.1268\n",
      "Epoch : 5 | Training Loss : 0.1490\n",
      "Epoch : 5 | Training Loss : 0.1194\n",
      "Epoch : 5 | Training Loss : 0.0973\n",
      "Epoch : 5 | Training Loss : 0.1525\n",
      "Epoch : 5 | Training Loss : 0.1184\n",
      "Epoch : 5 | Training Loss : 0.1127\n",
      "Epoch : 5 | Training Loss : 0.1186\n",
      "Epoch : 5 | Training Loss : 0.1096\n",
      "Epoch : 5 | Training Loss : 0.0992\n",
      "Epoch : 5 | Training Loss : 0.0897\n",
      "Epoch : 5 | Training Loss : 0.1711\n",
      "Epoch : 5 | Training Loss : 0.1098\n",
      "Epoch : 5 | Training Loss : 0.1001\n",
      "Epoch : 6 | Training Loss : 0.0616\n",
      "Epoch : 6 | Training Loss : 0.1320\n",
      "Epoch : 6 | Training Loss : 0.0986\n",
      "Epoch : 6 | Training Loss : 0.0414\n",
      "Epoch : 6 | Training Loss : 0.0939\n",
      "Epoch : 6 | Training Loss : 0.0663\n",
      "Epoch : 6 | Training Loss : 0.0667\n",
      "Epoch : 6 | Training Loss : 0.0846\n",
      "Epoch : 6 | Training Loss : 0.0794\n",
      "Epoch : 6 | Training Loss : 0.1023\n",
      "Epoch : 6 | Training Loss : 0.0432\n",
      "Epoch : 6 | Training Loss : 0.0518\n",
      "Epoch : 6 | Training Loss : 0.0597\n",
      "Epoch : 6 | Training Loss : 0.0572\n",
      "Epoch : 6 | Training Loss : 0.0922\n",
      "Epoch : 6 | Training Loss : 0.0490\n",
      "Epoch : 6 | Training Loss : 0.0423\n",
      "Epoch : 6 | Training Loss : 0.0488\n",
      "Epoch : 6 | Training Loss : 0.0971\n",
      "Epoch : 6 | Training Loss : 0.0406\n",
      "Epoch : 6 | Training Loss : 0.0977\n",
      "Epoch : 6 | Training Loss : 0.0405\n",
      "Epoch : 6 | Training Loss : 0.0579\n",
      "Epoch : 6 | Training Loss : 0.0605\n",
      "Epoch : 6 | Training Loss : 0.0397\n",
      "Epoch : 6 | Training Loss : 0.0620\n",
      "Epoch : 6 | Training Loss : 0.0430\n",
      "Epoch : 6 | Training Loss : 0.0750\n",
      "Epoch : 6 | Training Loss : 0.0546\n",
      "Epoch : 6 | Training Loss : 0.0850\n",
      "Epoch : 6 | Training Loss : 0.0406\n",
      "Epoch : 6 | Training Loss : 0.0615\n",
      "Epoch : 6 | Training Loss : 0.0337\n",
      "Epoch : 6 | Training Loss : 0.0373\n",
      "Epoch : 6 | Training Loss : 0.0481\n",
      "Epoch : 6 | Training Loss : 0.1378\n",
      "Epoch : 6 | Training Loss : 0.0648\n",
      "Epoch : 6 | Training Loss : 0.0213\n",
      "Epoch : 6 | Training Loss : 0.0748\n",
      "Epoch : 6 | Training Loss : 0.1032\n",
      "Epoch : 6 | Training Loss : 0.0338\n",
      "Epoch : 6 | Training Loss : 0.0501\n",
      "Epoch : 6 | Training Loss : 0.1283\n",
      "Epoch : 6 | Training Loss : 0.0610\n",
      "Epoch : 6 | Training Loss : 0.0559\n",
      "Epoch : 6 | Training Loss : 0.0384\n",
      "Epoch : 6 | Training Loss : 0.0995\n",
      "Epoch : 6 | Training Loss : 0.0898\n",
      "Epoch : 6 | Training Loss : 0.0734\n",
      "Epoch : 6 | Training Loss : 0.1113\n",
      "Epoch : 6 | Training Loss : 0.0687\n",
      "Epoch : 6 | Training Loss : 0.0349\n",
      "Epoch : 6 | Training Loss : 0.0860\n",
      "Epoch : 6 | Training Loss : 0.0906\n",
      "Epoch : 6 | Training Loss : 0.0475\n",
      "Epoch : 6 | Training Loss : 0.0589\n",
      "Epoch : 6 | Training Loss : 0.0392\n",
      "Epoch : 6 | Training Loss : 0.0489\n",
      "Epoch : 6 | Training Loss : 0.0625\n",
      "Epoch : 6 | Training Loss : 0.0628\n",
      "Epoch : 6 | Training Loss : 0.0530\n",
      "Epoch : 6 | Training Loss : 0.1018\n",
      "Epoch : 6 | Training Loss : 0.0386\n",
      "Epoch : 6 | Training Loss : 0.0473\n",
      "Epoch : 6 | Training Loss : 0.0648\n",
      "Epoch : 6 | Training Loss : 0.0536\n",
      "Epoch : 6 | Training Loss : 0.0964\n",
      "Epoch : 6 | Training Loss : 0.0615\n",
      "Epoch : 6 | Training Loss : 0.0760\n",
      "Epoch : 6 | Training Loss : 0.0435\n",
      "Epoch : 6 | Training Loss : 0.0784\n",
      "Epoch : 6 | Training Loss : 0.0610\n",
      "Epoch : 6 | Training Loss : 0.0591\n",
      "Epoch : 6 | Training Loss : 0.0448\n",
      "Epoch : 6 | Training Loss : 0.0499\n",
      "Epoch : 6 | Training Loss : 0.0433\n",
      "Epoch : 6 | Training Loss : 0.1001\n",
      "Epoch : 6 | Training Loss : 0.0469\n",
      "Epoch : 6 | Training Loss : 0.0608\n",
      "Epoch : 6 | Training Loss : 0.0413\n",
      "Epoch : 6 | Training Loss : 0.0388\n",
      "Epoch : 6 | Training Loss : 0.0485\n",
      "Epoch : 6 | Training Loss : 0.0866\n",
      "Epoch : 6 | Training Loss : 0.0502\n",
      "Epoch : 6 | Training Loss : 0.0828\n",
      "Epoch : 6 | Training Loss : 0.0669\n",
      "Epoch : 6 | Training Loss : 0.0581\n",
      "Epoch : 6 | Training Loss : 0.0749\n",
      "Epoch : 6 | Training Loss : 0.0410\n",
      "Epoch : 6 | Training Loss : 0.0349\n",
      "Epoch : 6 | Training Loss : 0.0457\n",
      "Epoch : 6 | Training Loss : 0.0493\n",
      "Epoch : 6 | Training Loss : 0.0581\n",
      "Epoch : 6 | Training Loss : 0.0633\n",
      "Epoch : 6 | Training Loss : 0.1218\n",
      "Epoch : 6 | Training Loss : 0.0642\n",
      "Epoch : 6 | Training Loss : 0.0838\n",
      "Epoch : 6 | Training Loss : 0.1037\n",
      "Epoch : 6 | Training Loss : 0.0749\n",
      "Epoch : 6 | Training Loss : 0.0694\n",
      "Epoch : 6 | Training Loss : 0.0788\n",
      "Epoch : 6 | Training Loss : 0.0522\n",
      "Epoch : 6 | Training Loss : 0.0903\n",
      "Epoch : 6 | Training Loss : 0.0735\n",
      "Epoch : 6 | Training Loss : 0.0799\n",
      "Epoch : 6 | Training Loss : 0.0440\n",
      "Epoch : 6 | Training Loss : 0.0785\n",
      "Epoch : 6 | Training Loss : 0.0739\n",
      "Epoch : 6 | Training Loss : 0.0430\n",
      "Epoch : 6 | Training Loss : 0.0428\n",
      "Epoch : 6 | Training Loss : 0.0624\n",
      "Epoch : 6 | Training Loss : 0.0499\n",
      "Epoch : 6 | Training Loss : 0.0540\n",
      "Epoch : 6 | Training Loss : 0.0539\n",
      "Epoch : 6 | Training Loss : 0.0784\n",
      "Epoch : 6 | Training Loss : 0.0533\n",
      "Epoch : 6 | Training Loss : 0.1192\n",
      "Epoch : 6 | Training Loss : 0.0298\n",
      "Epoch : 6 | Training Loss : 0.0547\n",
      "Epoch : 6 | Training Loss : 0.0431\n",
      "Epoch : 6 | Training Loss : 0.0432\n",
      "Epoch : 6 | Training Loss : 0.0544\n",
      "Epoch : 6 | Training Loss : 0.0787\n",
      "Epoch : 6 | Training Loss : 0.0716\n",
      "Epoch : 6 | Training Loss : 0.0337\n",
      "Epoch : 6 | Training Loss : 0.0653\n",
      "Epoch : 6 | Training Loss : 0.0409\n",
      "Epoch : 6 | Training Loss : 0.0461\n",
      "Epoch : 6 | Training Loss : 0.0412\n",
      "Epoch : 6 | Training Loss : 0.0719\n",
      "Epoch : 6 | Training Loss : 0.0437\n",
      "Epoch : 6 | Training Loss : 0.0401\n",
      "Epoch : 6 | Training Loss : 0.0510\n",
      "Epoch : 6 | Training Loss : 0.0525\n",
      "Epoch : 6 | Training Loss : 0.0536\n",
      "Epoch : 6 | Training Loss : 0.0349\n",
      "Epoch : 6 | Training Loss : 0.0468\n",
      "Epoch : 6 | Training Loss : 0.1020\n",
      "Epoch : 6 | Training Loss : 0.0703\n",
      "Epoch : 6 | Training Loss : 0.0580\n",
      "Epoch : 6 | Training Loss : 0.0382\n",
      "Epoch : 6 | Training Loss : 0.0777\n",
      "Epoch : 6 | Training Loss : 0.1013\n",
      "Epoch : 6 | Training Loss : 0.0652\n",
      "Epoch : 6 | Training Loss : 0.0771\n",
      "Epoch : 6 | Training Loss : 0.0596\n",
      "Epoch : 6 | Training Loss : 0.0557\n",
      "Epoch : 6 | Training Loss : 0.0234\n",
      "Epoch : 6 | Training Loss : 0.0585\n",
      "Epoch : 6 | Training Loss : 0.1397\n",
      "Epoch : 6 | Training Loss : 0.0740\n",
      "Epoch : 6 | Training Loss : 0.0630\n",
      "Epoch : 6 | Training Loss : 0.0930\n",
      "Epoch : 6 | Training Loss : 0.0820\n",
      "Epoch : 6 | Training Loss : 0.0770\n",
      "Epoch : 6 | Training Loss : 0.0579\n",
      "Epoch : 6 | Training Loss : 0.0894\n",
      "Epoch : 6 | Training Loss : 0.0575\n",
      "Epoch : 6 | Training Loss : 0.0696\n",
      "Epoch : 6 | Training Loss : 0.1027\n",
      "Epoch : 6 | Training Loss : 0.1002\n",
      "Epoch : 6 | Training Loss : 0.0614\n",
      "Epoch : 6 | Training Loss : 0.0586\n",
      "Epoch : 6 | Training Loss : 0.0508\n",
      "Epoch : 6 | Training Loss : 0.0581\n",
      "Epoch : 6 | Training Loss : 0.0305\n",
      "Epoch : 6 | Training Loss : 0.0719\n",
      "Epoch : 6 | Training Loss : 0.1059\n",
      "Epoch : 6 | Training Loss : 0.0431\n",
      "Epoch : 6 | Training Loss : 0.0557\n",
      "Epoch : 6 | Training Loss : 0.0475\n",
      "Epoch : 6 | Training Loss : 0.0757\n",
      "Epoch : 6 | Training Loss : 0.0697\n",
      "Epoch : 6 | Training Loss : 0.0902\n",
      "Epoch : 6 | Training Loss : 0.0765\n",
      "Epoch : 6 | Training Loss : 0.0583\n",
      "Epoch : 6 | Training Loss : 0.0234\n",
      "Epoch : 6 | Training Loss : 0.0973\n",
      "Epoch : 6 | Training Loss : 0.0626\n",
      "Epoch : 6 | Training Loss : 0.0590\n",
      "Epoch : 6 | Training Loss : 0.0550\n",
      "Epoch : 6 | Training Loss : 0.0965\n",
      "Epoch : 6 | Training Loss : 0.0591\n",
      "Epoch : 6 | Training Loss : 0.0914\n",
      "Epoch : 6 | Training Loss : 0.0632\n",
      "Epoch : 6 | Training Loss : 0.1155\n",
      "Epoch : 6 | Training Loss : 0.0520\n",
      "Epoch : 6 | Training Loss : 0.1107\n",
      "Epoch : 6 | Training Loss : 0.1076\n",
      "Epoch : 6 | Training Loss : 0.0458\n",
      "Epoch : 6 | Training Loss : 0.0834\n",
      "Epoch : 6 | Training Loss : 0.0621\n",
      "Epoch : 6 | Training Loss : 0.0697\n",
      "Epoch : 6 | Training Loss : 0.0378\n",
      "Epoch : 6 | Training Loss : 0.0787\n",
      "Epoch : 6 | Training Loss : 0.0587\n",
      "Epoch : 6 | Training Loss : 0.0495\n",
      "Epoch : 6 | Training Loss : 0.1102\n",
      "Epoch : 6 | Training Loss : 0.1022\n",
      "Epoch : 6 | Training Loss : 0.0843\n",
      "Epoch : 6 | Training Loss : 0.0900\n",
      "Epoch : 6 | Training Loss : 0.0711\n",
      "Epoch : 6 | Training Loss : 0.0409\n",
      "Epoch : 6 | Training Loss : 0.0965\n",
      "Epoch : 6 | Training Loss : 0.1115\n",
      "Epoch : 6 | Training Loss : 0.0476\n",
      "Epoch : 6 | Training Loss : 0.0576\n",
      "Epoch : 6 | Training Loss : 0.1192\n",
      "Epoch : 6 | Training Loss : 0.0868\n",
      "Epoch : 6 | Training Loss : 0.0368\n",
      "Epoch : 6 | Training Loss : 0.1103\n",
      "Epoch : 6 | Training Loss : 0.0735\n",
      "Epoch : 6 | Training Loss : 0.0655\n",
      "Epoch : 6 | Training Loss : 0.0732\n",
      "Epoch : 6 | Training Loss : 0.0514\n",
      "Epoch : 6 | Training Loss : 0.0580\n",
      "Epoch : 6 | Training Loss : 0.0702\n",
      "Epoch : 6 | Training Loss : 0.0686\n",
      "Epoch : 6 | Training Loss : 0.1024\n",
      "Epoch : 6 | Training Loss : 0.0694\n",
      "Epoch : 6 | Training Loss : 0.0700\n",
      "Epoch : 6 | Training Loss : 0.0538\n",
      "Epoch : 6 | Training Loss : 0.1133\n",
      "Epoch : 6 | Training Loss : 0.0618\n",
      "Epoch : 6 | Training Loss : 0.0470\n",
      "Epoch : 6 | Training Loss : 0.0805\n",
      "Epoch : 6 | Training Loss : 0.0613\n",
      "Epoch : 6 | Training Loss : 0.0334\n",
      "Epoch : 6 | Training Loss : 0.0613\n",
      "Epoch : 6 | Training Loss : 0.0614\n",
      "Epoch : 6 | Training Loss : 0.0897\n",
      "Epoch : 6 | Training Loss : 0.0704\n",
      "Epoch : 6 | Training Loss : 0.0332\n",
      "Epoch : 6 | Training Loss : 0.0798\n",
      "Epoch : 6 | Training Loss : 0.0734\n",
      "Epoch : 6 | Training Loss : 0.0942\n",
      "Epoch : 6 | Training Loss : 0.0593\n",
      "Epoch : 6 | Training Loss : 0.0715\n",
      "Epoch : 6 | Training Loss : 0.1195\n",
      "Epoch : 6 | Training Loss : 0.0902\n",
      "Epoch : 6 | Training Loss : 0.0895\n",
      "Epoch : 6 | Training Loss : 0.0888\n",
      "Epoch : 6 | Training Loss : 0.0433\n",
      "Epoch : 7 | Training Loss : 0.0518\n",
      "Epoch : 7 | Training Loss : 0.0400\n",
      "Epoch : 7 | Training Loss : 0.0250\n",
      "Epoch : 7 | Training Loss : 0.0710\n",
      "Epoch : 7 | Training Loss : 0.0248\n",
      "Epoch : 7 | Training Loss : 0.0401\n",
      "Epoch : 7 | Training Loss : 0.0643\n",
      "Epoch : 7 | Training Loss : 0.0577\n",
      "Epoch : 7 | Training Loss : 0.0554\n",
      "Epoch : 7 | Training Loss : 0.0453\n",
      "Epoch : 7 | Training Loss : 0.0252\n",
      "Epoch : 7 | Training Loss : 0.0420\n",
      "Epoch : 7 | Training Loss : 0.0235\n",
      "Epoch : 7 | Training Loss : 0.0363\n",
      "Epoch : 7 | Training Loss : 0.0651\n",
      "Epoch : 7 | Training Loss : 0.0406\n",
      "Epoch : 7 | Training Loss : 0.0335\n",
      "Epoch : 7 | Training Loss : 0.0352\n",
      "Epoch : 7 | Training Loss : 0.0445\n",
      "Epoch : 7 | Training Loss : 0.0428\n",
      "Epoch : 7 | Training Loss : 0.0324\n",
      "Epoch : 7 | Training Loss : 0.0716\n",
      "Epoch : 7 | Training Loss : 0.0455\n",
      "Epoch : 7 | Training Loss : 0.0492\n",
      "Epoch : 7 | Training Loss : 0.0713\n",
      "Epoch : 7 | Training Loss : 0.0164\n",
      "Epoch : 7 | Training Loss : 0.0324\n",
      "Epoch : 7 | Training Loss : 0.0176\n",
      "Epoch : 7 | Training Loss : 0.0631\n",
      "Epoch : 7 | Training Loss : 0.0466\n",
      "Epoch : 7 | Training Loss : 0.0204\n",
      "Epoch : 7 | Training Loss : 0.0149\n",
      "Epoch : 7 | Training Loss : 0.0291\n",
      "Epoch : 7 | Training Loss : 0.0306\n",
      "Epoch : 7 | Training Loss : 0.0305\n",
      "Epoch : 7 | Training Loss : 0.0424\n",
      "Epoch : 7 | Training Loss : 0.0280\n",
      "Epoch : 7 | Training Loss : 0.0389\n",
      "Epoch : 7 | Training Loss : 0.0714\n",
      "Epoch : 7 | Training Loss : 0.0249\n",
      "Epoch : 7 | Training Loss : 0.0211\n",
      "Epoch : 7 | Training Loss : 0.0700\n",
      "Epoch : 7 | Training Loss : 0.0527\n",
      "Epoch : 7 | Training Loss : 0.0342\n",
      "Epoch : 7 | Training Loss : 0.0645\n",
      "Epoch : 7 | Training Loss : 0.0173\n",
      "Epoch : 7 | Training Loss : 0.0766\n",
      "Epoch : 7 | Training Loss : 0.0562\n",
      "Epoch : 7 | Training Loss : 0.0303\n",
      "Epoch : 7 | Training Loss : 0.0404\n",
      "Epoch : 7 | Training Loss : 0.0187\n",
      "Epoch : 7 | Training Loss : 0.0205\n",
      "Epoch : 7 | Training Loss : 0.0337\n",
      "Epoch : 7 | Training Loss : 0.0418\n",
      "Epoch : 7 | Training Loss : 0.0410\n",
      "Epoch : 7 | Training Loss : 0.0187\n",
      "Epoch : 7 | Training Loss : 0.0426\n",
      "Epoch : 7 | Training Loss : 0.0525\n",
      "Epoch : 7 | Training Loss : 0.0429\n",
      "Epoch : 7 | Training Loss : 0.0151\n",
      "Epoch : 7 | Training Loss : 0.0393\n",
      "Epoch : 7 | Training Loss : 0.0915\n",
      "Epoch : 7 | Training Loss : 0.0280\n",
      "Epoch : 7 | Training Loss : 0.0441\n",
      "Epoch : 7 | Training Loss : 0.0526\n",
      "Epoch : 7 | Training Loss : 0.0272\n",
      "Epoch : 7 | Training Loss : 0.0268\n",
      "Epoch : 7 | Training Loss : 0.0300\n",
      "Epoch : 7 | Training Loss : 0.0392\n",
      "Epoch : 7 | Training Loss : 0.0576\n",
      "Epoch : 7 | Training Loss : 0.0207\n",
      "Epoch : 7 | Training Loss : 0.0161\n",
      "Epoch : 7 | Training Loss : 0.0546\n",
      "Epoch : 7 | Training Loss : 0.0489\n",
      "Epoch : 7 | Training Loss : 0.0428\n",
      "Epoch : 7 | Training Loss : 0.0275\n",
      "Epoch : 7 | Training Loss : 0.0473\n",
      "Epoch : 7 | Training Loss : 0.0456\n",
      "Epoch : 7 | Training Loss : 0.0175\n",
      "Epoch : 7 | Training Loss : 0.0355\n",
      "Epoch : 7 | Training Loss : 0.0586\n",
      "Epoch : 7 | Training Loss : 0.0293\n",
      "Epoch : 7 | Training Loss : 0.0316\n",
      "Epoch : 7 | Training Loss : 0.0245\n",
      "Epoch : 7 | Training Loss : 0.0340\n",
      "Epoch : 7 | Training Loss : 0.0240\n",
      "Epoch : 7 | Training Loss : 0.0247\n",
      "Epoch : 7 | Training Loss : 0.0174\n",
      "Epoch : 7 | Training Loss : 0.0267\n",
      "Epoch : 7 | Training Loss : 0.0436\n",
      "Epoch : 7 | Training Loss : 0.0381\n",
      "Epoch : 7 | Training Loss : 0.0308\n",
      "Epoch : 7 | Training Loss : 0.0626\n",
      "Epoch : 7 | Training Loss : 0.0331\n",
      "Epoch : 7 | Training Loss : 0.0428\n",
      "Epoch : 7 | Training Loss : 0.0464\n",
      "Epoch : 7 | Training Loss : 0.0328\n",
      "Epoch : 7 | Training Loss : 0.0523\n",
      "Epoch : 7 | Training Loss : 0.0328\n",
      "Epoch : 7 | Training Loss : 0.0307\n",
      "Epoch : 7 | Training Loss : 0.0438\n",
      "Epoch : 7 | Training Loss : 0.0463\n",
      "Epoch : 7 | Training Loss : 0.0523\n",
      "Epoch : 7 | Training Loss : 0.0313\n",
      "Epoch : 7 | Training Loss : 0.0331\n",
      "Epoch : 7 | Training Loss : 0.0294\n",
      "Epoch : 7 | Training Loss : 0.0301\n",
      "Epoch : 7 | Training Loss : 0.0351\n",
      "Epoch : 7 | Training Loss : 0.0287\n",
      "Epoch : 7 | Training Loss : 0.0136\n",
      "Epoch : 7 | Training Loss : 0.0305\n",
      "Epoch : 7 | Training Loss : 0.0820\n",
      "Epoch : 7 | Training Loss : 0.0609\n",
      "Epoch : 7 | Training Loss : 0.0344\n",
      "Epoch : 7 | Training Loss : 0.0493\n",
      "Epoch : 7 | Training Loss : 0.0237\n",
      "Epoch : 7 | Training Loss : 0.0411\n",
      "Epoch : 7 | Training Loss : 0.0490\n",
      "Epoch : 7 | Training Loss : 0.0373\n",
      "Epoch : 7 | Training Loss : 0.0337\n",
      "Epoch : 7 | Training Loss : 0.0484\n",
      "Epoch : 7 | Training Loss : 0.1336\n",
      "Epoch : 7 | Training Loss : 0.0432\n",
      "Epoch : 7 | Training Loss : 0.0491\n",
      "Epoch : 7 | Training Loss : 0.0297\n",
      "Epoch : 7 | Training Loss : 0.0674\n",
      "Epoch : 7 | Training Loss : 0.0296\n",
      "Epoch : 7 | Training Loss : 0.0486\n",
      "Epoch : 7 | Training Loss : 0.0422\n",
      "Epoch : 7 | Training Loss : 0.0477\n",
      "Epoch : 7 | Training Loss : 0.0352\n",
      "Epoch : 7 | Training Loss : 0.0554\n",
      "Epoch : 7 | Training Loss : 0.0170\n",
      "Epoch : 7 | Training Loss : 0.0638\n",
      "Epoch : 7 | Training Loss : 0.0390\n",
      "Epoch : 7 | Training Loss : 0.0479\n",
      "Epoch : 7 | Training Loss : 0.0730\n",
      "Epoch : 7 | Training Loss : 0.0397\n",
      "Epoch : 7 | Training Loss : 0.0468\n",
      "Epoch : 7 | Training Loss : 0.0523\n",
      "Epoch : 7 | Training Loss : 0.0480\n",
      "Epoch : 7 | Training Loss : 0.0450\n",
      "Epoch : 7 | Training Loss : 0.0561\n",
      "Epoch : 7 | Training Loss : 0.0433\n",
      "Epoch : 7 | Training Loss : 0.0387\n",
      "Epoch : 7 | Training Loss : 0.0302\n",
      "Epoch : 7 | Training Loss : 0.0664\n",
      "Epoch : 7 | Training Loss : 0.0353\n",
      "Epoch : 7 | Training Loss : 0.0384\n",
      "Epoch : 7 | Training Loss : 0.0427\n",
      "Epoch : 7 | Training Loss : 0.0414\n",
      "Epoch : 7 | Training Loss : 0.0387\n",
      "Epoch : 7 | Training Loss : 0.0370\n",
      "Epoch : 7 | Training Loss : 0.0324\n",
      "Epoch : 7 | Training Loss : 0.0455\n",
      "Epoch : 7 | Training Loss : 0.0250\n",
      "Epoch : 7 | Training Loss : 0.0481\n",
      "Epoch : 7 | Training Loss : 0.0181\n",
      "Epoch : 7 | Training Loss : 0.0434\n",
      "Epoch : 7 | Training Loss : 0.0145\n",
      "Epoch : 7 | Training Loss : 0.0219\n",
      "Epoch : 7 | Training Loss : 0.0589\n",
      "Epoch : 7 | Training Loss : 0.0275\n",
      "Epoch : 7 | Training Loss : 0.0367\n",
      "Epoch : 7 | Training Loss : 0.0253\n",
      "Epoch : 7 | Training Loss : 0.0380\n",
      "Epoch : 7 | Training Loss : 0.0332\n",
      "Epoch : 7 | Training Loss : 0.0163\n",
      "Epoch : 7 | Training Loss : 0.0464\n",
      "Epoch : 7 | Training Loss : 0.0527\n",
      "Epoch : 7 | Training Loss : 0.0340\n",
      "Epoch : 7 | Training Loss : 0.0864\n",
      "Epoch : 7 | Training Loss : 0.0268\n",
      "Epoch : 7 | Training Loss : 0.0366\n",
      "Epoch : 7 | Training Loss : 0.0281\n",
      "Epoch : 7 | Training Loss : 0.0685\n",
      "Epoch : 7 | Training Loss : 0.0222\n",
      "Epoch : 7 | Training Loss : 0.0523\n",
      "Epoch : 7 | Training Loss : 0.0506\n",
      "Epoch : 7 | Training Loss : 0.0321\n",
      "Epoch : 7 | Training Loss : 0.0738\n",
      "Epoch : 7 | Training Loss : 0.0936\n",
      "Epoch : 7 | Training Loss : 0.0574\n",
      "Epoch : 7 | Training Loss : 0.0364\n",
      "Epoch : 7 | Training Loss : 0.0641\n",
      "Epoch : 7 | Training Loss : 0.0712\n",
      "Epoch : 7 | Training Loss : 0.0977\n",
      "Epoch : 7 | Training Loss : 0.0312\n",
      "Epoch : 7 | Training Loss : 0.0628\n",
      "Epoch : 7 | Training Loss : 0.0577\n",
      "Epoch : 7 | Training Loss : 0.1287\n",
      "Epoch : 7 | Training Loss : 0.0713\n",
      "Epoch : 7 | Training Loss : 0.0714\n",
      "Epoch : 7 | Training Loss : 0.0370\n",
      "Epoch : 7 | Training Loss : 0.0500\n",
      "Epoch : 7 | Training Loss : 0.0415\n",
      "Epoch : 7 | Training Loss : 0.0650\n",
      "Epoch : 7 | Training Loss : 0.0444\n",
      "Epoch : 7 | Training Loss : 0.0528\n",
      "Epoch : 7 | Training Loss : 0.0648\n",
      "Epoch : 7 | Training Loss : 0.0348\n",
      "Epoch : 7 | Training Loss : 0.0632\n",
      "Epoch : 7 | Training Loss : 0.0361\n",
      "Epoch : 7 | Training Loss : 0.0269\n",
      "Epoch : 7 | Training Loss : 0.0334\n",
      "Epoch : 7 | Training Loss : 0.0787\n",
      "Epoch : 7 | Training Loss : 0.0830\n",
      "Epoch : 7 | Training Loss : 0.0393\n",
      "Epoch : 7 | Training Loss : 0.0455\n",
      "Epoch : 7 | Training Loss : 0.0669\n",
      "Epoch : 7 | Training Loss : 0.0806\n",
      "Epoch : 7 | Training Loss : 0.0501\n",
      "Epoch : 7 | Training Loss : 0.0552\n",
      "Epoch : 7 | Training Loss : 0.0407\n",
      "Epoch : 7 | Training Loss : 0.0781\n",
      "Epoch : 7 | Training Loss : 0.0236\n",
      "Epoch : 7 | Training Loss : 0.0696\n",
      "Epoch : 7 | Training Loss : 0.0748\n",
      "Epoch : 7 | Training Loss : 0.0358\n",
      "Epoch : 7 | Training Loss : 0.0477\n",
      "Epoch : 7 | Training Loss : 0.0429\n",
      "Epoch : 7 | Training Loss : 0.0526\n",
      "Epoch : 7 | Training Loss : 0.0569\n",
      "Epoch : 7 | Training Loss : 0.0512\n",
      "Epoch : 7 | Training Loss : 0.0221\n",
      "Epoch : 7 | Training Loss : 0.0355\n",
      "Epoch : 7 | Training Loss : 0.0524\n",
      "Epoch : 7 | Training Loss : 0.0420\n",
      "Epoch : 7 | Training Loss : 0.0250\n",
      "Epoch : 7 | Training Loss : 0.0523\n",
      "Epoch : 7 | Training Loss : 0.0532\n",
      "Epoch : 7 | Training Loss : 0.0391\n",
      "Epoch : 7 | Training Loss : 0.0491\n",
      "Epoch : 7 | Training Loss : 0.0386\n",
      "Epoch : 7 | Training Loss : 0.0381\n",
      "Epoch : 7 | Training Loss : 0.0543\n",
      "Epoch : 7 | Training Loss : 0.0159\n",
      "Epoch : 7 | Training Loss : 0.0442\n",
      "Epoch : 7 | Training Loss : 0.0248\n",
      "Epoch : 7 | Training Loss : 0.0338\n",
      "Epoch : 7 | Training Loss : 0.0258\n",
      "Epoch : 7 | Training Loss : 0.0304\n",
      "Epoch : 7 | Training Loss : 0.0156\n",
      "Epoch : 8 | Training Loss : 0.0134\n",
      "Epoch : 8 | Training Loss : 0.0204\n",
      "Epoch : 8 | Training Loss : 0.0233\n",
      "Epoch : 8 | Training Loss : 0.0336\n",
      "Epoch : 8 | Training Loss : 0.0350\n",
      "Epoch : 8 | Training Loss : 0.0202\n",
      "Epoch : 8 | Training Loss : 0.0108\n",
      "Epoch : 8 | Training Loss : 0.0191\n",
      "Epoch : 8 | Training Loss : 0.0221\n",
      "Epoch : 8 | Training Loss : 0.0234\n",
      "Epoch : 8 | Training Loss : 0.0196\n",
      "Epoch : 8 | Training Loss : 0.0258\n",
      "Epoch : 8 | Training Loss : 0.0463\n",
      "Epoch : 8 | Training Loss : 0.0469\n",
      "Epoch : 8 | Training Loss : 0.0241\n",
      "Epoch : 8 | Training Loss : 0.0260\n",
      "Epoch : 8 | Training Loss : 0.0122\n",
      "Epoch : 8 | Training Loss : 0.0217\n",
      "Epoch : 8 | Training Loss : 0.0099\n",
      "Epoch : 8 | Training Loss : 0.0266\n",
      "Epoch : 8 | Training Loss : 0.0205\n",
      "Epoch : 8 | Training Loss : 0.0165\n",
      "Epoch : 8 | Training Loss : 0.0206\n",
      "Epoch : 8 | Training Loss : 0.0744\n",
      "Epoch : 8 | Training Loss : 0.0173\n",
      "Epoch : 8 | Training Loss : 0.0306\n",
      "Epoch : 8 | Training Loss : 0.0229\n",
      "Epoch : 8 | Training Loss : 0.0232\n",
      "Epoch : 8 | Training Loss : 0.0222\n",
      "Epoch : 8 | Training Loss : 0.0250\n",
      "Epoch : 8 | Training Loss : 0.0358\n",
      "Epoch : 8 | Training Loss : 0.0262\n",
      "Epoch : 8 | Training Loss : 0.0223\n",
      "Epoch : 8 | Training Loss : 0.0164\n",
      "Epoch : 8 | Training Loss : 0.0497\n",
      "Epoch : 8 | Training Loss : 0.0221\n",
      "Epoch : 8 | Training Loss : 0.0227\n",
      "Epoch : 8 | Training Loss : 0.0108\n",
      "Epoch : 8 | Training Loss : 0.0422\n",
      "Epoch : 8 | Training Loss : 0.0315\n",
      "Epoch : 8 | Training Loss : 0.0208\n",
      "Epoch : 8 | Training Loss : 0.0272\n",
      "Epoch : 8 | Training Loss : 0.0248\n",
      "Epoch : 8 | Training Loss : 0.0174\n",
      "Epoch : 8 | Training Loss : 0.0288\n",
      "Epoch : 8 | Training Loss : 0.0200\n",
      "Epoch : 8 | Training Loss : 0.0284\n",
      "Epoch : 8 | Training Loss : 0.0187\n",
      "Epoch : 8 | Training Loss : 0.0394\n",
      "Epoch : 8 | Training Loss : 0.0090\n",
      "Epoch : 8 | Training Loss : 0.0177\n",
      "Epoch : 8 | Training Loss : 0.0245\n",
      "Epoch : 8 | Training Loss : 0.0240\n",
      "Epoch : 8 | Training Loss : 0.0213\n",
      "Epoch : 8 | Training Loss : 0.0126\n",
      "Epoch : 8 | Training Loss : 0.0109\n",
      "Epoch : 8 | Training Loss : 0.0252\n",
      "Epoch : 8 | Training Loss : 0.0158\n",
      "Epoch : 8 | Training Loss : 0.0425\n",
      "Epoch : 8 | Training Loss : 0.0124\n",
      "Epoch : 8 | Training Loss : 0.0199\n",
      "Epoch : 8 | Training Loss : 0.0149\n",
      "Epoch : 8 | Training Loss : 0.0329\n",
      "Epoch : 8 | Training Loss : 0.0216\n",
      "Epoch : 8 | Training Loss : 0.0174\n",
      "Epoch : 8 | Training Loss : 0.0103\n",
      "Epoch : 8 | Training Loss : 0.0185\n",
      "Epoch : 8 | Training Loss : 0.0188\n",
      "Epoch : 8 | Training Loss : 0.0302\n",
      "Epoch : 8 | Training Loss : 0.0137\n",
      "Epoch : 8 | Training Loss : 0.0115\n",
      "Epoch : 8 | Training Loss : 0.0277\n",
      "Epoch : 8 | Training Loss : 0.0219\n",
      "Epoch : 8 | Training Loss : 0.0633\n",
      "Epoch : 8 | Training Loss : 0.0153\n",
      "Epoch : 8 | Training Loss : 0.0159\n",
      "Epoch : 8 | Training Loss : 0.0414\n",
      "Epoch : 8 | Training Loss : 0.0216\n",
      "Epoch : 8 | Training Loss : 0.0160\n",
      "Epoch : 8 | Training Loss : 0.0064\n",
      "Epoch : 8 | Training Loss : 0.0154\n",
      "Epoch : 8 | Training Loss : 0.0235\n",
      "Epoch : 8 | Training Loss : 0.0132\n",
      "Epoch : 8 | Training Loss : 0.0352\n",
      "Epoch : 8 | Training Loss : 0.0148\n",
      "Epoch : 8 | Training Loss : 0.0247\n",
      "Epoch : 8 | Training Loss : 0.0210\n",
      "Epoch : 8 | Training Loss : 0.0302\n",
      "Epoch : 8 | Training Loss : 0.0280\n",
      "Epoch : 8 | Training Loss : 0.0198\n",
      "Epoch : 8 | Training Loss : 0.0186\n",
      "Epoch : 8 | Training Loss : 0.0107\n",
      "Epoch : 8 | Training Loss : 0.0152\n",
      "Epoch : 8 | Training Loss : 0.0629\n",
      "Epoch : 8 | Training Loss : 0.0087\n",
      "Epoch : 8 | Training Loss : 0.0179\n",
      "Epoch : 8 | Training Loss : 0.0150\n",
      "Epoch : 8 | Training Loss : 0.0157\n",
      "Epoch : 8 | Training Loss : 0.0237\n",
      "Epoch : 8 | Training Loss : 0.0351\n",
      "Epoch : 8 | Training Loss : 0.0202\n",
      "Epoch : 8 | Training Loss : 0.0121\n",
      "Epoch : 8 | Training Loss : 0.0155\n",
      "Epoch : 8 | Training Loss : 0.0223\n",
      "Epoch : 8 | Training Loss : 0.0136\n",
      "Epoch : 8 | Training Loss : 0.0152\n",
      "Epoch : 8 | Training Loss : 0.0265\n",
      "Epoch : 8 | Training Loss : 0.0371\n",
      "Epoch : 8 | Training Loss : 0.0205\n",
      "Epoch : 8 | Training Loss : 0.0223\n",
      "Epoch : 8 | Training Loss : 0.0109\n",
      "Epoch : 8 | Training Loss : 0.0273\n",
      "Epoch : 8 | Training Loss : 0.0158\n",
      "Epoch : 8 | Training Loss : 0.0237\n",
      "Epoch : 8 | Training Loss : 0.0239\n",
      "Epoch : 8 | Training Loss : 0.0260\n",
      "Epoch : 8 | Training Loss : 0.0502\n",
      "Epoch : 8 | Training Loss : 0.0536\n",
      "Epoch : 8 | Training Loss : 0.0253\n",
      "Epoch : 8 | Training Loss : 0.0179\n",
      "Epoch : 8 | Training Loss : 0.0315\n",
      "Epoch : 8 | Training Loss : 0.0330\n",
      "Epoch : 8 | Training Loss : 0.0218\n",
      "Epoch : 8 | Training Loss : 0.0095\n",
      "Epoch : 8 | Training Loss : 0.0324\n",
      "Epoch : 8 | Training Loss : 0.0784\n",
      "Epoch : 8 | Training Loss : 0.0123\n",
      "Epoch : 8 | Training Loss : 0.0235\n",
      "Epoch : 8 | Training Loss : 0.0541\n",
      "Epoch : 8 | Training Loss : 0.0723\n",
      "Epoch : 8 | Training Loss : 0.0271\n",
      "Epoch : 8 | Training Loss : 0.0127\n",
      "Epoch : 8 | Training Loss : 0.0198\n",
      "Epoch : 8 | Training Loss : 0.0201\n",
      "Epoch : 8 | Training Loss : 0.0227\n",
      "Epoch : 8 | Training Loss : 0.0262\n",
      "Epoch : 8 | Training Loss : 0.0164\n",
      "Epoch : 8 | Training Loss : 0.0308\n",
      "Epoch : 8 | Training Loss : 0.0345\n",
      "Epoch : 8 | Training Loss : 0.0131\n",
      "Epoch : 8 | Training Loss : 0.0251\n",
      "Epoch : 8 | Training Loss : 0.0218\n",
      "Epoch : 8 | Training Loss : 0.0137\n",
      "Epoch : 8 | Training Loss : 0.0471\n",
      "Epoch : 8 | Training Loss : 0.0260\n",
      "Epoch : 8 | Training Loss : 0.0383\n",
      "Epoch : 8 | Training Loss : 0.0191\n",
      "Epoch : 8 | Training Loss : 0.0202\n",
      "Epoch : 8 | Training Loss : 0.0185\n",
      "Epoch : 8 | Training Loss : 0.0342\n",
      "Epoch : 8 | Training Loss : 0.0135\n",
      "Epoch : 8 | Training Loss : 0.0154\n",
      "Epoch : 8 | Training Loss : 0.0207\n",
      "Epoch : 8 | Training Loss : 0.0212\n",
      "Epoch : 8 | Training Loss : 0.0260\n",
      "Epoch : 8 | Training Loss : 0.0261\n",
      "Epoch : 8 | Training Loss : 0.0123\n",
      "Epoch : 8 | Training Loss : 0.0212\n",
      "Epoch : 8 | Training Loss : 0.0193\n",
      "Epoch : 8 | Training Loss : 0.0287\n",
      "Epoch : 8 | Training Loss : 0.0119\n",
      "Epoch : 8 | Training Loss : 0.0143\n",
      "Epoch : 8 | Training Loss : 0.0240\n",
      "Epoch : 8 | Training Loss : 0.0135\n",
      "Epoch : 8 | Training Loss : 0.0191\n",
      "Epoch : 8 | Training Loss : 0.0109\n",
      "Epoch : 8 | Training Loss : 0.0073\n",
      "Epoch : 8 | Training Loss : 0.0079\n",
      "Epoch : 8 | Training Loss : 0.0271\n",
      "Epoch : 8 | Training Loss : 0.0124\n",
      "Epoch : 8 | Training Loss : 0.0141\n",
      "Epoch : 8 | Training Loss : 0.0182\n",
      "Epoch : 8 | Training Loss : 0.0319\n",
      "Epoch : 8 | Training Loss : 0.0206\n",
      "Epoch : 8 | Training Loss : 0.0107\n",
      "Epoch : 8 | Training Loss : 0.0116\n",
      "Epoch : 8 | Training Loss : 0.0224\n",
      "Epoch : 8 | Training Loss : 0.0265\n",
      "Epoch : 8 | Training Loss : 0.0131\n",
      "Epoch : 8 | Training Loss : 0.0388\n",
      "Epoch : 8 | Training Loss : 0.0098\n",
      "Epoch : 8 | Training Loss : 0.0105\n",
      "Epoch : 8 | Training Loss : 0.0083\n",
      "Epoch : 8 | Training Loss : 0.0173\n",
      "Epoch : 8 | Training Loss : 0.0197\n",
      "Epoch : 8 | Training Loss : 0.0147\n",
      "Epoch : 8 | Training Loss : 0.0242\n",
      "Epoch : 8 | Training Loss : 0.0159\n",
      "Epoch : 8 | Training Loss : 0.0377\n",
      "Epoch : 8 | Training Loss : 0.0328\n",
      "Epoch : 8 | Training Loss : 0.0236\n",
      "Epoch : 8 | Training Loss : 0.0089\n",
      "Epoch : 8 | Training Loss : 0.0227\n",
      "Epoch : 8 | Training Loss : 0.0222\n",
      "Epoch : 8 | Training Loss : 0.0448\n",
      "Epoch : 8 | Training Loss : 0.0212\n",
      "Epoch : 8 | Training Loss : 0.0144\n",
      "Epoch : 8 | Training Loss : 0.0141\n",
      "Epoch : 8 | Training Loss : 0.0206\n",
      "Epoch : 8 | Training Loss : 0.0151\n",
      "Epoch : 8 | Training Loss : 0.0093\n",
      "Epoch : 8 | Training Loss : 0.0162\n",
      "Epoch : 8 | Training Loss : 0.0156\n",
      "Epoch : 8 | Training Loss : 0.0368\n",
      "Epoch : 8 | Training Loss : 0.0107\n",
      "Epoch : 8 | Training Loss : 0.0264\n",
      "Epoch : 8 | Training Loss : 0.0102\n",
      "Epoch : 8 | Training Loss : 0.0275\n",
      "Epoch : 8 | Training Loss : 0.0301\n",
      "Epoch : 8 | Training Loss : 0.0072\n",
      "Epoch : 8 | Training Loss : 0.0155\n",
      "Epoch : 8 | Training Loss : 0.0154\n",
      "Epoch : 8 | Training Loss : 0.0336\n",
      "Epoch : 8 | Training Loss : 0.0137\n",
      "Epoch : 8 | Training Loss : 0.0107\n",
      "Epoch : 8 | Training Loss : 0.0292\n",
      "Epoch : 8 | Training Loss : 0.0094\n",
      "Epoch : 8 | Training Loss : 0.0180\n",
      "Epoch : 8 | Training Loss : 0.0151\n",
      "Epoch : 8 | Training Loss : 0.0253\n",
      "Epoch : 8 | Training Loss : 0.0213\n",
      "Epoch : 8 | Training Loss : 0.0172\n",
      "Epoch : 8 | Training Loss : 0.0135\n",
      "Epoch : 8 | Training Loss : 0.0184\n",
      "Epoch : 8 | Training Loss : 0.0191\n",
      "Epoch : 8 | Training Loss : 0.0277\n",
      "Epoch : 8 | Training Loss : 0.0595\n",
      "Epoch : 8 | Training Loss : 0.0229\n",
      "Epoch : 8 | Training Loss : 0.0082\n",
      "Epoch : 8 | Training Loss : 0.0187\n",
      "Epoch : 8 | Training Loss : 0.0125\n",
      "Epoch : 8 | Training Loss : 0.0089\n",
      "Epoch : 8 | Training Loss : 0.0114\n",
      "Epoch : 8 | Training Loss : 0.0162\n",
      "Epoch : 8 | Training Loss : 0.0513\n",
      "Epoch : 8 | Training Loss : 0.0101\n",
      "Epoch : 8 | Training Loss : 0.0243\n",
      "Epoch : 8 | Training Loss : 0.0224\n",
      "Epoch : 8 | Training Loss : 0.0090\n",
      "Epoch : 8 | Training Loss : 0.0101\n",
      "Epoch : 8 | Training Loss : 0.0647\n",
      "Epoch : 8 | Training Loss : 0.0163\n",
      "Epoch : 8 | Training Loss : 0.0134\n",
      "Epoch : 9 | Training Loss : 0.0242\n",
      "Epoch : 9 | Training Loss : 0.0097\n",
      "Epoch : 9 | Training Loss : 0.0121\n",
      "Epoch : 9 | Training Loss : 0.0091\n",
      "Epoch : 9 | Training Loss : 0.0110\n",
      "Epoch : 9 | Training Loss : 0.0087\n",
      "Epoch : 9 | Training Loss : 0.0134\n",
      "Epoch : 9 | Training Loss : 0.0085\n",
      "Epoch : 9 | Training Loss : 0.0304\n",
      "Epoch : 9 | Training Loss : 0.0147\n",
      "Epoch : 9 | Training Loss : 0.0084\n",
      "Epoch : 9 | Training Loss : 0.0452\n",
      "Epoch : 9 | Training Loss : 0.0167\n",
      "Epoch : 9 | Training Loss : 0.0085\n",
      "Epoch : 9 | Training Loss : 0.0075\n",
      "Epoch : 9 | Training Loss : 0.0244\n",
      "Epoch : 9 | Training Loss : 0.0123\n",
      "Epoch : 9 | Training Loss : 0.0281\n",
      "Epoch : 9 | Training Loss : 0.0421\n",
      "Epoch : 9 | Training Loss : 0.0226\n",
      "Epoch : 9 | Training Loss : 0.0200\n",
      "Epoch : 9 | Training Loss : 0.0145\n",
      "Epoch : 9 | Training Loss : 0.0163\n",
      "Epoch : 9 | Training Loss : 0.0330\n",
      "Epoch : 9 | Training Loss : 0.0201\n",
      "Epoch : 9 | Training Loss : 0.0055\n",
      "Epoch : 9 | Training Loss : 0.0081\n",
      "Epoch : 9 | Training Loss : 0.0314\n",
      "Epoch : 9 | Training Loss : 0.0080\n",
      "Epoch : 9 | Training Loss : 0.0161\n",
      "Epoch : 9 | Training Loss : 0.0219\n",
      "Epoch : 9 | Training Loss : 0.0055\n",
      "Epoch : 9 | Training Loss : 0.0084\n",
      "Epoch : 9 | Training Loss : 0.0191\n",
      "Epoch : 9 | Training Loss : 0.0112\n",
      "Epoch : 9 | Training Loss : 0.0153\n",
      "Epoch : 9 | Training Loss : 0.0095\n",
      "Epoch : 9 | Training Loss : 0.0362\n",
      "Epoch : 9 | Training Loss : 0.0072\n",
      "Epoch : 9 | Training Loss : 0.0052\n",
      "Epoch : 9 | Training Loss : 0.0046\n",
      "Epoch : 9 | Training Loss : 0.0154\n",
      "Epoch : 9 | Training Loss : 0.0177\n",
      "Epoch : 9 | Training Loss : 0.0155\n",
      "Epoch : 9 | Training Loss : 0.0165\n",
      "Epoch : 9 | Training Loss : 0.0081\n",
      "Epoch : 9 | Training Loss : 0.0131\n",
      "Epoch : 9 | Training Loss : 0.0115\n",
      "Epoch : 9 | Training Loss : 0.0156\n",
      "Epoch : 9 | Training Loss : 0.0102\n",
      "Epoch : 9 | Training Loss : 0.0118\n",
      "Epoch : 9 | Training Loss : 0.0044\n",
      "Epoch : 9 | Training Loss : 0.0129\n",
      "Epoch : 9 | Training Loss : 0.0103\n",
      "Epoch : 9 | Training Loss : 0.0129\n",
      "Epoch : 9 | Training Loss : 0.0175\n",
      "Epoch : 9 | Training Loss : 0.0198\n",
      "Epoch : 9 | Training Loss : 0.0052\n",
      "Epoch : 9 | Training Loss : 0.0027\n",
      "Epoch : 9 | Training Loss : 0.0059\n",
      "Epoch : 9 | Training Loss : 0.0029\n",
      "Epoch : 9 | Training Loss : 0.0202\n",
      "Epoch : 9 | Training Loss : 0.0226\n",
      "Epoch : 9 | Training Loss : 0.0078\n",
      "Epoch : 9 | Training Loss : 0.0161\n",
      "Epoch : 9 | Training Loss : 0.0111\n",
      "Epoch : 9 | Training Loss : 0.0078\n",
      "Epoch : 9 | Training Loss : 0.0075\n",
      "Epoch : 9 | Training Loss : 0.0124\n",
      "Epoch : 9 | Training Loss : 0.0092\n",
      "Epoch : 9 | Training Loss : 0.0061\n",
      "Epoch : 9 | Training Loss : 0.0050\n",
      "Epoch : 9 | Training Loss : 0.0105\n",
      "Epoch : 9 | Training Loss : 0.0195\n",
      "Epoch : 9 | Training Loss : 0.0122\n",
      "Epoch : 9 | Training Loss : 0.0098\n",
      "Epoch : 9 | Training Loss : 0.0176\n",
      "Epoch : 9 | Training Loss : 0.0084\n",
      "Epoch : 9 | Training Loss : 0.0104\n",
      "Epoch : 9 | Training Loss : 0.0112\n",
      "Epoch : 9 | Training Loss : 0.0106\n",
      "Epoch : 9 | Training Loss : 0.0062\n",
      "Epoch : 9 | Training Loss : 0.0081\n",
      "Epoch : 9 | Training Loss : 0.0150\n",
      "Epoch : 9 | Training Loss : 0.0026\n",
      "Epoch : 9 | Training Loss : 0.0093\n",
      "Epoch : 9 | Training Loss : 0.0273\n",
      "Epoch : 9 | Training Loss : 0.0070\n",
      "Epoch : 9 | Training Loss : 0.0039\n",
      "Epoch : 9 | Training Loss : 0.0057\n",
      "Epoch : 9 | Training Loss : 0.0032\n",
      "Epoch : 9 | Training Loss : 0.0089\n",
      "Epoch : 9 | Training Loss : 0.0144\n",
      "Epoch : 9 | Training Loss : 0.0024\n",
      "Epoch : 9 | Training Loss : 0.0055\n",
      "Epoch : 9 | Training Loss : 0.0088\n",
      "Epoch : 9 | Training Loss : 0.0211\n",
      "Epoch : 9 | Training Loss : 0.0293\n",
      "Epoch : 9 | Training Loss : 0.0065\n",
      "Epoch : 9 | Training Loss : 0.0043\n",
      "Epoch : 9 | Training Loss : 0.0120\n",
      "Epoch : 9 | Training Loss : 0.0113\n",
      "Epoch : 9 | Training Loss : 0.0031\n",
      "Epoch : 9 | Training Loss : 0.0068\n",
      "Epoch : 9 | Training Loss : 0.0102\n",
      "Epoch : 9 | Training Loss : 0.0040\n",
      "Epoch : 9 | Training Loss : 0.0122\n",
      "Epoch : 9 | Training Loss : 0.0036\n",
      "Epoch : 9 | Training Loss : 0.0193\n",
      "Epoch : 9 | Training Loss : 0.0136\n",
      "Epoch : 9 | Training Loss : 0.0206\n",
      "Epoch : 9 | Training Loss : 0.0055\n",
      "Epoch : 9 | Training Loss : 0.0064\n",
      "Epoch : 9 | Training Loss : 0.0091\n",
      "Epoch : 9 | Training Loss : 0.0099\n",
      "Epoch : 9 | Training Loss : 0.0146\n",
      "Epoch : 9 | Training Loss : 0.0013\n",
      "Epoch : 9 | Training Loss : 0.0106\n",
      "Epoch : 9 | Training Loss : 0.0223\n",
      "Epoch : 9 | Training Loss : 0.0127\n",
      "Epoch : 9 | Training Loss : 0.0100\n",
      "Epoch : 9 | Training Loss : 0.0082\n",
      "Epoch : 9 | Training Loss : 0.0054\n",
      "Epoch : 9 | Training Loss : 0.0095\n",
      "Epoch : 9 | Training Loss : 0.0096\n",
      "Epoch : 9 | Training Loss : 0.0172\n",
      "Epoch : 9 | Training Loss : 0.0046\n",
      "Epoch : 9 | Training Loss : 0.0082\n",
      "Epoch : 9 | Training Loss : 0.0088\n",
      "Epoch : 9 | Training Loss : 0.0323\n",
      "Epoch : 9 | Training Loss : 0.0255\n",
      "Epoch : 9 | Training Loss : 0.0224\n",
      "Epoch : 9 | Training Loss : 0.0121\n",
      "Epoch : 9 | Training Loss : 0.0427\n",
      "Epoch : 9 | Training Loss : 0.0181\n",
      "Epoch : 9 | Training Loss : 0.0189\n",
      "Epoch : 9 | Training Loss : 0.0100\n",
      "Epoch : 9 | Training Loss : 0.0429\n",
      "Epoch : 9 | Training Loss : 0.0575\n",
      "Epoch : 9 | Training Loss : 0.0066\n",
      "Epoch : 9 | Training Loss : 0.0374\n",
      "Epoch : 9 | Training Loss : 0.0495\n",
      "Epoch : 9 | Training Loss : 0.0233\n",
      "Epoch : 9 | Training Loss : 0.0056\n",
      "Epoch : 9 | Training Loss : 0.0104\n",
      "Epoch : 9 | Training Loss : 0.0545\n",
      "Epoch : 9 | Training Loss : 0.0205\n",
      "Epoch : 9 | Training Loss : 0.0258\n",
      "Epoch : 9 | Training Loss : 0.0139\n",
      "Epoch : 9 | Training Loss : 0.0182\n",
      "Epoch : 9 | Training Loss : 0.0222\n",
      "Epoch : 9 | Training Loss : 0.0386\n",
      "Epoch : 9 | Training Loss : 0.0082\n",
      "Epoch : 9 | Training Loss : 0.0260\n",
      "Epoch : 9 | Training Loss : 0.0269\n",
      "Epoch : 9 | Training Loss : 0.0167\n",
      "Epoch : 9 | Training Loss : 0.0103\n",
      "Epoch : 9 | Training Loss : 0.0150\n",
      "Epoch : 9 | Training Loss : 0.0364\n",
      "Epoch : 9 | Training Loss : 0.0233\n",
      "Epoch : 9 | Training Loss : 0.0185\n",
      "Epoch : 9 | Training Loss : 0.0280\n",
      "Epoch : 9 | Training Loss : 0.0211\n",
      "Epoch : 9 | Training Loss : 0.0409\n",
      "Epoch : 9 | Training Loss : 0.0344\n",
      "Epoch : 9 | Training Loss : 0.0095\n",
      "Epoch : 9 | Training Loss : 0.0335\n",
      "Epoch : 9 | Training Loss : 0.0110\n",
      "Epoch : 9 | Training Loss : 0.0163\n",
      "Epoch : 9 | Training Loss : 0.0178\n",
      "Epoch : 9 | Training Loss : 0.0193\n",
      "Epoch : 9 | Training Loss : 0.0363\n",
      "Epoch : 9 | Training Loss : 0.0080\n",
      "Epoch : 9 | Training Loss : 0.0273\n",
      "Epoch : 9 | Training Loss : 0.0327\n",
      "Epoch : 9 | Training Loss : 0.0363\n",
      "Epoch : 9 | Training Loss : 0.0121\n",
      "Epoch : 9 | Training Loss : 0.0123\n",
      "Epoch : 9 | Training Loss : 0.0249\n",
      "Epoch : 9 | Training Loss : 0.0140\n",
      "Epoch : 9 | Training Loss : 0.0301\n",
      "Epoch : 9 | Training Loss : 0.0242\n",
      "Epoch : 9 | Training Loss : 0.0313\n",
      "Epoch : 9 | Training Loss : 0.0095\n",
      "Epoch : 9 | Training Loss : 0.0060\n",
      "Epoch : 9 | Training Loss : 0.0341\n",
      "Epoch : 9 | Training Loss : 0.0364\n",
      "Epoch : 9 | Training Loss : 0.0330\n",
      "Epoch : 9 | Training Loss : 0.0082\n",
      "Epoch : 9 | Training Loss : 0.0085\n",
      "Epoch : 9 | Training Loss : 0.0090\n",
      "Epoch : 9 | Training Loss : 0.0108\n",
      "Epoch : 9 | Training Loss : 0.0171\n",
      "Epoch : 9 | Training Loss : 0.0224\n",
      "Epoch : 9 | Training Loss : 0.0127\n",
      "Epoch : 9 | Training Loss : 0.0219\n",
      "Epoch : 9 | Training Loss : 0.0221\n",
      "Epoch : 9 | Training Loss : 0.0337\n",
      "Epoch : 9 | Training Loss : 0.0684\n",
      "Epoch : 9 | Training Loss : 0.0197\n",
      "Epoch : 9 | Training Loss : 0.0079\n",
      "Epoch : 9 | Training Loss : 0.0202\n",
      "Epoch : 9 | Training Loss : 0.0143\n",
      "Epoch : 9 | Training Loss : 0.0229\n",
      "Epoch : 9 | Training Loss : 0.0317\n",
      "Epoch : 9 | Training Loss : 0.0340\n",
      "Epoch : 9 | Training Loss : 0.0302\n",
      "Epoch : 9 | Training Loss : 0.0176\n",
      "Epoch : 9 | Training Loss : 0.0063\n",
      "Epoch : 9 | Training Loss : 0.0204\n",
      "Epoch : 9 | Training Loss : 0.0172\n",
      "Epoch : 9 | Training Loss : 0.0132\n",
      "Epoch : 9 | Training Loss : 0.0314\n",
      "Epoch : 9 | Training Loss : 0.0137\n",
      "Epoch : 9 | Training Loss : 0.0150\n",
      "Epoch : 9 | Training Loss : 0.0134\n",
      "Epoch : 9 | Training Loss : 0.0249\n",
      "Epoch : 9 | Training Loss : 0.0174\n",
      "Epoch : 9 | Training Loss : 0.0192\n",
      "Epoch : 9 | Training Loss : 0.0331\n",
      "Epoch : 9 | Training Loss : 0.0234\n",
      "Epoch : 9 | Training Loss : 0.0347\n",
      "Epoch : 9 | Training Loss : 0.0168\n",
      "Epoch : 9 | Training Loss : 0.0129\n",
      "Epoch : 9 | Training Loss : 0.0386\n",
      "Epoch : 9 | Training Loss : 0.0119\n",
      "Epoch : 9 | Training Loss : 0.0134\n",
      "Epoch : 9 | Training Loss : 0.0118\n",
      "Epoch : 9 | Training Loss : 0.0156\n",
      "Epoch : 9 | Training Loss : 0.0173\n",
      "Epoch : 9 | Training Loss : 0.0260\n",
      "Epoch : 9 | Training Loss : 0.0123\n",
      "Epoch : 9 | Training Loss : 0.0101\n",
      "Epoch : 9 | Training Loss : 0.0112\n",
      "Epoch : 9 | Training Loss : 0.0155\n",
      "Epoch : 9 | Training Loss : 0.0503\n",
      "Epoch : 9 | Training Loss : 0.0166\n",
      "Epoch : 9 | Training Loss : 0.0093\n",
      "Epoch : 9 | Training Loss : 0.0211\n",
      "Epoch : 9 | Training Loss : 0.0464\n",
      "Epoch : 9 | Training Loss : 0.0386\n",
      "Epoch : 9 | Training Loss : 0.0210\n",
      "Epoch : 9 | Training Loss : 0.0328\n",
      "Epoch : 10 | Training Loss : 0.0128\n",
      "Epoch : 10 | Training Loss : 0.0116\n",
      "Epoch : 10 | Training Loss : 0.0172\n",
      "Epoch : 10 | Training Loss : 0.0071\n",
      "Epoch : 10 | Training Loss : 0.0200\n",
      "Epoch : 10 | Training Loss : 0.0243\n",
      "Epoch : 10 | Training Loss : 0.0157\n",
      "Epoch : 10 | Training Loss : 0.0074\n",
      "Epoch : 10 | Training Loss : 0.0055\n",
      "Epoch : 10 | Training Loss : 0.0036\n",
      "Epoch : 10 | Training Loss : 0.0101\n",
      "Epoch : 10 | Training Loss : 0.0083\n",
      "Epoch : 10 | Training Loss : 0.0097\n",
      "Epoch : 10 | Training Loss : 0.0078\n",
      "Epoch : 10 | Training Loss : 0.0216\n",
      "Epoch : 10 | Training Loss : 0.0057\n",
      "Epoch : 10 | Training Loss : 0.0242\n",
      "Epoch : 10 | Training Loss : 0.0235\n",
      "Epoch : 10 | Training Loss : 0.0080\n",
      "Epoch : 10 | Training Loss : 0.0099\n",
      "Epoch : 10 | Training Loss : 0.0111\n",
      "Epoch : 10 | Training Loss : 0.0084\n",
      "Epoch : 10 | Training Loss : 0.0064\n",
      "Epoch : 10 | Training Loss : 0.0087\n",
      "Epoch : 10 | Training Loss : 0.0177\n",
      "Epoch : 10 | Training Loss : 0.0094\n",
      "Epoch : 10 | Training Loss : 0.0099\n",
      "Epoch : 10 | Training Loss : 0.0147\n",
      "Epoch : 10 | Training Loss : 0.0091\n",
      "Epoch : 10 | Training Loss : 0.0117\n",
      "Epoch : 10 | Training Loss : 0.0075\n",
      "Epoch : 10 | Training Loss : 0.0052\n",
      "Epoch : 10 | Training Loss : 0.0061\n",
      "Epoch : 10 | Training Loss : 0.0050\n",
      "Epoch : 10 | Training Loss : 0.0069\n",
      "Epoch : 10 | Training Loss : 0.0106\n",
      "Epoch : 10 | Training Loss : 0.0351\n",
      "Epoch : 10 | Training Loss : 0.0106\n",
      "Epoch : 10 | Training Loss : 0.0074\n",
      "Epoch : 10 | Training Loss : 0.0148\n",
      "Epoch : 10 | Training Loss : 0.0155\n",
      "Epoch : 10 | Training Loss : 0.0091\n",
      "Epoch : 10 | Training Loss : 0.0233\n",
      "Epoch : 10 | Training Loss : 0.0156\n",
      "Epoch : 10 | Training Loss : 0.0156\n",
      "Epoch : 10 | Training Loss : 0.0079\n",
      "Epoch : 10 | Training Loss : 0.0029\n",
      "Epoch : 10 | Training Loss : 0.0179\n",
      "Epoch : 10 | Training Loss : 0.0071\n",
      "Epoch : 10 | Training Loss : 0.0103\n",
      "Epoch : 10 | Training Loss : 0.0365\n",
      "Epoch : 10 | Training Loss : 0.0181\n",
      "Epoch : 10 | Training Loss : 0.0080\n",
      "Epoch : 10 | Training Loss : 0.0032\n",
      "Epoch : 10 | Training Loss : 0.0153\n",
      "Epoch : 10 | Training Loss : 0.0078\n",
      "Epoch : 10 | Training Loss : 0.0113\n",
      "Epoch : 10 | Training Loss : 0.0106\n",
      "Epoch : 10 | Training Loss : 0.0107\n",
      "Epoch : 10 | Training Loss : 0.0173\n",
      "Epoch : 10 | Training Loss : 0.0119\n",
      "Epoch : 10 | Training Loss : 0.0135\n",
      "Epoch : 10 | Training Loss : 0.0055\n",
      "Epoch : 10 | Training Loss : 0.0157\n",
      "Epoch : 10 | Training Loss : 0.0041\n",
      "Epoch : 10 | Training Loss : 0.0102\n",
      "Epoch : 10 | Training Loss : 0.0108\n",
      "Epoch : 10 | Training Loss : 0.0110\n",
      "Epoch : 10 | Training Loss : 0.0091\n",
      "Epoch : 10 | Training Loss : 0.0188\n",
      "Epoch : 10 | Training Loss : 0.0049\n",
      "Epoch : 10 | Training Loss : 0.0142\n",
      "Epoch : 10 | Training Loss : 0.0144\n",
      "Epoch : 10 | Training Loss : 0.0091\n",
      "Epoch : 10 | Training Loss : 0.0205\n",
      "Epoch : 10 | Training Loss : 0.0178\n",
      "Epoch : 10 | Training Loss : 0.0028\n",
      "Epoch : 10 | Training Loss : 0.0144\n",
      "Epoch : 10 | Training Loss : 0.0075\n",
      "Epoch : 10 | Training Loss : 0.0068\n",
      "Epoch : 10 | Training Loss : 0.0060\n",
      "Epoch : 10 | Training Loss : 0.0137\n",
      "Epoch : 10 | Training Loss : 0.0204\n",
      "Epoch : 10 | Training Loss : 0.0071\n",
      "Epoch : 10 | Training Loss : 0.0044\n",
      "Epoch : 10 | Training Loss : 0.0033\n",
      "Epoch : 10 | Training Loss : 0.0124\n",
      "Epoch : 10 | Training Loss : 0.0109\n",
      "Epoch : 10 | Training Loss : 0.0060\n",
      "Epoch : 10 | Training Loss : 0.0053\n",
      "Epoch : 10 | Training Loss : 0.0041\n",
      "Epoch : 10 | Training Loss : 0.0130\n",
      "Epoch : 10 | Training Loss : 0.0112\n",
      "Epoch : 10 | Training Loss : 0.0056\n",
      "Epoch : 10 | Training Loss : 0.0033\n",
      "Epoch : 10 | Training Loss : 0.0046\n",
      "Epoch : 10 | Training Loss : 0.0038\n",
      "Epoch : 10 | Training Loss : 0.0045\n",
      "Epoch : 10 | Training Loss : 0.0206\n",
      "Epoch : 10 | Training Loss : 0.0045\n",
      "Epoch : 10 | Training Loss : 0.0066\n",
      "Epoch : 10 | Training Loss : 0.0072\n",
      "Epoch : 10 | Training Loss : 0.0048\n",
      "Epoch : 10 | Training Loss : 0.0127\n",
      "Epoch : 10 | Training Loss : 0.0633\n",
      "Epoch : 10 | Training Loss : 0.0044\n",
      "Epoch : 10 | Training Loss : 0.0092\n",
      "Epoch : 10 | Training Loss : 0.0100\n",
      "Epoch : 10 | Training Loss : 0.0243\n",
      "Epoch : 10 | Training Loss : 0.0025\n",
      "Epoch : 10 | Training Loss : 0.0038\n",
      "Epoch : 10 | Training Loss : 0.0112\n",
      "Epoch : 10 | Training Loss : 0.0031\n",
      "Epoch : 10 | Training Loss : 0.0171\n",
      "Epoch : 10 | Training Loss : 0.0054\n",
      "Epoch : 10 | Training Loss : 0.0066\n",
      "Epoch : 10 | Training Loss : 0.0029\n",
      "Epoch : 10 | Training Loss : 0.0159\n",
      "Epoch : 10 | Training Loss : 0.0109\n",
      "Epoch : 10 | Training Loss : 0.0120\n",
      "Epoch : 10 | Training Loss : 0.0148\n",
      "Epoch : 10 | Training Loss : 0.0097\n",
      "Epoch : 10 | Training Loss : 0.0068\n",
      "Epoch : 10 | Training Loss : 0.0054\n",
      "Epoch : 10 | Training Loss : 0.0076\n",
      "Epoch : 10 | Training Loss : 0.0051\n",
      "Epoch : 10 | Training Loss : 0.0024\n",
      "Epoch : 10 | Training Loss : 0.0049\n",
      "Epoch : 10 | Training Loss : 0.0050\n",
      "Epoch : 10 | Training Loss : 0.0033\n",
      "Epoch : 10 | Training Loss : 0.0167\n",
      "Epoch : 10 | Training Loss : 0.0216\n",
      "Epoch : 10 | Training Loss : 0.0052\n",
      "Epoch : 10 | Training Loss : 0.0118\n",
      "Epoch : 10 | Training Loss : 0.0061\n",
      "Epoch : 10 | Training Loss : 0.0043\n",
      "Epoch : 10 | Training Loss : 0.0057\n",
      "Epoch : 10 | Training Loss : 0.0051\n",
      "Epoch : 10 | Training Loss : 0.0053\n",
      "Epoch : 10 | Training Loss : 0.0060\n",
      "Epoch : 10 | Training Loss : 0.0048\n",
      "Epoch : 10 | Training Loss : 0.0126\n",
      "Epoch : 10 | Training Loss : 0.0416\n",
      "Epoch : 10 | Training Loss : 0.0122\n",
      "Epoch : 10 | Training Loss : 0.0163\n",
      "Epoch : 10 | Training Loss : 0.0140\n",
      "Epoch : 10 | Training Loss : 0.0041\n",
      "Epoch : 10 | Training Loss : 0.0057\n",
      "Epoch : 10 | Training Loss : 0.0067\n",
      "Epoch : 10 | Training Loss : 0.0018\n",
      "Epoch : 10 | Training Loss : 0.0177\n",
      "Epoch : 10 | Training Loss : 0.0164\n",
      "Epoch : 10 | Training Loss : 0.0093\n",
      "Epoch : 10 | Training Loss : 0.0335\n",
      "Epoch : 10 | Training Loss : 0.0156\n",
      "Epoch : 10 | Training Loss : 0.0036\n",
      "Epoch : 10 | Training Loss : 0.0284\n",
      "Epoch : 10 | Training Loss : 0.0101\n",
      "Epoch : 10 | Training Loss : 0.0383\n",
      "Epoch : 10 | Training Loss : 0.0327\n",
      "Epoch : 10 | Training Loss : 0.0268\n",
      "Epoch : 10 | Training Loss : 0.0270\n",
      "Epoch : 10 | Training Loss : 0.0082\n",
      "Epoch : 10 | Training Loss : 0.0077\n",
      "Epoch : 10 | Training Loss : 0.0148\n",
      "Epoch : 10 | Training Loss : 0.0058\n",
      "Epoch : 10 | Training Loss : 0.0171\n",
      "Epoch : 10 | Training Loss : 0.0853\n",
      "Epoch : 10 | Training Loss : 0.0183\n",
      "Epoch : 10 | Training Loss : 0.0116\n",
      "Epoch : 10 | Training Loss : 0.0212\n",
      "Epoch : 10 | Training Loss : 0.0092\n",
      "Epoch : 10 | Training Loss : 0.0144\n",
      "Epoch : 10 | Training Loss : 0.0221\n",
      "Epoch : 10 | Training Loss : 0.0252\n",
      "Epoch : 10 | Training Loss : 0.0449\n",
      "Epoch : 10 | Training Loss : 0.0535\n",
      "Epoch : 10 | Training Loss : 0.0184\n",
      "Epoch : 10 | Training Loss : 0.0202\n",
      "Epoch : 10 | Training Loss : 0.0241\n",
      "Epoch : 10 | Training Loss : 0.0070\n",
      "Epoch : 10 | Training Loss : 0.0689\n",
      "Epoch : 10 | Training Loss : 0.0193\n",
      "Epoch : 10 | Training Loss : 0.0257\n",
      "Epoch : 10 | Training Loss : 0.0168\n",
      "Epoch : 10 | Training Loss : 0.0147\n",
      "Epoch : 10 | Training Loss : 0.0213\n",
      "Epoch : 10 | Training Loss : 0.0162\n",
      "Epoch : 10 | Training Loss : 0.0081\n",
      "Epoch : 10 | Training Loss : 0.0090\n",
      "Epoch : 10 | Training Loss : 0.0111\n",
      "Epoch : 10 | Training Loss : 0.0108\n",
      "Epoch : 10 | Training Loss : 0.0169\n",
      "Epoch : 10 | Training Loss : 0.0144\n",
      "Epoch : 10 | Training Loss : 0.0084\n",
      "Epoch : 10 | Training Loss : 0.0096\n",
      "Epoch : 10 | Training Loss : 0.0465\n",
      "Epoch : 10 | Training Loss : 0.0141\n",
      "Epoch : 10 | Training Loss : 0.0234\n",
      "Epoch : 10 | Training Loss : 0.0360\n",
      "Epoch : 10 | Training Loss : 0.0046\n",
      "Epoch : 10 | Training Loss : 0.0277\n",
      "Epoch : 10 | Training Loss : 0.0183\n",
      "Epoch : 10 | Training Loss : 0.0135\n",
      "Epoch : 10 | Training Loss : 0.0079\n",
      "Epoch : 10 | Training Loss : 0.0042\n",
      "Epoch : 10 | Training Loss : 0.0161\n",
      "Epoch : 10 | Training Loss : 0.0233\n",
      "Epoch : 10 | Training Loss : 0.0179\n",
      "Epoch : 10 | Training Loss : 0.0149\n",
      "Epoch : 10 | Training Loss : 0.0142\n",
      "Epoch : 10 | Training Loss : 0.0233\n",
      "Epoch : 10 | Training Loss : 0.0283\n",
      "Epoch : 10 | Training Loss : 0.0490\n",
      "Epoch : 10 | Training Loss : 0.0043\n",
      "Epoch : 10 | Training Loss : 0.0048\n",
      "Epoch : 10 | Training Loss : 0.0330\n",
      "Epoch : 10 | Training Loss : 0.0046\n",
      "Epoch : 10 | Training Loss : 0.0436\n",
      "Epoch : 10 | Training Loss : 0.0134\n",
      "Epoch : 10 | Training Loss : 0.0600\n",
      "Epoch : 10 | Training Loss : 0.0406\n",
      "Epoch : 10 | Training Loss : 0.0440\n",
      "Epoch : 10 | Training Loss : 0.0285\n",
      "Epoch : 10 | Training Loss : 0.0042\n",
      "Epoch : 10 | Training Loss : 0.0847\n",
      "Epoch : 10 | Training Loss : 0.0154\n",
      "Epoch : 10 | Training Loss : 0.0277\n",
      "Epoch : 10 | Training Loss : 0.0369\n",
      "Epoch : 10 | Training Loss : 0.0367\n",
      "Epoch : 10 | Training Loss : 0.0505\n",
      "Epoch : 10 | Training Loss : 0.0229\n",
      "Epoch : 10 | Training Loss : 0.0311\n",
      "Epoch : 10 | Training Loss : 0.0132\n",
      "Epoch : 10 | Training Loss : 0.0092\n",
      "Epoch : 10 | Training Loss : 0.0493\n",
      "Epoch : 10 | Training Loss : 0.0084\n",
      "Epoch : 10 | Training Loss : 0.0350\n",
      "Epoch : 10 | Training Loss : 0.0253\n",
      "Epoch : 10 | Training Loss : 0.0430\n",
      "Epoch : 10 | Training Loss : 0.0214\n",
      "Epoch : 10 | Training Loss : 0.0385\n",
      "Epoch : 10 | Training Loss : 0.0031\n",
      "Epoch : 11 | Training Loss : 0.0204\n",
      "Epoch : 11 | Training Loss : 0.0125\n",
      "Epoch : 11 | Training Loss : 0.0534\n",
      "Epoch : 11 | Training Loss : 0.0185\n",
      "Epoch : 11 | Training Loss : 0.0407\n",
      "Epoch : 11 | Training Loss : 0.0168\n",
      "Epoch : 11 | Training Loss : 0.0097\n",
      "Epoch : 11 | Training Loss : 0.0217\n",
      "Epoch : 11 | Training Loss : 0.0433\n",
      "Epoch : 11 | Training Loss : 0.0298\n",
      "Epoch : 11 | Training Loss : 0.0242\n",
      "Epoch : 11 | Training Loss : 0.0057\n",
      "Epoch : 11 | Training Loss : 0.0366\n",
      "Epoch : 11 | Training Loss : 0.0314\n",
      "Epoch : 11 | Training Loss : 0.0100\n",
      "Epoch : 11 | Training Loss : 0.0042\n",
      "Epoch : 11 | Training Loss : 0.0170\n",
      "Epoch : 11 | Training Loss : 0.0224\n",
      "Epoch : 11 | Training Loss : 0.0199\n",
      "Epoch : 11 | Training Loss : 0.0167\n",
      "Epoch : 11 | Training Loss : 0.0124\n",
      "Epoch : 11 | Training Loss : 0.0072\n",
      "Epoch : 11 | Training Loss : 0.0060\n",
      "Epoch : 11 | Training Loss : 0.0121\n",
      "Epoch : 11 | Training Loss : 0.0290\n",
      "Epoch : 11 | Training Loss : 0.0054\n",
      "Epoch : 11 | Training Loss : 0.0499\n",
      "Epoch : 11 | Training Loss : 0.0107\n",
      "Epoch : 11 | Training Loss : 0.0063\n",
      "Epoch : 11 | Training Loss : 0.0068\n",
      "Epoch : 11 | Training Loss : 0.0180\n",
      "Epoch : 11 | Training Loss : 0.0069\n",
      "Epoch : 11 | Training Loss : 0.0192\n",
      "Epoch : 11 | Training Loss : 0.0098\n",
      "Epoch : 11 | Training Loss : 0.0157\n",
      "Epoch : 11 | Training Loss : 0.0106\n",
      "Epoch : 11 | Training Loss : 0.0127\n",
      "Epoch : 11 | Training Loss : 0.0167\n",
      "Epoch : 11 | Training Loss : 0.0094\n",
      "Epoch : 11 | Training Loss : 0.0045\n",
      "Epoch : 11 | Training Loss : 0.0148\n",
      "Epoch : 11 | Training Loss : 0.0046\n",
      "Epoch : 11 | Training Loss : 0.0056\n",
      "Epoch : 11 | Training Loss : 0.0148\n",
      "Epoch : 11 | Training Loss : 0.0049\n",
      "Epoch : 11 | Training Loss : 0.0054\n",
      "Epoch : 11 | Training Loss : 0.0082\n",
      "Epoch : 11 | Training Loss : 0.0072\n",
      "Epoch : 11 | Training Loss : 0.0158\n",
      "Epoch : 11 | Training Loss : 0.0148\n",
      "Epoch : 11 | Training Loss : 0.0107\n",
      "Epoch : 11 | Training Loss : 0.0102\n",
      "Epoch : 11 | Training Loss : 0.0120\n",
      "Epoch : 11 | Training Loss : 0.0066\n",
      "Epoch : 11 | Training Loss : 0.0130\n",
      "Epoch : 11 | Training Loss : 0.0093\n",
      "Epoch : 11 | Training Loss : 0.0061\n",
      "Epoch : 11 | Training Loss : 0.0077\n",
      "Epoch : 11 | Training Loss : 0.0070\n",
      "Epoch : 11 | Training Loss : 0.0768\n",
      "Epoch : 11 | Training Loss : 0.0117\n",
      "Epoch : 11 | Training Loss : 0.0075\n",
      "Epoch : 11 | Training Loss : 0.0043\n",
      "Epoch : 11 | Training Loss : 0.0047\n",
      "Epoch : 11 | Training Loss : 0.0034\n",
      "Epoch : 11 | Training Loss : 0.0153\n",
      "Epoch : 11 | Training Loss : 0.0074\n",
      "Epoch : 11 | Training Loss : 0.0038\n",
      "Epoch : 11 | Training Loss : 0.0067\n",
      "Epoch : 11 | Training Loss : 0.0110\n",
      "Epoch : 11 | Training Loss : 0.0039\n",
      "Epoch : 11 | Training Loss : 0.0095\n",
      "Epoch : 11 | Training Loss : 0.0069\n",
      "Epoch : 11 | Training Loss : 0.0117\n",
      "Epoch : 11 | Training Loss : 0.0147\n",
      "Epoch : 11 | Training Loss : 0.0048\n",
      "Epoch : 11 | Training Loss : 0.0059\n",
      "Epoch : 11 | Training Loss : 0.0157\n",
      "Epoch : 11 | Training Loss : 0.0208\n",
      "Epoch : 11 | Training Loss : 0.0109\n",
      "Epoch : 11 | Training Loss : 0.0047\n",
      "Epoch : 11 | Training Loss : 0.0083\n",
      "Epoch : 11 | Training Loss : 0.0184\n",
      "Epoch : 11 | Training Loss : 0.0116\n",
      "Epoch : 11 | Training Loss : 0.0100\n",
      "Epoch : 11 | Training Loss : 0.0050\n",
      "Epoch : 11 | Training Loss : 0.0121\n",
      "Epoch : 11 | Training Loss : 0.0056\n",
      "Epoch : 11 | Training Loss : 0.0153\n",
      "Epoch : 11 | Training Loss : 0.0422\n",
      "Epoch : 11 | Training Loss : 0.0081\n",
      "Epoch : 11 | Training Loss : 0.0121\n",
      "Epoch : 11 | Training Loss : 0.0118\n",
      "Epoch : 11 | Training Loss : 0.0097\n",
      "Epoch : 11 | Training Loss : 0.0139\n",
      "Epoch : 11 | Training Loss : 0.0063\n",
      "Epoch : 11 | Training Loss : 0.0203\n",
      "Epoch : 11 | Training Loss : 0.0132\n",
      "Epoch : 11 | Training Loss : 0.0060\n",
      "Epoch : 11 | Training Loss : 0.0277\n",
      "Epoch : 11 | Training Loss : 0.0064\n",
      "Epoch : 11 | Training Loss : 0.0103\n",
      "Epoch : 11 | Training Loss : 0.0051\n",
      "Epoch : 11 | Training Loss : 0.0077\n",
      "Epoch : 11 | Training Loss : 0.0092\n",
      "Epoch : 11 | Training Loss : 0.0104\n",
      "Epoch : 11 | Training Loss : 0.0111\n",
      "Epoch : 11 | Training Loss : 0.0060\n",
      "Epoch : 11 | Training Loss : 0.0399\n",
      "Epoch : 11 | Training Loss : 0.0153\n",
      "Epoch : 11 | Training Loss : 0.0212\n",
      "Epoch : 11 | Training Loss : 0.0168\n",
      "Epoch : 11 | Training Loss : 0.0239\n",
      "Epoch : 11 | Training Loss : 0.0042\n",
      "Epoch : 11 | Training Loss : 0.0065\n",
      "Epoch : 11 | Training Loss : 0.0249\n",
      "Epoch : 11 | Training Loss : 0.0091\n",
      "Epoch : 11 | Training Loss : 0.0242\n",
      "Epoch : 11 | Training Loss : 0.0150\n",
      "Epoch : 11 | Training Loss : 0.0101\n",
      "Epoch : 11 | Training Loss : 0.0060\n",
      "Epoch : 11 | Training Loss : 0.0121\n",
      "Epoch : 11 | Training Loss : 0.0292\n",
      "Epoch : 11 | Training Loss : 0.0021\n",
      "Epoch : 11 | Training Loss : 0.0066\n",
      "Epoch : 11 | Training Loss : 0.0084\n",
      "Epoch : 11 | Training Loss : 0.0244\n",
      "Epoch : 11 | Training Loss : 0.0924\n",
      "Epoch : 11 | Training Loss : 0.0063\n",
      "Epoch : 11 | Training Loss : 0.0442\n",
      "Epoch : 11 | Training Loss : 0.0584\n",
      "Epoch : 11 | Training Loss : 0.0098\n",
      "Epoch : 11 | Training Loss : 0.0111\n",
      "Epoch : 11 | Training Loss : 0.0113\n",
      "Epoch : 11 | Training Loss : 0.0088\n",
      "Epoch : 11 | Training Loss : 0.0254\n",
      "Epoch : 11 | Training Loss : 0.0365\n",
      "Epoch : 11 | Training Loss : 0.0654\n",
      "Epoch : 11 | Training Loss : 0.0374\n",
      "Epoch : 11 | Training Loss : 0.0204\n",
      "Epoch : 11 | Training Loss : 0.0158\n",
      "Epoch : 11 | Training Loss : 0.0319\n",
      "Epoch : 11 | Training Loss : 0.0145\n",
      "Epoch : 11 | Training Loss : 0.0142\n",
      "Epoch : 11 | Training Loss : 0.0280\n",
      "Epoch : 11 | Training Loss : 0.0564\n",
      "Epoch : 11 | Training Loss : 0.0143\n",
      "Epoch : 11 | Training Loss : 0.0216\n",
      "Epoch : 11 | Training Loss : 0.0130\n",
      "Epoch : 11 | Training Loss : 0.0078\n",
      "Epoch : 11 | Training Loss : 0.0327\n",
      "Epoch : 11 | Training Loss : 0.0201\n",
      "Epoch : 11 | Training Loss : 0.0171\n",
      "Epoch : 11 | Training Loss : 0.0089\n",
      "Epoch : 11 | Training Loss : 0.0082\n",
      "Epoch : 11 | Training Loss : 0.0857\n",
      "Epoch : 11 | Training Loss : 0.0070\n",
      "Epoch : 11 | Training Loss : 0.0330\n",
      "Epoch : 11 | Training Loss : 0.0148\n",
      "Epoch : 11 | Training Loss : 0.0065\n",
      "Epoch : 11 | Training Loss : 0.0151\n",
      "Epoch : 11 | Training Loss : 0.0108\n",
      "Epoch : 11 | Training Loss : 0.0187\n",
      "Epoch : 11 | Training Loss : 0.0352\n",
      "Epoch : 11 | Training Loss : 0.0115\n",
      "Epoch : 11 | Training Loss : 0.0050\n",
      "Epoch : 11 | Training Loss : 0.0148\n",
      "Epoch : 11 | Training Loss : 0.0236\n",
      "Epoch : 11 | Training Loss : 0.0200\n",
      "Epoch : 11 | Training Loss : 0.0060\n",
      "Epoch : 11 | Training Loss : 0.0149\n",
      "Epoch : 11 | Training Loss : 0.0234\n",
      "Epoch : 11 | Training Loss : 0.0067\n",
      "Epoch : 11 | Training Loss : 0.0161\n",
      "Epoch : 11 | Training Loss : 0.0140\n",
      "Epoch : 11 | Training Loss : 0.0217\n",
      "Epoch : 11 | Training Loss : 0.0224\n",
      "Epoch : 11 | Training Loss : 0.0089\n",
      "Epoch : 11 | Training Loss : 0.0059\n",
      "Epoch : 11 | Training Loss : 0.0113\n",
      "Epoch : 11 | Training Loss : 0.0099\n",
      "Epoch : 11 | Training Loss : 0.0117\n",
      "Epoch : 11 | Training Loss : 0.0037\n",
      "Epoch : 11 | Training Loss : 0.0114\n",
      "Epoch : 11 | Training Loss : 0.0119\n",
      "Epoch : 11 | Training Loss : 0.0156\n",
      "Epoch : 11 | Training Loss : 0.0124\n",
      "Epoch : 11 | Training Loss : 0.0314\n",
      "Epoch : 11 | Training Loss : 0.0120\n",
      "Epoch : 11 | Training Loss : 0.0180\n",
      "Epoch : 11 | Training Loss : 0.0063\n",
      "Epoch : 11 | Training Loss : 0.0068\n",
      "Epoch : 11 | Training Loss : 0.0277\n",
      "Epoch : 11 | Training Loss : 0.0196\n",
      "Epoch : 11 | Training Loss : 0.0315\n",
      "Epoch : 11 | Training Loss : 0.0114\n",
      "Epoch : 11 | Training Loss : 0.0085\n",
      "Epoch : 11 | Training Loss : 0.0048\n",
      "Epoch : 11 | Training Loss : 0.0205\n",
      "Epoch : 11 | Training Loss : 0.0073\n",
      "Epoch : 11 | Training Loss : 0.0165\n",
      "Epoch : 11 | Training Loss : 0.0103\n",
      "Epoch : 11 | Training Loss : 0.0117\n",
      "Epoch : 11 | Training Loss : 0.0067\n",
      "Epoch : 11 | Training Loss : 0.0265\n",
      "Epoch : 11 | Training Loss : 0.0238\n",
      "Epoch : 11 | Training Loss : 0.0254\n",
      "Epoch : 11 | Training Loss : 0.0237\n",
      "Epoch : 11 | Training Loss : 0.0135\n",
      "Epoch : 11 | Training Loss : 0.0386\n",
      "Epoch : 11 | Training Loss : 0.0062\n",
      "Epoch : 11 | Training Loss : 0.0211\n",
      "Epoch : 11 | Training Loss : 0.0307\n",
      "Epoch : 11 | Training Loss : 0.0614\n",
      "Epoch : 11 | Training Loss : 0.0131\n",
      "Epoch : 11 | Training Loss : 0.0128\n",
      "Epoch : 11 | Training Loss : 0.0226\n",
      "Epoch : 11 | Training Loss : 0.0316\n",
      "Epoch : 11 | Training Loss : 0.0133\n",
      "Epoch : 11 | Training Loss : 0.0091\n",
      "Epoch : 11 | Training Loss : 0.0269\n",
      "Epoch : 11 | Training Loss : 0.0115\n",
      "Epoch : 11 | Training Loss : 0.0741\n",
      "Epoch : 11 | Training Loss : 0.0268\n",
      "Epoch : 11 | Training Loss : 0.0257\n",
      "Epoch : 11 | Training Loss : 0.0210\n",
      "Epoch : 11 | Training Loss : 0.0322\n",
      "Epoch : 11 | Training Loss : 0.0172\n",
      "Epoch : 11 | Training Loss : 0.0162\n",
      "Epoch : 11 | Training Loss : 0.0297\n",
      "Epoch : 11 | Training Loss : 0.0471\n",
      "Epoch : 11 | Training Loss : 0.0204\n",
      "Epoch : 11 | Training Loss : 0.0286\n",
      "Epoch : 11 | Training Loss : 0.0067\n",
      "Epoch : 11 | Training Loss : 0.0216\n",
      "Epoch : 11 | Training Loss : 0.0055\n",
      "Epoch : 11 | Training Loss : 0.0254\n",
      "Epoch : 11 | Training Loss : 0.0551\n",
      "Epoch : 11 | Training Loss : 0.0052\n",
      "Epoch : 11 | Training Loss : 0.0410\n",
      "Epoch : 11 | Training Loss : 0.0081\n",
      "Epoch : 11 | Training Loss : 0.0145\n",
      "Epoch : 11 | Training Loss : 0.0622\n",
      "Epoch : 12 | Training Loss : 0.0155\n",
      "Epoch : 12 | Training Loss : 0.0114\n",
      "Epoch : 12 | Training Loss : 0.0189\n",
      "Epoch : 12 | Training Loss : 0.0054\n",
      "Epoch : 12 | Training Loss : 0.0141\n",
      "Epoch : 12 | Training Loss : 0.0115\n",
      "Epoch : 12 | Training Loss : 0.0110\n",
      "Epoch : 12 | Training Loss : 0.0203\n",
      "Epoch : 12 | Training Loss : 0.0200\n",
      "Epoch : 12 | Training Loss : 0.0304\n",
      "Epoch : 12 | Training Loss : 0.0150\n",
      "Epoch : 12 | Training Loss : 0.0334\n",
      "Epoch : 12 | Training Loss : 0.0133\n",
      "Epoch : 12 | Training Loss : 0.0172\n",
      "Epoch : 12 | Training Loss : 0.0325\n",
      "Epoch : 12 | Training Loss : 0.0097\n",
      "Epoch : 12 | Training Loss : 0.0104\n",
      "Epoch : 12 | Training Loss : 0.0321\n",
      "Epoch : 12 | Training Loss : 0.0056\n",
      "Epoch : 12 | Training Loss : 0.0204\n",
      "Epoch : 12 | Training Loss : 0.0319\n",
      "Epoch : 12 | Training Loss : 0.0041\n",
      "Epoch : 12 | Training Loss : 0.0145\n",
      "Epoch : 12 | Training Loss : 0.0130\n",
      "Epoch : 12 | Training Loss : 0.0184\n",
      "Epoch : 12 | Training Loss : 0.0074\n",
      "Epoch : 12 | Training Loss : 0.0131\n",
      "Epoch : 12 | Training Loss : 0.0171\n",
      "Epoch : 12 | Training Loss : 0.0131\n",
      "Epoch : 12 | Training Loss : 0.0123\n",
      "Epoch : 12 | Training Loss : 0.0091\n",
      "Epoch : 12 | Training Loss : 0.0065\n",
      "Epoch : 12 | Training Loss : 0.0163\n",
      "Epoch : 12 | Training Loss : 0.0267\n",
      "Epoch : 12 | Training Loss : 0.0034\n",
      "Epoch : 12 | Training Loss : 0.0290\n",
      "Epoch : 12 | Training Loss : 0.0050\n",
      "Epoch : 12 | Training Loss : 0.0082\n",
      "Epoch : 12 | Training Loss : 0.0114\n",
      "Epoch : 12 | Training Loss : 0.0209\n",
      "Epoch : 12 | Training Loss : 0.0075\n",
      "Epoch : 12 | Training Loss : 0.0164\n",
      "Epoch : 12 | Training Loss : 0.0089\n",
      "Epoch : 12 | Training Loss : 0.0231\n",
      "Epoch : 12 | Training Loss : 0.0059\n",
      "Epoch : 12 | Training Loss : 0.0103\n",
      "Epoch : 12 | Training Loss : 0.0123\n",
      "Epoch : 12 | Training Loss : 0.0189\n",
      "Epoch : 12 | Training Loss : 0.0130\n",
      "Epoch : 12 | Training Loss : 0.0031\n",
      "Epoch : 12 | Training Loss : 0.0222\n",
      "Epoch : 12 | Training Loss : 0.0293\n",
      "Epoch : 12 | Training Loss : 0.0053\n",
      "Epoch : 12 | Training Loss : 0.0069\n",
      "Epoch : 12 | Training Loss : 0.0237\n",
      "Epoch : 12 | Training Loss : 0.0034\n",
      "Epoch : 12 | Training Loss : 0.0433\n",
      "Epoch : 12 | Training Loss : 0.0100\n",
      "Epoch : 12 | Training Loss : 0.0102\n",
      "Epoch : 12 | Training Loss : 0.0104\n",
      "Epoch : 12 | Training Loss : 0.0044\n",
      "Epoch : 12 | Training Loss : 0.0078\n",
      "Epoch : 12 | Training Loss : 0.0041\n",
      "Epoch : 12 | Training Loss : 0.0074\n",
      "Epoch : 12 | Training Loss : 0.0066\n",
      "Epoch : 12 | Training Loss : 0.0101\n",
      "Epoch : 12 | Training Loss : 0.0119\n",
      "Epoch : 12 | Training Loss : 0.0251\n",
      "Epoch : 12 | Training Loss : 0.0050\n",
      "Epoch : 12 | Training Loss : 0.0193\n",
      "Epoch : 12 | Training Loss : 0.0107\n",
      "Epoch : 12 | Training Loss : 0.0133\n",
      "Epoch : 12 | Training Loss : 0.0120\n",
      "Epoch : 12 | Training Loss : 0.0056\n",
      "Epoch : 12 | Training Loss : 0.0089\n",
      "Epoch : 12 | Training Loss : 0.0471\n",
      "Epoch : 12 | Training Loss : 0.0133\n",
      "Epoch : 12 | Training Loss : 0.0109\n",
      "Epoch : 12 | Training Loss : 0.0362\n",
      "Epoch : 12 | Training Loss : 0.0735\n",
      "Epoch : 12 | Training Loss : 0.0121\n",
      "Epoch : 12 | Training Loss : 0.0060\n",
      "Epoch : 12 | Training Loss : 0.0047\n",
      "Epoch : 12 | Training Loss : 0.0137\n",
      "Epoch : 12 | Training Loss : 0.0673\n",
      "Epoch : 12 | Training Loss : 0.0132\n",
      "Epoch : 12 | Training Loss : 0.0128\n",
      "Epoch : 12 | Training Loss : 0.0138\n",
      "Epoch : 12 | Training Loss : 0.0342\n",
      "Epoch : 12 | Training Loss : 0.0296\n",
      "Epoch : 12 | Training Loss : 0.0576\n",
      "Epoch : 12 | Training Loss : 0.0063\n",
      "Epoch : 12 | Training Loss : 0.0084\n",
      "Epoch : 12 | Training Loss : 0.0158\n",
      "Epoch : 12 | Training Loss : 0.0064\n",
      "Epoch : 12 | Training Loss : 0.0083\n",
      "Epoch : 12 | Training Loss : 0.0460\n",
      "Epoch : 12 | Training Loss : 0.0453\n",
      "Epoch : 12 | Training Loss : 0.0162\n",
      "Epoch : 12 | Training Loss : 0.0596\n",
      "Epoch : 12 | Training Loss : 0.1458\n",
      "Epoch : 12 | Training Loss : 0.0221\n",
      "Epoch : 12 | Training Loss : 0.1235\n",
      "Epoch : 12 | Training Loss : 0.0137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jgril\\Documents\\GitHub\\8803_Final_Project\\8803FinalProject.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/8803FinalProject.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m out \u001b[39m=\u001b[39m net(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/8803FinalProject.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(out, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/8803FinalProject.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/8803FinalProject.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jgril/Documents/GitHub/8803_Final_Project/8803FinalProject.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m | Training Loss : \u001b[39m\u001b[39m{:0.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[1;32mc:\\Users\\jgril\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\jgril\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Problem 3 (d)\n",
    "\n",
    "# perform training \n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "# initliaze the network\n",
    "net = MySimpleCNN()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  net.train() # training mode\n",
    "\n",
    "  for iteration, (x, y) in enumerate(trainloader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = net(x)\n",
    "    loss = loss_function(out, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch : {} | Training Loss : {:0.4f}'.format(epoch, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.086\n",
      "[1,   200] loss: 1.087\n",
      "[1,   300] loss: 1.084\n",
      "[1,   400] loss: 1.085\n",
      "[1,   500] loss: 1.086\n",
      "[1,   600] loss: 1.084\n",
      "[1,   700] loss: 1.086\n",
      "[2,   100] loss: 1.086\n",
      "[2,   200] loss: 1.086\n",
      "[2,   300] loss: 1.087\n",
      "[2,   400] loss: 1.085\n",
      "[2,   500] loss: 1.085\n",
      "[2,   600] loss: 1.085\n",
      "[2,   700] loss: 1.085\n",
      "[3,   100] loss: 1.090\n",
      "[3,   200] loss: 1.083\n",
      "[3,   300] loss: 1.083\n",
      "[3,   400] loss: 1.087\n",
      "[3,   500] loss: 1.087\n",
      "[3,   600] loss: 1.085\n",
      "[3,   700] loss: 1.083\n",
      "[4,   100] loss: 1.086\n",
      "[4,   200] loss: 1.084\n",
      "[4,   300] loss: 1.085\n",
      "[4,   400] loss: 1.087\n",
      "[4,   500] loss: 1.084\n",
      "[4,   600] loss: 1.086\n",
      "[4,   700] loss: 1.087\n",
      "[5,   100] loss: 1.086\n",
      "[5,   200] loss: 1.084\n",
      "[5,   300] loss: 1.086\n",
      "[5,   400] loss: 1.084\n",
      "[5,   500] loss: 1.086\n",
      "[5,   600] loss: 1.086\n",
      "[5,   700] loss: 1.086\n",
      "[6,   100] loss: 1.086\n",
      "[6,   200] loss: 1.086\n",
      "[6,   300] loss: 1.086\n",
      "[6,   400] loss: 1.082\n",
      "[6,   500] loss: 1.085\n",
      "[6,   600] loss: 1.086\n",
      "[6,   700] loss: 1.084\n",
      "[7,   100] loss: 1.084\n",
      "[7,   200] loss: 1.085\n",
      "[7,   300] loss: 1.088\n",
      "[7,   400] loss: 1.087\n",
      "[7,   500] loss: 1.084\n",
      "[7,   600] loss: 1.086\n",
      "[7,   700] loss: 1.084\n",
      "[8,   100] loss: 1.085\n",
      "[8,   200] loss: 1.086\n",
      "[8,   300] loss: 1.086\n",
      "[8,   400] loss: 1.086\n",
      "[8,   500] loss: 1.086\n",
      "[8,   600] loss: 1.084\n",
      "[8,   700] loss: 1.086\n",
      "[9,   100] loss: 1.083\n",
      "[9,   200] loss: 1.086\n",
      "[9,   300] loss: 1.084\n",
      "[9,   400] loss: 1.086\n",
      "[9,   500] loss: 1.087\n",
      "[9,   600] loss: 1.085\n",
      "[9,   700] loss: 1.086\n",
      "[10,   100] loss: 1.085\n",
      "[10,   200] loss: 1.085\n",
      "[10,   300] loss: 1.084\n",
      "[10,   400] loss: 1.085\n",
      "[10,   500] loss: 1.083\n",
      "[10,   600] loss: 1.087\n",
      "[10,   700] loss: 1.088\n",
      "Accuracy of the network on the test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 3)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    def train(self, image_trainset, image_testset, lr=0.01, epochs=10, batch_size=32):\n",
    "        '''\n",
    "        # Define the data loaders and transformations\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        test_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        '''\n",
    "        #train_dataset = datasets.ImageFolder(root='./train', transform=train_transforms)\n",
    "        #test_dataset = datasets.ImageFolder(root='./test', transform=test_transforms)\n",
    "        self.train_loader = torch.utils.data.DataLoader(image_trainset, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(image_testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        model = CNN()\n",
    "        for epoch in range(epochs): \n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.train_loader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 99:\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / 100))\n",
    "                    running_loss = 0.0\n",
    "    def test(self, lr=0.01, epochs=10, batch_size=32):\n",
    "        # Evaluate the model on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                images, labels = data\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        \n",
    "\n",
    "    \n",
    "model.train(image_trainset, image_testset, lr=0.01, epochs=10, batch_size=32, )\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f53be5b945d9bd707dfb463269b5f5206ccedac637d039afb7a68ece5d290f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
